{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install dependencies"
      ],
      "metadata": {
        "id": "H8jcouD6JDh5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhE-l7UWJ8lH",
        "outputId": "478c873f-d408-4e32-b616-dcadb157edad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fairseq\n",
            "  Downloading fairseq-0.12.2.tar.gz (9.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.15.1)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq) (0.29.36)\n",
            "Collecting hydra-core<1.1,>=1.0.7 (from fairseq)\n",
            "  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting omegaconf<2.1 (from fairseq)\n",
            "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fairseq) (2023.6.3)\n",
            "Collecting sacrebleu>=1.4.12 (from fairseq)\n",
            "  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.0.1+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fairseq) (4.66.1)\n",
            "Collecting bitarray (from fairseq)\n",
            "  Downloading bitarray-2.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (286 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.2/286.2 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.0.2+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.23.5)\n",
            "Collecting antlr4-python3-runtime==4.8 (from hydra-core<1.1,>=1.0.7->fairseq)\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq) (4.7.1)\n",
            "Collecting portalocker (from sacrebleu>=1.4.12->fairseq)\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (0.9.0)\n",
            "Collecting colorama (from sacrebleu>=1.4.12->fairseq)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (4.9.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->fairseq) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->fairseq) (16.0.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq) (2.21)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->fairseq) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->fairseq) (1.3.0)\n",
            "Building wheels for collected packages: fairseq, antlr4-python3-runtime\n",
            "  Building wheel for fairseq (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-0.12.2-cp310-cp310-linux_x86_64.whl size=11288406 sha256=7d9ae7728008eaef55788745c58374ebc15361a56d333a09fa9c46a36279d470\n",
            "  Stored in directory: /root/.cache/pip/wheels/e4/35/55/9c66f65ec7c83fd6fbc2b9502a0ac81b2448a1196159dacc32\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141210 sha256=7b4f64b21759505fe55d0e83b18a36f689bf930b1b9785ba992dcf53c8d40d10\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/20/bd/e1477d664f22d99989fd28ee1a43d6633dddb5cb9e801350d5\n",
            "Successfully built fairseq antlr4-python3-runtime\n",
            "Installing collected packages: bitarray, antlr4-python3-runtime, portalocker, omegaconf, colorama, sacrebleu, hydra-core, fairseq\n",
            "Successfully installed antlr4-python3-runtime-4.8 bitarray-2.8.1 colorama-0.4.6 fairseq-0.12.2 hydra-core-1.0.7 omegaconf-2.0.6 portalocker-2.7.0 sacrebleu-2.3.1\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n"
          ]
        }
      ],
      "source": [
        "!pip install fairseq\n",
        "!pip install sentencepiece\n",
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/google/sentencepiece.git\n",
        "!mkdir sentencepiece/build\n",
        "!cd sentencepiece/build && cmake ..\n",
        "!cd sentencepiece/build && make -j $(nproc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJRMe-VqXOze",
        "outputId": "bad468c7-2229-4ce1-db09-9c0596c09ee3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'sentencepiece'...\n",
            "remote: Enumerating objects: 4823, done.\u001b[K\n",
            "remote: Counting objects: 100% (1450/1450), done.\u001b[K\n",
            "remote: Compressing objects: 100% (320/320), done.\u001b[K\n",
            "remote: Total 4823 (delta 1175), reused 1195 (delta 1089), pack-reused 3373\u001b[K\n",
            "Receiving objects: 100% (4823/4823), 26.77 MiB | 28.74 MiB/s, done.\n",
            "Resolving deltas: 100% (3314/3314), done.\n",
            "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:15 (cmake_minimum_required):\n",
            "  Compatibility with CMake < 3.5 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value or use a ...<max> suffix to tell\n",
            "  CMake that the project does not need compatibility with older versions.\n",
            "\n",
            "\u001b[0m\n",
            "-- VERSION: 0.2.00\n",
            "-- The C compiler identification is GNU 11.4.0\n",
            "-- The CXX compiler identification is GNU 11.4.0\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Check for working C compiler: /usr/bin/cc - skipped\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "-- Found Threads: TRUE  \n",
            "-- Not Found TCMalloc: TCMALLOC_LIB-NOTFOUND\n",
            "-- Configuring done (0.6s)\n",
            "-- Generating done (0.0s)\n",
            "-- Build files have been written to: /content/sentencepiece/build\n",
            "[  1%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/arena.cc.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/arenastring.cc.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece_train-static.dir/builder.cc.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/arena.cc.o\u001b[0m\n",
            "[  4%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/arenastring.cc.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece_train-static.dir/unicode_script.cc.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/bytestream.cc.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/coded_stream.cc.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/bytestream.cc.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/coded_stream.cc.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/common.cc.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/common.cc.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/extension_set.cc.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/generated_enum_util.cc.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/extension_set.cc.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece_train-static.dir/trainer_factory.cc.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/generated_message_table_driven_lite.cc.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece_train-static.dir/trainer_interface.cc.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/generated_message_util.cc.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/implicit_weak_message.cc.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/generated_enum_util.cc.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/int128.cc.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/generated_message_table_driven_lite.cc.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/io_win32.cc.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/message_lite.cc.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/parse_context.cc.o\u001b[0m\n",
            "In file included from \u001b[01m\u001b[K/usr/include/string.h:535\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/sentencepiece/src/../third_party/protobuf-lite/google/protobuf/stubs/port.h:39\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/sentencepiece/src/../third_party/protobuf-lite/google/protobuf/stubs/macros.h:34\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/sentencepiece/src/../third_party/protobuf-lite/google/protobuf/stubs/common.h:46\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/sentencepiece/src/../third_party/protobuf-lite/google/protobuf/message_lite.h:45\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/sentencepiece/third_party/protobuf-lite/message_lite.cc:36\u001b[m\u001b[K:\n",
            "In function ‘\u001b[01m\u001b[Kvoid* memcpy(void*, const void*, size_t)\u001b[m\u001b[K’,\n",
            "    inlined from ‘\u001b[01m\u001b[Kgoogle::protobuf::uint8* google::protobuf::io::EpsCopyOutputStream::WriteRaw(const void*, int, google::protobuf::uint8*)\u001b[m\u001b[K’ at \u001b[01m\u001b[K/content/sentencepiece/src/../third_party/protobuf-lite/google/protobuf/io/coded_stream.h:699:16\u001b[m\u001b[K,\n",
            "    inlined from ‘\u001b[01m\u001b[Kvirtual google::protobuf::uint8* google::protobuf::internal::ImplicitWeakMessage::_InternalSerialize(google::protobuf::uint8*, google::protobuf::io::EpsCopyOutputStream*) const\u001b[m\u001b[K’ at \u001b[01m\u001b[K/content/sentencepiece/src/../third_party/protobuf-lite/google/protobuf/implicit_weak_message.h:85:28\u001b[m\u001b[K,\n",
            "    inlined from ‘\u001b[01m\u001b[Kbool google::protobuf::MessageLite::SerializePartialToZeroCopyStream(google::protobuf::io::ZeroCopyOutputStream*) const\u001b[m\u001b[K’ at \u001b[01m\u001b[K/content/sentencepiece/third_party/protobuf-lite/message_lite.cc:419:30\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/include/x86_64-linux-gnu/bits/string_fortified.h:29:33:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid* __builtin___memcpy_chk(void*, const void*, long unsigned int, long unsigned int)\u001b[m\u001b[K’ specified size between 18446744071562067968 and 18446744073709551615 exceeds maximum object size 9223372036854775807 [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wstringop-overflow=\u0007-Wstringop-overflow=\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   29 |   return \u001b[01;35m\u001b[K__builtin___memcpy_chk (__dest, __src, __len,\u001b[m\u001b[K\n",
            "      |          \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "   30 | \u001b[01;35m\u001b[K                                 __glibc_objsize0 (__dest))\u001b[m\u001b[K;\n",
            "      |                                  \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "[ 25%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/repeated_field.cc.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/generated_message_util.cc.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece_train-static.dir/unigram_model_trainer.cc.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/implicit_weak_message.cc.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/int128.cc.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/io_win32.cc.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/message_lite.cc.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/parse_context.cc.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/status.cc.o\u001b[0m\n",
            "In file included from \u001b[01m\u001b[K/usr/include/string.h:535\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/sentencepiece/src/../third_party/protobuf-lite/google/protobuf/stubs/port.h:39\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/sentencepiece/src/../third_party/protobuf-lite/google/protobuf/stubs/macros.h:34\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/sentencepiece/src/../third_party/protobuf-lite/google/protobuf/stubs/common.h:46\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/sentencepiece/src/../third_party/protobuf-lite/google/protobuf/message_lite.h:45\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/sentencepiece/third_party/protobuf-lite/message_lite.cc:36\u001b[m\u001b[K:\n",
            "In function ‘\u001b[01m\u001b[Kvoid* memcpy(void*, const void*, size_t)\u001b[m\u001b[K’,\n",
            "    inlined from ‘\u001b[01m\u001b[Kgoogle::protobuf::uint8* google::protobuf::io::EpsCopyOutputStream::WriteRaw(const void*, int, google::protobuf::uint8*)\u001b[m\u001b[K’ at \u001b[01m\u001b[K/content/sentencepiece/src/../third_party/protobuf-lite/google/protobuf/io/coded_stream.h:699:16\u001b[m\u001b[K,\n",
            "    inlined from ‘\u001b[01m\u001b[Kvirtual google::protobuf::uint8* google::protobuf::internal::ImplicitWeakMessage::_InternalSerialize(google::protobuf::uint8*, google::protobuf::io::EpsCopyOutputStream*) const\u001b[m\u001b[K’ at \u001b[01m\u001b[K/content/sentencepiece/src/../third_party/protobuf-lite/google/protobuf/implicit_weak_message.h:85:28\u001b[m\u001b[K,\n",
            "    inlined from ‘\u001b[01m\u001b[Kbool google::protobuf::MessageLite::SerializePartialToZeroCopyStream(google::protobuf::io::ZeroCopyOutputStream*) const\u001b[m\u001b[K’ at \u001b[01m\u001b[K/content/sentencepiece/third_party/protobuf-lite/message_lite.cc:419:30\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/include/x86_64-linux-gnu/bits/string_fortified.h:29:33:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid* __builtin___memcpy_chk(void*, const void*, long unsigned int, long unsigned int)\u001b[m\u001b[K’ specified size between 18446744071562067968 and 18446744073709551615 exceeds maximum object size 9223372036854775807 [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wstringop-overflow=\u0007-Wstringop-overflow=\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   29 |   return \u001b[01;35m\u001b[K__builtin___memcpy_chk (__dest, __src, __len,\u001b[m\u001b[K\n",
            "      |          \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "   30 | \u001b[01;35m\u001b[K                                 __glibc_objsize0 (__dest))\u001b[m\u001b[K;\n",
            "      |                                  \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "[ 32%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/repeated_field.cc.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/statusor.cc.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/stringpiece.cc.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/status.cc.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/stringprintf.cc.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/statusor.cc.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/structurally_valid.cc.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/stringpiece.cc.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/strutil.cc.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/stringprintf.cc.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/structurally_valid.cc.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/strutil.cc.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece_train-static.dir/word_model_trainer.cc.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/time.cc.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/time.cc.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/wire_format_lite.cc.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/zero_copy_stream.cc.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/wire_format_lite.cc.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece_train-static.dir/char_model_trainer.cc.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/zero_copy_stream_impl.cc.o\u001b[0m\n",
            "[ 49%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/protobuf-lite/zero_copy_stream_impl_lite.cc.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/builtin_pb/sentencepiece.pb.cc.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/builtin_pb/sentencepiece_model.pb.cc.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/zero_copy_stream.cc.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/zero_copy_stream_impl.cc.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece_train-static.dir/bpe_model_trainer.cc.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/protobuf-lite/zero_copy_stream_impl_lite.cc.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/bpe_model.cc.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/builtin_pb/sentencepiece.pb.cc.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/char_model.cc.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/error.cc.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/builtin_pb/sentencepiece_model.pb.cc.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece_train-static.dir/sentencepiece_trainer.cc.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/filesystem.cc.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/model_factory.cc.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/model_interface.cc.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/normalizer.cc.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/bpe_model.cc.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/sentencepiece_processor.cc.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece_train-static.dir/pretokenizer_for_training.cc.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/unigram_model.cc.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/char_model.cc.o\u001b[0m\n",
            "[ 69%] \u001b[32m\u001b[1mLinking CXX static library libsentencepiece_train.a\u001b[0m\n",
            "[ 69%] Built target sentencepiece_train-static\n",
            "[ 70%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/error.cc.o\u001b[0m\n",
            "[ 71%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/filesystem.cc.o\u001b[0m\n",
            "[ 71%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/model_factory.cc.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/model_interface.cc.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/normalizer.cc.o\u001b[0m\n",
            "[ 74%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/util.cc.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/word_model.cc.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/sentencepiece_processor.cc.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece.dir/__/third_party/absl/flags/flag.cc.o\u001b[0m\n",
            "[ 78%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/unigram_model.cc.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/util.cc.o\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/word_model.cc.o\u001b[0m\n",
            "[ 80%] \u001b[32m\u001b[1mLinking CXX shared library libsentencepiece.so\u001b[0m\n",
            "[ 80%] Built target sentencepiece\n",
            "[ 81%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece_train.dir/builder.cc.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece-static.dir/__/third_party/absl/flags/flag.cc.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece_train.dir/unicode_script.cc.o\u001b[0m\n",
            "[ 84%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece_train.dir/trainer_factory.cc.o\u001b[0m\n",
            "[ 84%] \u001b[32m\u001b[1mLinking CXX static library libsentencepiece.a\u001b[0m\n",
            "[ 84%] Built target sentencepiece-static\n",
            "[ 85%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece_train.dir/trainer_interface.cc.o\u001b[0m\n",
            "[ 86%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece_train.dir/unigram_model_trainer.cc.o\u001b[0m\n",
            "[ 87%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece_train.dir/word_model_trainer.cc.o\u001b[0m\n",
            "[ 88%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece_train.dir/char_model_trainer.cc.o\u001b[0m\n",
            "[ 88%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece_train.dir/bpe_model_trainer.cc.o\u001b[0m\n",
            "[ 89%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece_train.dir/sentencepiece_trainer.cc.o\u001b[0m\n",
            "[ 90%] \u001b[32mBuilding CXX object src/CMakeFiles/sentencepiece_train.dir/pretokenizer_for_training.cc.o\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object src/CMakeFiles/spm_encode.dir/spm_encode_main.cc.o\u001b[0m\n",
            "[ 92%] \u001b[32mBuilding CXX object src/CMakeFiles/spm_decode.dir/spm_decode_main.cc.o\u001b[0m\n",
            "[ 93%] \u001b[32mBuilding CXX object src/CMakeFiles/spm_export_vocab.dir/spm_export_vocab_main.cc.o\u001b[0m\n",
            "[ 93%] \u001b[32m\u001b[1mLinking CXX executable spm_decode\u001b[0m\n",
            "[ 93%] Built target spm_decode\n",
            "[ 94%] \u001b[32m\u001b[1mLinking CXX shared library libsentencepiece_train.so\u001b[0m\n",
            "[ 94%] Built target sentencepiece_train\n",
            "[ 95%] \u001b[32mBuilding CXX object src/CMakeFiles/spm_normalize.dir/spm_normalize_main.cc.o\u001b[0m\n",
            "[ 96%] \u001b[32mBuilding CXX object src/CMakeFiles/spm_train.dir/spm_train_main.cc.o\u001b[0m\n",
            "[ 97%] \u001b[32m\u001b[1mLinking CXX executable spm_encode\u001b[0m\n",
            "[ 97%] Built target spm_encode\n",
            "[ 98%] \u001b[32m\u001b[1mLinking CXX executable spm_export_vocab\u001b[0m\n",
            "[ 98%] Built target spm_export_vocab\n",
            "[ 99%] \u001b[32m\u001b[1mLinking CXX executable spm_normalize\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable spm_train\u001b[0m\n",
            "[100%] Built target spm_normalize\n",
            "[100%] Built target spm_train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%env SPM=/content/sentencepiece/build/src\n",
        "!echo $SPM"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCpeXDEsXp1f",
        "outputId": "9c9d2c13-550e-48eb-c12e-e928333cd4aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: SPM=/content/sentencepiece/build/src\n",
            "/content/sentencepiece/build/src\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fairseq/models/mbart/mbart.cc25.v2.tar.gz\n",
        "!tar -xzvf mbart.cc25.v2.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vv-QBCyPXLIQ",
        "outputId": "11e0c0e4-1a14-4c99-cb85-18a8ed0c343a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-08-16 08:41:49--  https://dl.fbaipublicfiles.com/fairseq/models/mbart/mbart.cc25.v2.tar.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.224.2.21, 13.224.2.6, 13.224.2.42, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.224.2.21|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5618826847 (5.2G) [application/gzip]\n",
            "Saving to: ‘mbart.cc25.v2.tar.gz’\n",
            "\n",
            "mbart.cc25.v2.tar.g 100%[===================>]   5.23G  42.0MB/s    in 2m 6s   \n",
            "\n",
            "2023-08-16 08:43:55 (42.6 MB/s) - ‘mbart.cc25.v2.tar.gz’ saved [5618826847/5618826847]\n",
            "\n",
            "mbart.cc25.v2/\n",
            "mbart.cc25.v2/sentence.bpe.model\n",
            "mbart.cc25.v2/dict.txt\n",
            "mbart.cc25.v2/model.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import neccessary libs\n",
        "import os\n",
        "import torch\n",
        "from glob import glob\n",
        "from typing import List\n",
        "\n",
        "from google.colab import drive\n",
        "from fairseq.data import Dictionary\n",
        "from fairseq.tokenizer import tokenize_line"
      ],
      "metadata": {
        "id": "YjB0n-YuJS1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount from drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ZTvhpUVGNA8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dedbc7e1-a7a2-4c8a-fdf7-cd8285b8121a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create stored data folder\n",
        "!mkdir tokenized\n",
        "!mkdir model"
      ],
      "metadata": {
        "id": "BZ9ybRRzLnhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TQ5kaFVLuAu"
      },
      "source": [
        "# Dataset pre-processing\n",
        "1. Split dataset to train, val, test set\n",
        "1.   Using the pretrained Sentencepiece model to pre-process our data (also do this to validation/test sets). It will split the sentences into **subwords**, adding the special symbol `▁` to the first subword of a word.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!$SPM/spm_encode --model=\"drive/MyDrive/sentencepiece.bpe.model\" --output_format=piece < \"drive/MyDrive/train.en\" > tokenized/train.spm.en\n",
        "!$SPM/spm_encode --model=\"drive/MyDrive/sentencepiece.bpe.model\" --output_format=piece < \"drive/MyDrive/train.vi\" > tokenized/train.spm.vi\n",
        "\n",
        "!$SPM/spm_encode --model=\"drive/MyDrive/sentencepiece.bpe.model\" --output_format=piece < \"drive/MyDrive/valid.en\" > tokenized/val.spm.en\n",
        "!$SPM/spm_encode --model=\"drive/MyDrive/sentencepiece.bpe.model\" --output_format=piece < \"drive/MyDrive/valid.vi\" > tokenized/val.spm.vi"
      ],
      "metadata": {
        "id": "opd4VWZLMA6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pruning the pre-trained model\n",
        "Most of the words in the large vocabulary used by the original pre-training model are not actually used in the finetune process, so this part of redundant information can be removed.\n",
        "\n",
        "Reduce the size of the pre-trained model by pruning the word embeddings for fine-tuning:\n",
        "\n",
        "- Firstly, build a new vocab for our dataset"
      ],
      "metadata": {
        "id": "bnkUb7haLngj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build new vocab\n",
        "def pad_dict(d: Dictionary, num_extra_symbols: int, padding_factor: int = 8) -> None:\n",
        "    i = 0\n",
        "    while (len(d) + num_extra_symbols) % padding_factor != 0:\n",
        "        symbol = f\"madeupword{i:04d}\"\n",
        "        d.add_symbol(symbol, n=0)\n",
        "        i += 1\n",
        "\n",
        "langs = 'ar_AR,cs_CZ,de_DE,en_XX,es_XX,et_EE,fi_FI,fr_XX,gu_IN,hi_IN,it_IT,ja_XX,kk_KZ,ko_KR,lt_LT,lv_LV,my_MM,ne_NP,nl_XX,ro_RO,ru_RU,si_LK,tr_TR,vi_VN,zh_CN'\n",
        "data_dir = \"./tokenized/*.spm.*\"\n",
        "output = './model/dict.txt'\n",
        "\n",
        "langs = langs.split(\",\")\n",
        "ft_dict = Dictionary()\n",
        "\n",
        "for data_path in glob(data_dir):\n",
        "  Dictionary.add_file_to_dictionary(data_path, ft_dict, tokenize_line, 4)\n",
        "\n",
        "ft_dict.finalize(padding_factor=0)\n",
        "pad_dict(ft_dict, len(langs) + 1)\n",
        "ft_dict.save(output)"
      ],
      "metadata": {
        "id": "3kO3avbyLjwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Pruning word embeddings base on new vocab built"
      ],
      "metadata": {
        "id": "4lwy2_Tx8woG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pruning word embeddings by the new vocab\n",
        "\n",
        "def load_dict(langs: List[str], path: str) -> Dictionary:\n",
        "    d = Dictionary.load(path)\n",
        "    for l in langs:\n",
        "        d.add_symbol(f\"[{l}]\")\n",
        "    d.add_symbol(\"<mask>\")\n",
        "    return d\n",
        "\n",
        "pre_train_dir = './mbart.cc25.v2'\n",
        "ft_dict_path = './model/dict.txt'\n",
        "output = './model/model.pt'\n",
        "\n",
        "pre_dict = load_dict(langs, os.path.join(pre_train_dir, \"dict.txt\"))\n",
        "ft_dict = load_dict(langs, ft_dict_path)\n",
        "data = torch.load(os.path.join(pre_train_dir, \"model.pt\"))\n",
        "model = data[\"model\"]\n",
        "mapping: List[int] = []\n",
        "\n",
        "for i in range(len(ft_dict)):\n",
        "    word = ft_dict[i]\n",
        "    mapping.append(pre_dict.index(word))\n",
        "\n",
        "for name in [\"encoder.embed_tokens.weight\", \"decoder.embed_tokens.weight\"]:\n",
        "    pre_tensor: torch.Tensor = model[name]\n",
        "    ft_tensor = torch.zeros(\n",
        "        [len(ft_dict), 1024], dtype=pre_tensor.dtype, layout=pre_tensor.layout, device=pre_tensor.device,\n",
        "    )\n",
        "    for ft_i, pre_i in enumerate(mapping):\n",
        "      ft_tensor[ft_i] = pre_tensor[pre_i]\n",
        "    model[name] = ft_tensor\n",
        "\n",
        "torch.save(data, output)"
      ],
      "metadata": {
        "id": "qbM3FfG2Logz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Binarize pre-process data\n",
        "\n",
        "Binarize the data to the Fairseq format."
      ],
      "metadata": {
        "id": "LEYcH9wnQTjk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!fairseq-preprocess \\\n",
        "  --source-lang \"vi\" \\\n",
        "  --target-lang \"en\" \\\n",
        "  --trainpref \"tokenized/train.spm\" \\\n",
        "  --validpref \"tokenized/val.spm\" \\\n",
        "  --destdir \"bin\" \\\n",
        "  --bpe sentencepiece \\\n",
        "  --thresholdtgt 0 \\\n",
        "  --thresholdsrc 0 \\\n",
        "  --srcdict \"model/dict.txt\" \\\n",
        "  --tgtdict \"model/dict.txt\" \\\n",
        "  --workers 70"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GfddzivET_Cn",
        "outputId": "96b9f624-ebea-4049-c98f-2cfe4dc79a2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-08-16 08:49:48.522154: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-08-16 08:49:50 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2023-08-16 08:49:50 | INFO | fairseq_cli.preprocess | Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe='sentencepiece', optimizer=None, lr_scheduler='fixed', scoring='bleu', task='translation', source_lang='vi', target_lang='en', trainpref='tokenized/train.spm', validpref='tokenized/val.spm', testpref=None, align_suffix=None, destdir='bin', thresholdtgt=0, thresholdsrc=0, tgtdict='model/dict.txt', srcdict='model/dict.txt', nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=False, padding_factor=8, workers=70, dict_only=False)\n",
            "2023-08-16 08:49:50 | INFO | fairseq_cli.preprocess | [vi] Dictionary: 41630 types\n",
            "2023-08-16 08:53:09 | INFO | fairseq_cli.preprocess | [vi] tokenized/train.spm.vi: 1500000 sents, 45176887 tokens, 0.0% replaced (by <unk>)\n",
            "2023-08-16 08:53:09 | INFO | fairseq_cli.preprocess | [vi] Dictionary: 41630 types\n",
            "2023-08-16 08:53:29 | INFO | fairseq_cli.preprocess | [vi] tokenized/val.spm.vi: 114483 sents, 3448327 tokens, 0.0% replaced (by <unk>)\n",
            "2023-08-16 08:53:29 | INFO | fairseq_cli.preprocess | [en] Dictionary: 41630 types\n",
            "2023-08-16 08:56:21 | INFO | fairseq_cli.preprocess | [en] tokenized/train.spm.en: 1500000 sents, 39682298 tokens, 0.0% replaced (by <unk>)\n",
            "2023-08-16 08:56:21 | INFO | fairseq_cli.preprocess | [en] Dictionary: 41630 types\n",
            "2023-08-16 08:56:36 | INFO | fairseq_cli.preprocess | [en] tokenized/val.spm.en: 114483 sents, 3027009 tokens, 0.0% replaced (by <unk>)\n",
            "2023-08-16 08:56:36 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training\n",
        "\n",
        "Enable half precision (`--fp16`) or define/use a smaller model to speed up the training. But in case using (`--memory-efficient-fp16`) for using memory more efficient\n",
        "\n",
        "In the case of Colab timing out, change the `--keep-interval-updates` and `--no-epoch-checkpoints` flags to save intermidate checkpoints and then resume the training from the last checkpoint."
      ],
      "metadata": {
        "id": "GymK_SJSchJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#%reset # Release memory"
      ],
      "metadata": {
        "id": "HywtGT2ZcIBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!fairseq-train \\\n",
        "  \"bin\" \\\n",
        "  --encoder-normalize-before --decoder-normalize-before \\\n",
        "  --arch mbart_large --layernorm-embedding \\\n",
        "  --task translation_from_pretrained_bart \\\n",
        "  --source-lang vi --target-lang en \\\n",
        "  --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
        "  --dataset-impl mmap \\\n",
        "  --optimizer adam --adam-eps 1e-06 --adam-betas '(0.9, 0.98)' \\\n",
        "  --lr-scheduler polynomial_decay --lr 5e-05 --warmup-updates 2500 --total-num-update 40000 \\\n",
        "  --dropout 0.3 --attention-dropout 0.1 --weight-decay 0.0 \\\n",
        "  --max-tokens 4000 --update-freq 2 \\\n",
        "  --save-interval 1 --save-interval-updates 1000 --keep-interval-updates 10 --no-epoch-checkpoints \\\n",
        "  --seed 222 --log-format simple --log-interval 2 \\\n",
        "  --restore-file model/model.pt \\\n",
        "  --reset-optimizer --reset-meters --reset-dataloader --reset-lr-scheduler \\\n",
        "  --langs ar_AR,cs_CZ,de_DE,en_XX,es_XX,et_EE,fi_FI,fr_XX,gu_IN,hi_IN,it_IT,ja_XX,kk_KZ,ko_KR,lt_LT,lv_LV,my_MM,ne_NP,nl_XX,ro_RO,ru_RU,si_LK,tr_TR,vi_VN,zh_CN \\\n",
        "  --memory-efficient-fp16\n",
        "  # --ddp-backend no_c10d \\\n",
        "  --wandb-project \"finetune mBart\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e53WkIZTVx7R",
        "outputId": "40c008c7-7c47-441e-a028-1d8674429dfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-08-16 08:58:48.615996: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-08-16 08:58:49 | INFO | numexpr.utils | NumExpr defaulting to 4 threads.\n",
            "2023-08-16 08:58:50 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2023-08-16 08:58:52 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 2, 'log_format': 'simple', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 222, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [2], 'lr': [5e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'model/model.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': True, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 1000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=2, log_format='simple', log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=222, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=True, memory_efficient_fp16=True, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='polynomial_decay', scoring='bleu', task='translation_from_pretrained_bart', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=4000, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl='mmap', data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=4000, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='mbart_large', max_epoch=0, max_update=0, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[2], lr=[5e-05], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='checkpoints', restore_file='model/model.pt', continue_once=None, finetune_from_model=None, reset_dataloader=True, reset_lr_scheduler=True, reset_meters=True, reset_optimizer=True, optimizer_overrides='{}', save_interval=1, save_interval_updates=1000, keep_interval_updates=10, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, data='bin', source_lang='vi', target_lang='en', load_alignments=False, left_pad_source=True, left_pad_target=False, upsample_primary=-1, truncate_source=False, num_batch_buckets=0, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_print_samples=False, langs='ar_AR,cs_CZ,de_DE,en_XX,es_XX,et_EE,fi_FI,fr_XX,gu_IN,hi_IN,it_IT,ja_XX,kk_KZ,ko_KR,lt_LT,lv_LV,my_MM,ne_NP,nl_XX,ro_RO,ru_RU,si_LK,tr_TR,vi_VN,zh_CN', prepend_bos=False, label_smoothing=0.1, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-06, weight_decay=0.0, use_old_adam=False, fp16_adam_stats=False, warmup_updates=2500, force_anneal=None, end_learning_rate=0.0, power=1.0, total_num_update='40000', pad=1, eos=2, unk=3, encoder_normalize_before=True, decoder_normalize_before=True, layernorm_embedding=True, dropout=0.3, attention_dropout=0.1, no_seed_provided=False, no_scale_embedding=False, encoder_embed_path=None, encoder_embed_dim=1024, encoder_ffn_embed_dim=4096, encoder_layers=12, encoder_attention_heads=16, encoder_learned_pos=True, decoder_embed_path=None, decoder_embed_dim=1024, decoder_ffn_embed_dim=4096, decoder_layers=12, decoder_attention_heads=16, decoder_learned_pos=True, relu_dropout=0.0, max_target_positions=1024, max_source_positions=1024, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, share_decoder_input_output_embed=True, share_all_embeddings=True, decoder_output_dim=1024, decoder_input_dim=1024, activation_fn='gelu', pooler_activation_fn='tanh', pooler_dropout=0.0, _name='mbart_large'), 'task': Namespace(no_progress_bar=False, log_interval=2, log_format='simple', log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=222, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=True, memory_efficient_fp16=True, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='polynomial_decay', scoring='bleu', task='translation_from_pretrained_bart', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=4000, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl='mmap', data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=4000, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='mbart_large', max_epoch=0, max_update=0, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[2], lr=[5e-05], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='checkpoints', restore_file='model/model.pt', continue_once=None, finetune_from_model=None, reset_dataloader=True, reset_lr_scheduler=True, reset_meters=True, reset_optimizer=True, optimizer_overrides='{}', save_interval=1, save_interval_updates=1000, keep_interval_updates=10, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, data='bin', source_lang='vi', target_lang='en', load_alignments=False, left_pad_source=True, left_pad_target=False, upsample_primary=-1, truncate_source=False, num_batch_buckets=0, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_print_samples=False, langs='ar_AR,cs_CZ,de_DE,en_XX,es_XX,et_EE,fi_FI,fr_XX,gu_IN,hi_IN,it_IT,ja_XX,kk_KZ,ko_KR,lt_LT,lv_LV,my_MM,ne_NP,nl_XX,ro_RO,ru_RU,si_LK,tr_TR,vi_VN,zh_CN', prepend_bos=False, label_smoothing=0.1, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-06, weight_decay=0.0, use_old_adam=False, fp16_adam_stats=False, warmup_updates=2500, force_anneal=None, end_learning_rate=0.0, power=1.0, total_num_update='40000', pad=1, eos=2, unk=3, encoder_normalize_before=True, decoder_normalize_before=True, layernorm_embedding=True, dropout=0.3, attention_dropout=0.1, no_seed_provided=False, no_scale_embedding=False, encoder_embed_path=None, encoder_embed_dim=1024, encoder_ffn_embed_dim=4096, encoder_layers=12, encoder_attention_heads=16, encoder_learned_pos=True, decoder_embed_path=None, decoder_embed_dim=1024, decoder_ffn_embed_dim=4096, decoder_layers=12, decoder_attention_heads=16, decoder_learned_pos=True, relu_dropout=0.0, max_target_positions=1024, max_source_positions=1024, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, share_decoder_input_output_embed=True, share_all_embeddings=True, decoder_output_dim=1024, decoder_input_dim=1024, activation_fn='gelu', pooler_activation_fn='tanh', pooler_dropout=0.0, _name='translation_from_pretrained_bart'), 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [5e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 2500, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 40000.0, 'lr': [5e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
            "2023-08-16 08:58:52 | INFO | fairseq.tasks.translation | [vi] dictionary: 41630 types\n",
            "2023-08-16 08:58:52 | INFO | fairseq.tasks.translation | [en] dictionary: 41630 types\n",
            "2023-08-16 08:59:01 | INFO | fairseq_cli.train | BARTModel(\n",
            "  (encoder): TransformerEncoderBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(41656, 1024, padding_idx=1)\n",
            "    (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)\n",
            "    (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "    (layers): ModuleList(\n",
            "      (0-11): 12 x TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (decoder): TransformerDecoderBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(41656, 1024, padding_idx=1)\n",
            "    (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)\n",
            "    (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "    (layers): ModuleList(\n",
            "      (0-11): 12 x TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "    (output_projection): Linear(in_features=1024, out_features=41656, bias=False)\n",
            "  )\n",
            "  (classification_heads): ModuleDict()\n",
            ")\n",
            "2023-08-16 08:59:01 | INFO | fairseq_cli.train | task: TranslationFromPretrainedBARTTask\n",
            "2023-08-16 08:59:01 | INFO | fairseq_cli.train | model: BARTModel\n",
            "2023-08-16 08:59:01 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\n",
            "2023-08-16 08:59:01 | INFO | fairseq_cli.train | num. shared model params: 548,585,472 (num. trained: 548,585,472)\n",
            "2023-08-16 08:59:01 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
            "2023-08-16 08:59:01 | INFO | fairseq.data.data_utils | loaded 114,483 examples from: bin/valid.vi-en.vi\n",
            "2023-08-16 08:59:01 | INFO | fairseq.data.data_utils | loaded 114,483 examples from: bin/valid.vi-en.en\n",
            "2023-08-16 08:59:01 | INFO | fairseq.tasks.translation | bin valid vi-en 114483 examples\n",
            "2023-08-16 08:59:09 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight\n",
            "2023-08-16 08:59:09 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight\n",
            "2023-08-16 08:59:09 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2023-08-16 08:59:09 | INFO | fairseq.utils | rank   0: capabilities =  7.0  ; total memory = 15.772 GB ; name = Tesla V100-SXM2-16GB                    \n",
            "2023-08-16 08:59:09 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2023-08-16 08:59:09 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2023-08-16 08:59:09 | INFO | fairseq_cli.train | max tokens per device = 4000 and max sentences per device = None\n",
            "2023-08-16 08:59:09 | INFO | fairseq.trainer | Preparing to load checkpoint model/model.pt\n",
            "2023-08-16 08:59:40 | INFO | fairseq.trainer | Loaded checkpoint model/model.pt (epoch 142 @ 0 updates)\n",
            "2023-08-16 08:59:40 | INFO | fairseq.trainer | loading train data for epoch 1\n",
            "2023-08-16 08:59:40 | INFO | fairseq.data.data_utils | loaded 1,500,000 examples from: bin/train.vi-en.vi\n",
            "2023-08-16 08:59:41 | INFO | fairseq.data.data_utils | loaded 1,500,000 examples from: bin/train.vi-en.en\n",
            "2023-08-16 08:59:41 | INFO | fairseq.tasks.translation | bin train vi-en 1500000 examples\n",
            "2023-08-16 08:59:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 6299\n",
            "2023-08-16 08:59:42 | INFO | fairseq.trainer | begin training epoch 1\n",
            "2023-08-16 08:59:42 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/fairseq/utils.py:374: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library\n",
            "  warnings.warn(\n",
            "2023-08-16 08:59:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0\n",
            "2023-08-16 08:59:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0\n",
            "2023-08-16 08:59:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
            "2023-08-16 08:59:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0\n",
            "2023-08-16 08:59:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0\n",
            "2023-08-16 08:59:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0\n",
            "2023-08-16 08:59:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0\n",
            "2023-08-16 08:59:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.5\n",
            "2023-08-16 08:59:51 | INFO | train_inner | epoch 001:     10 / 6299 loss=22.559, nll_loss=20.769, ppl=1.78726e+06, wps=4514.1, ups=0.73, wpb=6204, bsz=192, num_updates=2, lr=4e-08, gnorm=119.304, loss_scale=0.5, train_wall=9, gb_free=4.5, wall=42\n",
            "2023-08-16 08:59:52 | INFO | train_inner | epoch 001:     12 / 6299 loss=22.434, nll_loss=20.549, ppl=1.53378e+06, wps=12990.1, ups=1.88, wpb=6904, bsz=396, num_updates=4, lr=8e-08, gnorm=129.259, loss_scale=0.5, train_wall=1, gb_free=5.5, wall=43\n",
            "2023-08-16 08:59:53 | INFO | train_inner | epoch 001:     14 / 6299 loss=22.861, nll_loss=21.004, ppl=2.10234e+06, wps=12214.1, ups=1.86, wpb=6560, bsz=448, num_updates=6, lr=1.2e-07, gnorm=86.336, loss_scale=0.5, train_wall=1, gb_free=5.5, wall=44\n",
            "2023-08-16 08:59:54 | INFO | train_inner | epoch 001:     16 / 6299 loss=22.158, nll_loss=20.325, ppl=1.3136e+06, wps=13957.5, ups=2.12, wpb=6589.5, bsz=184, num_updates=8, lr=1.6e-07, gnorm=95.188, loss_scale=0.5, train_wall=1, gb_free=4.7, wall=45\n",
            "2023-08-16 08:59:55 | INFO | train_inner | epoch 001:     18 / 6299 loss=22.499, nll_loss=20.678, ppl=1.67795e+06, wps=14232, ups=2.16, wpb=6596, bsz=272, num_updates=10, lr=2e-07, gnorm=103.956, loss_scale=0.5, train_wall=1, gb_free=5.1, wall=46\n",
            "2023-08-16 08:59:56 | INFO | train_inner | epoch 001:     20 / 6299 loss=22.282, nll_loss=20.496, ppl=1.47853e+06, wps=13554, ups=2.29, wpb=5917.5, bsz=164, num_updates=12, lr=2.4e-07, gnorm=108.709, loss_scale=0.5, train_wall=1, gb_free=4.3, wall=47\n",
            "2023-08-16 08:59:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.25\n",
            "2023-08-16 08:59:57 | INFO | train_inner | epoch 001:     23 / 6299 loss=22.234, nll_loss=20.436, ppl=1.41811e+06, wps=9851.5, ups=1.5, wpb=6560, bsz=176, num_updates=14, lr=2.8e-07, gnorm=112.237, loss_scale=0.25, train_wall=1, gb_free=4.9, wall=48\n",
            "2023-08-16 08:59:58 | INFO | train_inner | epoch 001:     25 / 6299 loss=22.163, nll_loss=20.281, ppl=1.2742e+06, wps=15168.6, ups=2.19, wpb=6916, bsz=360, num_updates=16, lr=3.2e-07, gnorm=111.498, loss_scale=0.25, train_wall=1, gb_free=5.5, wall=49\n",
            "2023-08-16 08:59:59 | INFO | train_inner | epoch 001:     27 / 6299 loss=22.602, nll_loss=20.722, ppl=1.72925e+06, wps=14181.8, ups=2.06, wpb=6886.5, bsz=440, num_updates=18, lr=3.6e-07, gnorm=109.035, loss_scale=0.25, train_wall=1, gb_free=4.9, wall=50\n",
            "2023-08-16 09:00:00 | INFO | train_inner | epoch 001:     29 / 6299 loss=22.561, nll_loss=20.749, ppl=1.76252e+06, wps=12846.1, ups=2.04, wpb=6299, bsz=304, num_updates=20, lr=4e-07, gnorm=97.005, loss_scale=0.25, train_wall=1, gb_free=3.4, wall=51\n",
            "2023-08-16 09:00:01 | INFO | train_inner | epoch 001:     31 / 6299 loss=22.435, nll_loss=20.541, ppl=1.52584e+06, wps=15527.4, ups=2.11, wpb=7360, bsz=400, num_updates=22, lr=4.4e-07, gnorm=95.612, loss_scale=0.25, train_wall=1, gb_free=5.1, wall=52\n",
            "2023-08-16 09:00:02 | INFO | train_inner | epoch 001:     33 / 6299 loss=22.641, nll_loss=20.854, ppl=1.89554e+06, wps=12639.6, ups=2.01, wpb=6304, bsz=248, num_updates=24, lr=4.8e-07, gnorm=98.762, loss_scale=0.25, train_wall=1, gb_free=5.8, wall=53\n",
            "2023-08-16 09:00:03 | INFO | train_inner | epoch 001:     35 / 6299 loss=22.166, nll_loss=20.32, ppl=1.30907e+06, wps=14085, ups=1.99, wpb=7063, bsz=252, num_updates=26, lr=5.2e-07, gnorm=152.576, loss_scale=0.25, train_wall=1, gb_free=4.5, wall=54\n",
            "2023-08-16 09:00:04 | INFO | train_inner | epoch 001:     37 / 6299 loss=22.233, nll_loss=20.393, ppl=1.37706e+06, wps=13459.5, ups=2.02, wpb=6675.5, bsz=216, num_updates=28, lr=5.6e-07, gnorm=102.135, loss_scale=0.25, train_wall=1, gb_free=4.5, wall=55\n",
            "2023-08-16 09:00:05 | INFO | train_inner | epoch 001:     39 / 6299 loss=22.261, nll_loss=20.42, ppl=1.40283e+06, wps=12245.9, ups=2.21, wpb=5529.5, bsz=192, num_updates=30, lr=6e-07, gnorm=97.043, loss_scale=0.25, train_wall=1, gb_free=4.7, wall=56\n",
            "2023-08-16 09:00:06 | INFO | train_inner | epoch 001:     41 / 6299 loss=21.859, nll_loss=20.005, ppl=1.05196e+06, wps=13400.5, ups=2.16, wpb=6201.5, bsz=224, num_updates=32, lr=6.4e-07, gnorm=86.11, loss_scale=0.25, train_wall=1, gb_free=4.9, wall=57\n",
            "2023-08-16 09:00:07 | INFO | train_inner | epoch 001:     43 / 6299 loss=21.776, nll_loss=19.917, ppl=989764, wps=14598.5, ups=2.04, wpb=7139.5, bsz=364, num_updates=34, lr=6.8e-07, gnorm=125.975, loss_scale=0.25, train_wall=1, gb_free=4.8, wall=58\n",
            "2023-08-16 09:00:08 | INFO | train_inner | epoch 001:     45 / 6299 loss=21.989, nll_loss=20.193, ppl=1.199e+06, wps=13720.2, ups=2.05, wpb=6706, bsz=128, num_updates=36, lr=7.2e-07, gnorm=101.33, loss_scale=0.25, train_wall=1, gb_free=4.2, wall=59\n",
            "2023-08-16 09:00:09 | INFO | train_inner | epoch 001:     47 / 6299 loss=21.829, nll_loss=19.949, ppl=1.01205e+06, wps=13940, ups=1.96, wpb=7114.5, bsz=304, num_updates=38, lr=7.6e-07, gnorm=117.707, loss_scale=0.25, train_wall=1, gb_free=5, wall=60\n",
            "2023-08-16 09:00:10 | INFO | train_inner | epoch 001:     49 / 6299 loss=21.832, nll_loss=19.979, ppl=1.03343e+06, wps=13473.5, ups=2.06, wpb=6548.5, bsz=196, num_updates=40, lr=8e-07, gnorm=106.928, loss_scale=0.25, train_wall=1, gb_free=4.9, wall=61\n",
            "2023-08-16 09:00:10 | INFO | train_inner | epoch 001:     51 / 6299 loss=21.742, nll_loss=19.848, ppl=943481, wps=14285.7, ups=2.08, wpb=6873, bsz=336, num_updates=42, lr=8.4e-07, gnorm=123.421, loss_scale=0.25, train_wall=1, gb_free=4.4, wall=61\n",
            "2023-08-16 09:00:11 | INFO | train_inner | epoch 001:     53 / 6299 loss=22.311, nll_loss=20.495, ppl=1.47753e+06, wps=13479.1, ups=2.07, wpb=6516, bsz=216, num_updates=44, lr=8.8e-07, gnorm=90.021, loss_scale=0.25, train_wall=1, gb_free=4.7, wall=62\n",
            "2023-08-16 09:00:12 | INFO | train_inner | epoch 001:     55 / 6299 loss=21.726, nll_loss=19.893, ppl=973331, wps=14538, ups=2.05, wpb=7084, bsz=244, num_updates=46, lr=9.2e-07, gnorm=116.401, loss_scale=0.25, train_wall=1, gb_free=4.6, wall=63\n",
            "2023-08-16 09:00:13 | INFO | train_inner | epoch 001:     57 / 6299 loss=22.114, nll_loss=20.267, ppl=1.26216e+06, wps=12456.8, ups=2.17, wpb=5738, bsz=212, num_updates=48, lr=9.6e-07, gnorm=143.064, loss_scale=0.25, train_wall=1, gb_free=4.6, wall=64\n",
            "2023-08-16 09:00:14 | INFO | train_inner | epoch 001:     59 / 6299 loss=21.985, nll_loss=20.166, ppl=1.17682e+06, wps=13209.2, ups=2.03, wpb=6492, bsz=304, num_updates=50, lr=1e-06, gnorm=93.697, loss_scale=0.25, train_wall=1, gb_free=5, wall=65\n",
            "2023-08-16 09:00:15 | INFO | train_inner | epoch 001:     61 / 6299 loss=21.132, nll_loss=19.295, ppl=643282, wps=13818.5, ups=2.1, wpb=6579.5, bsz=152, num_updates=52, lr=1.04e-06, gnorm=130.477, loss_scale=0.25, train_wall=1, gb_free=4.7, wall=66\n",
            "2023-08-16 09:00:16 | INFO | train_inner | epoch 001:     63 / 6299 loss=21.772, nll_loss=19.84, ppl=938600, wps=13411.9, ups=2.03, wpb=6609, bsz=420, num_updates=54, lr=1.08e-06, gnorm=106.21, loss_scale=0.25, train_wall=1, gb_free=4.9, wall=67\n",
            "2023-08-16 09:00:17 | INFO | train_inner | epoch 001:     65 / 6299 loss=21.342, nll_loss=19.476, ppl=729143, wps=14354.3, ups=1.97, wpb=7273, bsz=284, num_updates=56, lr=1.12e-06, gnorm=145.733, loss_scale=0.25, train_wall=1, gb_free=4.3, wall=68\n",
            "2023-08-16 09:00:18 | INFO | train_inner | epoch 001:     67 / 6299 loss=25.69, nll_loss=24.329, ppl=2.10792e+07, wps=12910.7, ups=2.25, wpb=5726.5, bsz=172, num_updates=58, lr=1.16e-06, gnorm=162.052, loss_scale=0.25, train_wall=1, gb_free=5.4, wall=69\n",
            "2023-08-16 09:00:19 | INFO | train_inner | epoch 001:     69 / 6299 loss=21.356, nll_loss=19.51, ppl=746787, wps=14026.2, ups=2.02, wpb=6960, bsz=216, num_updates=60, lr=1.2e-06, gnorm=120.531, loss_scale=0.25, train_wall=1, gb_free=5.3, wall=70\n",
            "2023-08-16 09:00:20 | INFO | train_inner | epoch 001:     71 / 6299 loss=20.826, nll_loss=19.016, ppl=530263, wps=13214.9, ups=2.13, wpb=6198, bsz=156, num_updates=62, lr=1.24e-06, gnorm=108.8, loss_scale=0.25, train_wall=1, gb_free=4.5, wall=71\n",
            "2023-08-16 09:00:21 | INFO | train_inner | epoch 001:     73 / 6299 loss=21.008, nll_loss=19.239, ppl=618935, wps=13152.6, ups=2.16, wpb=6080, bsz=156, num_updates=64, lr=1.28e-06, gnorm=100.508, loss_scale=0.25, train_wall=1, gb_free=5.1, wall=72\n",
            "2023-08-16 09:00:22 | INFO | train_inner | epoch 001:     75 / 6299 loss=20.885, nll_loss=19.031, ppl=535616, wps=14895.7, ups=2.08, wpb=7154.5, bsz=212, num_updates=66, lr=1.32e-06, gnorm=85.229, loss_scale=0.25, train_wall=1, gb_free=4.9, wall=73\n",
            "2023-08-16 09:00:23 | INFO | train_inner | epoch 001:     77 / 6299 loss=20.774, nll_loss=18.996, ppl=522935, wps=13404.5, ups=2.14, wpb=6272, bsz=144, num_updates=68, lr=1.36e-06, gnorm=114.301, loss_scale=0.25, train_wall=1, gb_free=4.5, wall=74\n",
            "2023-08-16 09:00:24 | INFO | train_inner | epoch 001:     79 / 6299 loss=20.304, nll_loss=18.501, ppl=370987, wps=13441.9, ups=2.23, wpb=6032.5, bsz=132, num_updates=70, lr=1.4e-06, gnorm=96.941, loss_scale=0.25, train_wall=1, gb_free=5.2, wall=75\n",
            "2023-08-16 09:00:25 | INFO | train_inner | epoch 001:     81 / 6299 loss=20.894, nll_loss=19.039, ppl=538503, wps=14568.5, ups=2.07, wpb=7048.5, bsz=344, num_updates=72, lr=1.44e-06, gnorm=86.452, loss_scale=0.25, train_wall=1, gb_free=4.6, wall=76\n",
            "2023-08-16 09:00:26 | INFO | train_inner | epoch 001:     83 / 6299 loss=20.326, nll_loss=18.53, ppl=378527, wps=13232.7, ups=2.07, wpb=6391.5, bsz=172, num_updates=74, lr=1.48e-06, gnorm=93.08, loss_scale=0.25, train_wall=1, gb_free=5.2, wall=77\n",
            "2023-08-16 09:00:27 | INFO | train_inner | epoch 001:     85 / 6299 loss=20.022, nll_loss=18.212, ppl=303551, wps=13873.2, ups=2.09, wpb=6631.5, bsz=132, num_updates=76, lr=1.52e-06, gnorm=126.259, loss_scale=0.25, train_wall=1, gb_free=4.5, wall=78\n",
            "2023-08-16 09:00:28 | INFO | train_inner | epoch 001:     87 / 6299 loss=20.344, nll_loss=18.506, ppl=372183, wps=13895.5, ups=1.96, wpb=7092, bsz=292, num_updates=78, lr=1.56e-06, gnorm=76.803, loss_scale=0.25, train_wall=1, gb_free=4.4, wall=79\n",
            "2023-08-16 09:00:29 | INFO | train_inner | epoch 001:     89 / 6299 loss=20.514, nll_loss=18.722, ppl=432358, wps=12067.6, ups=2.09, wpb=5772, bsz=264, num_updates=80, lr=1.6e-06, gnorm=100.86, loss_scale=0.25, train_wall=1, gb_free=5.4, wall=80\n",
            "2023-08-16 09:00:30 | INFO | train_inner | epoch 001:     91 / 6299 loss=20.347, nll_loss=18.498, ppl=370227, wps=13537.7, ups=2.03, wpb=6667.5, bsz=328, num_updates=82, lr=1.64e-06, gnorm=93.511, loss_scale=0.25, train_wall=1, gb_free=4.5, wall=81\n",
            "2023-08-16 09:00:31 | INFO | train_inner | epoch 001:     93 / 6299 loss=19.267, nll_loss=17.402, ppl=173239, wps=14168.5, ups=1.99, wpb=7112, bsz=176, num_updates=84, lr=1.68e-06, gnorm=105.757, loss_scale=0.25, train_wall=1, gb_free=4.7, wall=82\n",
            "2023-08-16 09:00:32 | INFO | train_inner | epoch 001:     95 / 6299 loss=19.534, nll_loss=17.726, ppl=216745, wps=13367.3, ups=2.14, wpb=6247, bsz=168, num_updates=86, lr=1.72e-06, gnorm=78.475, loss_scale=0.25, train_wall=1, gb_free=5.6, wall=83\n",
            "2023-08-16 09:00:33 | INFO | train_inner | epoch 001:     97 / 6299 loss=19.805, nll_loss=17.958, ppl=254666, wps=14142, ups=2.05, wpb=6896.5, bsz=280, num_updates=88, lr=1.76e-06, gnorm=82.147, loss_scale=0.25, train_wall=1, gb_free=4.9, wall=84\n",
            "2023-08-16 09:00:34 | INFO | train_inner | epoch 001:     99 / 6299 loss=19.465, nll_loss=17.669, ppl=208445, wps=13302.1, ups=2.1, wpb=6340, bsz=204, num_updates=90, lr=1.8e-06, gnorm=95.298, loss_scale=0.25, train_wall=1, gb_free=5.4, wall=85\n",
            "2023-08-16 09:00:34 | INFO | train_inner | epoch 001:    101 / 6299 loss=19.296, nll_loss=17.481, ppl=182995, wps=13298.7, ups=2.13, wpb=6252, bsz=232, num_updates=92, lr=1.84e-06, gnorm=89.645, loss_scale=0.25, train_wall=1, gb_free=4.8, wall=85\n",
            "2023-08-16 09:00:35 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 246.00 MiB (GPU 0; 15.77 GiB total capacity; 14.09 GiB already allocated; 76.12 MiB free; 14.44 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2023-08-16 09:00:35 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1            |        cudaMalloc retries: 4         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  14422 MiB |  14435 MiB |   6088 GiB |   6074 GiB |\n",
            "|       from large pool |  14415 MiB |  14427 MiB |   6046 GiB |   6032 GiB |\n",
            "|       from small pool |      7 MiB |      8 MiB |     42 GiB |     42 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  14422 MiB |  14435 MiB |   6088 GiB |   6074 GiB |\n",
            "|       from large pool |  14415 MiB |  14427 MiB |   6046 GiB |   6032 GiB |\n",
            "|       from small pool |      7 MiB |      8 MiB |     42 GiB |     42 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  14372 MiB |  14384 MiB |   6039 GiB |   6025 GiB |\n",
            "|       from large pool |  14365 MiB |  14377 MiB |   5997 GiB |   5983 GiB |\n",
            "|       from small pool |      7 MiB |      8 MiB |     42 GiB |     42 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14790 MiB |  14842 MiB |  85866 MiB |  71076 MiB |\n",
            "|       from large pool |  14782 MiB |  14832 MiB |  85674 MiB |  70892 MiB |\n",
            "|       from small pool |      8 MiB |    168 MiB |    192 MiB |    184 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 375847 KiB |   3549 MiB |   5788 GiB |   5788 GiB |\n",
            "|       from large pool | 375035 KiB |   3548 MiB |   5745 GiB |   5744 GiB |\n",
            "|       from small pool |    812 KiB |      2 MiB |     43 GiB |     43 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    2756    |    2759    |     901 K  |     898 K  |\n",
            "|       from large pool |    1321    |    1323    |     510 K  |     508 K  |\n",
            "|       from small pool |    1435    |    1436    |     391 K  |     389 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    2756    |    2759    |     901 K  |     898 K  |\n",
            "|       from large pool |    1321    |    1323    |     510 K  |     508 K  |\n",
            "|       from small pool |    1435    |    1436    |     391 K  |     389 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     304    |     382    |    1677    |    1373    |\n",
            "|       from large pool |     300    |     301    |    1581    |    1281    |\n",
            "|       from small pool |       4    |      84    |      96    |      92    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |     233    |     233    |  460764    |  460531    |\n",
            "|       from large pool |     195    |     195    |  313280    |  313085    |\n",
            "|       from small pool |      38    |      39    |  147484    |  147446    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2023-08-16 09:00:35 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2023-08-16 09:00:36 | INFO | train_inner | epoch 001:    104 / 6299 loss=19.59, nll_loss=17.707, ppl=213976, wps=9534.1, ups=1.35, wpb=7080, bsz=404, num_updates=94, lr=1.88e-06, gnorm=90.659, loss_scale=0.25, train_wall=1, gb_free=1.7, wall=87\n",
            "2023-08-16 09:00:37 | INFO | train_inner | epoch 001:    106 / 6299 loss=17.924, nll_loss=16.071, ppl=68840.9, wps=15058.4, ups=2.2, wpb=6836, bsz=148, num_updates=96, lr=1.92e-06, gnorm=78.587, loss_scale=0.25, train_wall=1, gb_free=4.9, wall=88\n",
            "2023-08-16 09:00:38 | INFO | train_inner | epoch 001:    108 / 6299 loss=18.825, nll_loss=16.968, ppl=128193, wps=13856.3, ups=2.05, wpb=6748, bsz=252, num_updates=98, lr=1.96e-06, gnorm=80.286, loss_scale=0.25, train_wall=1, gb_free=5.6, wall=89\n",
            "2023-08-16 09:00:39 | INFO | train_inner | epoch 001:    110 / 6299 loss=24.185, nll_loss=23.023, ppl=8.52364e+06, wps=12409.7, ups=1.99, wpb=6234, bsz=124, num_updates=100, lr=2e-06, gnorm=109.238, loss_scale=0.25, train_wall=1, gb_free=4.6, wall=90\n",
            "2023-08-16 09:00:40 | INFO | train_inner | epoch 001:    112 / 6299 loss=19.623, nll_loss=17.768, ppl=223175, wps=14442.8, ups=2.06, wpb=7028, bsz=384, num_updates=102, lr=2.04e-06, gnorm=76.262, loss_scale=0.25, train_wall=1, gb_free=4.9, wall=91\n",
            "2023-08-16 09:00:41 | INFO | train_inner | epoch 001:    114 / 6299 loss=18.876, nll_loss=17.071, ppl=137710, wps=12202.8, ups=2.15, wpb=5663, bsz=244, num_updates=104, lr=2.08e-06, gnorm=126.583, loss_scale=0.25, train_wall=1, gb_free=5.4, wall=92\n",
            "2023-08-16 09:00:42 | INFO | train_inner | epoch 001:    116 / 6299 loss=18.116, nll_loss=16.342, ppl=83050, wps=13408.8, ups=2.12, wpb=6317, bsz=192, num_updates=106, lr=2.12e-06, gnorm=76.098, loss_scale=0.25, train_wall=1, gb_free=5.5, wall=93\n",
            "2023-08-16 09:00:43 | INFO | train_inner | epoch 001:    118 / 6299 loss=18.299, nll_loss=16.494, ppl=92309.9, wps=13126.1, ups=2.1, wpb=6261.5, bsz=200, num_updates=108, lr=2.16e-06, gnorm=110.955, loss_scale=0.25, train_wall=1, gb_free=4.7, wall=94\n",
            "2023-08-16 09:00:44 | INFO | train_inner | epoch 001:    120 / 6299 loss=17.445, nll_loss=15.667, ppl=52011.4, wps=13267.9, ups=2.12, wpb=6245.5, bsz=136, num_updates=110, lr=2.2e-06, gnorm=88.392, loss_scale=0.25, train_wall=1, gb_free=5.6, wall=95\n",
            "2023-08-16 09:00:45 | INFO | train_inner | epoch 001:    122 / 6299 loss=17.057, nll_loss=15.266, ppl=39409.8, wps=13195.3, ups=2.14, wpb=6155.5, bsz=104, num_updates=112, lr=2.24e-06, gnorm=82.618, loss_scale=0.25, train_wall=1, gb_free=5.5, wall=96\n",
            "2023-08-16 09:00:45 | INFO | train_inner | epoch 001:    124 / 6299 loss=18.161, nll_loss=16.327, ppl=82215.2, wps=13792.2, ups=2.13, wpb=6484, bsz=276, num_updates=114, lr=2.28e-06, gnorm=98.059, loss_scale=0.25, train_wall=1, gb_free=5, wall=96\n",
            "2023-08-16 09:00:46 | INFO | train_inner | epoch 001:    126 / 6299 loss=16.965, nll_loss=15.153, ppl=36441.3, wps=13536.4, ups=2.16, wpb=6256, bsz=160, num_updates=116, lr=2.32e-06, gnorm=59.643, loss_scale=0.25, train_wall=1, gb_free=5, wall=97\n",
            "2023-08-16 09:00:47 | INFO | train_inner | epoch 001:    128 / 6299 loss=16.942, nll_loss=15.164, ppl=36703.3, wps=12562.1, ups=2.23, wpb=5633, bsz=128, num_updates=118, lr=2.36e-06, gnorm=113.8, loss_scale=0.25, train_wall=1, gb_free=5.5, wall=98\n",
            "2023-08-16 09:00:48 | INFO | train_inner | epoch 001:    130 / 6299 loss=17.381, nll_loss=15.539, ppl=47595.1, wps=14124.7, ups=2.13, wpb=6625, bsz=276, num_updates=120, lr=2.4e-06, gnorm=58.671, loss_scale=0.25, train_wall=1, gb_free=5.1, wall=99\n",
            "2023-08-16 09:00:49 | INFO | train_inner | epoch 001:    132 / 6299 loss=17.313, nll_loss=15.438, ppl=44376.5, wps=14839.9, ups=2.13, wpb=6972.5, bsz=316, num_updates=122, lr=2.44e-06, gnorm=66.571, loss_scale=0.25, train_wall=1, gb_free=5.6, wall=100\n",
            "2023-08-16 09:00:50 | INFO | train_inner | epoch 001:    134 / 6299 loss=16.948, nll_loss=15.135, ppl=35993.3, wps=13632.3, ups=1.98, wpb=6892, bsz=236, num_updates=124, lr=2.48e-06, gnorm=56.111, loss_scale=0.25, train_wall=1, gb_free=4.6, wall=101\n",
            "2023-08-16 09:00:51 | INFO | train_inner | epoch 001:    136 / 6299 loss=17.733, nll_loss=15.871, ppl=59918.3, wps=13577, ups=2.03, wpb=6680, bsz=380, num_updates=126, lr=2.52e-06, gnorm=90.378, loss_scale=0.25, train_wall=1, gb_free=5.4, wall=102\n",
            "2023-08-16 09:00:52 | INFO | train_inner | epoch 001:    138 / 6299 loss=18.124, nll_loss=16.264, ppl=78675.1, wps=14102.1, ups=2.05, wpb=6888, bsz=560, num_updates=128, lr=2.56e-06, gnorm=72.013, loss_scale=0.25, train_wall=1, gb_free=5.3, wall=103\n",
            "2023-08-16 09:00:53 | INFO | train_inner | epoch 001:    140 / 6299 loss=15.824, nll_loss=14.012, ppl=16515.9, wps=14183.9, ups=2.11, wpb=6732, bsz=140, num_updates=130, lr=2.6e-06, gnorm=74.069, loss_scale=0.25, train_wall=1, gb_free=4.7, wall=104\n",
            "2023-08-16 09:00:54 | INFO | train_inner | epoch 001:    142 / 6299 loss=16.685, nll_loss=14.863, ppl=29801.4, wps=13404.4, ups=1.99, wpb=6724, bsz=264, num_updates=132, lr=2.64e-06, gnorm=50.81, loss_scale=0.25, train_wall=1, gb_free=5.2, wall=105\n",
            "2023-08-16 09:00:55 | INFO | train_inner | epoch 001:    144 / 6299 loss=17.23, nll_loss=15.369, ppl=42307.1, wps=12498.5, ups=2.01, wpb=6216.5, bsz=384, num_updates=134, lr=2.68e-06, gnorm=96.975, loss_scale=0.25, train_wall=1, gb_free=5.9, wall=106\n",
            "2023-08-16 09:00:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.125\n",
            "2023-08-16 09:00:57 | INFO | train_inner | epoch 001:    147 / 6299 loss=16.633, nll_loss=14.803, ppl=28584.2, wps=8897.5, ups=1.36, wpb=6529.5, bsz=304, num_updates=136, lr=2.72e-06, gnorm=47.269, loss_scale=0.125, train_wall=1, gb_free=5.1, wall=108\n",
            "2023-08-16 09:00:58 | INFO | train_inner | epoch 001:    149 / 6299 loss=16.096, nll_loss=14.259, ppl=19602.6, wps=13119.6, ups=1.89, wpb=6935.5, bsz=288, num_updates=138, lr=2.76e-06, gnorm=77.128, loss_scale=0.125, train_wall=1, gb_free=4.5, wall=109\n",
            "2023-08-16 09:00:59 | INFO | train_inner | epoch 001:    151 / 6299 loss=16.296, nll_loss=14.442, ppl=22260.5, wps=13310.7, ups=1.98, wpb=6728.5, bsz=332, num_updates=140, lr=2.8e-06, gnorm=99.343, loss_scale=0.125, train_wall=1, gb_free=5, wall=110\n",
            "2023-08-16 09:01:00 | INFO | train_inner | epoch 001:    153 / 6299 loss=15.794, nll_loss=13.971, ppl=16055.6, wps=14100.2, ups=2.12, wpb=6640.5, bsz=204, num_updates=142, lr=2.84e-06, gnorm=40.922, loss_scale=0.125, train_wall=1, gb_free=4.5, wall=111\n",
            "2023-08-16 09:01:00 | INFO | train_inner | epoch 001:    155 / 6299 loss=15.451, nll_loss=13.644, ppl=12801.2, wps=13789.7, ups=2.2, wpb=6269, bsz=176, num_updates=144, lr=2.88e-06, gnorm=38.381, loss_scale=0.125, train_wall=1, gb_free=5, wall=111\n",
            "2023-08-16 09:01:01 | INFO | train_inner | epoch 001:    157 / 6299 loss=15.966, nll_loss=14.171, ppl=18444.5, wps=13189.4, ups=2.09, wpb=6324, bsz=248, num_updates=146, lr=2.92e-06, gnorm=102.336, loss_scale=0.125, train_wall=1, gb_free=4.8, wall=112\n",
            "2023-08-16 09:01:02 | INFO | train_inner | epoch 001:    159 / 6299 loss=15.222, nll_loss=13.409, ppl=10877.8, wps=14124.9, ups=2.01, wpb=7034.5, bsz=168, num_updates=148, lr=2.96e-06, gnorm=57.617, loss_scale=0.125, train_wall=1, gb_free=4.9, wall=113\n",
            "2023-08-16 09:01:03 | INFO | train_inner | epoch 001:    161 / 6299 loss=15.789, nll_loss=13.979, ppl=16141.7, wps=13549.4, ups=2.08, wpb=6505.5, bsz=252, num_updates=150, lr=3e-06, gnorm=49.074, loss_scale=0.125, train_wall=1, gb_free=5.4, wall=114\n",
            "2023-08-16 09:01:04 | INFO | train_inner | epoch 001:    163 / 6299 loss=16.087, nll_loss=14.222, ppl=19108.6, wps=14651.8, ups=2.19, wpb=6702.5, bsz=388, num_updates=152, lr=3.04e-06, gnorm=48.847, loss_scale=0.125, train_wall=1, gb_free=5.4, wall=115\n",
            "2023-08-16 09:01:05 | INFO | train_inner | epoch 001:    165 / 6299 loss=15.464, nll_loss=13.639, ppl=12752.8, wps=14771.6, ups=2.02, wpb=7323.5, bsz=304, num_updates=154, lr=3.08e-06, gnorm=46.233, loss_scale=0.125, train_wall=1, gb_free=4.8, wall=116\n",
            "2023-08-16 09:01:06 | INFO | train_inner | epoch 001:    167 / 6299 loss=14.941, nll_loss=13.161, ppl=9158.4, wps=12912.5, ups=2.16, wpb=5976.5, bsz=132, num_updates=156, lr=3.12e-06, gnorm=33.143, loss_scale=0.125, train_wall=1, gb_free=5.2, wall=117\n",
            "2023-08-16 09:01:07 | INFO | train_inner | epoch 001:    169 / 6299 loss=15.618, nll_loss=13.799, ppl=14249.7, wps=13866.3, ups=2.06, wpb=6726.5, bsz=312, num_updates=158, lr=3.16e-06, gnorm=43.481, loss_scale=0.125, train_wall=1, gb_free=4.8, wall=118\n",
            "2023-08-16 09:01:08 | INFO | train_inner | epoch 001:    171 / 6299 loss=14.794, nll_loss=12.99, ppl=8136.91, wps=14073.4, ups=2.05, wpb=6858, bsz=176, num_updates=160, lr=3.2e-06, gnorm=33.001, loss_scale=0.125, train_wall=1, gb_free=4.3, wall=119\n",
            "2023-08-16 09:01:09 | INFO | train_inner | epoch 001:    173 / 6299 loss=14.956, nll_loss=13.161, ppl=9159.9, wps=13065.1, ups=2.02, wpb=6452, bsz=200, num_updates=162, lr=3.24e-06, gnorm=34.337, loss_scale=0.125, train_wall=1, gb_free=4.9, wall=120\n",
            "2023-08-16 09:01:10 | INFO | train_inner | epoch 001:    175 / 6299 loss=15.702, nll_loss=13.895, ppl=15237.8, wps=13374.2, ups=2.04, wpb=6560, bsz=396, num_updates=164, lr=3.28e-06, gnorm=30.044, loss_scale=0.125, train_wall=1, gb_free=4.9, wall=121\n",
            "2023-08-16 09:01:11 | INFO | train_inner | epoch 001:    177 / 6299 loss=14.611, nll_loss=12.84, ppl=7330.07, wps=13044.4, ups=2.16, wpb=6048, bsz=140, num_updates=166, lr=3.32e-06, gnorm=21.441, loss_scale=0.125, train_wall=1, gb_free=4.9, wall=122\n",
            "2023-08-16 09:01:12 | INFO | train_inner | epoch 001:    179 / 6299 loss=15.061, nll_loss=13.276, ppl=9922.15, wps=13596.2, ups=2.03, wpb=6692, bsz=268, num_updates=168, lr=3.36e-06, gnorm=106.303, loss_scale=0.125, train_wall=1, gb_free=5.1, wall=123\n",
            "2023-08-16 09:01:13 | INFO | train_inner | epoch 001:    181 / 6299 loss=15.664, nll_loss=13.858, ppl=14846.7, wps=13758.9, ups=1.98, wpb=6944, bsz=440, num_updates=170, lr=3.4e-06, gnorm=34.91, loss_scale=0.125, train_wall=1, gb_free=5.1, wall=124\n",
            "2023-08-16 09:01:14 | INFO | train_inner | epoch 001:    183 / 6299 loss=15.179, nll_loss=13.392, ppl=10746.2, wps=13265.4, ups=2.03, wpb=6533, bsz=320, num_updates=172, lr=3.44e-06, gnorm=34.306, loss_scale=0.125, train_wall=1, gb_free=5.3, wall=125\n",
            "2023-08-16 09:01:15 | INFO | train_inner | epoch 001:    185 / 6299 loss=15.095, nll_loss=13.311, ppl=10161, wps=13957.2, ups=2.09, wpb=6692, bsz=372, num_updates=174, lr=3.48e-06, gnorm=49.776, loss_scale=0.125, train_wall=1, gb_free=5.1, wall=126\n",
            "2023-08-16 09:01:16 | INFO | train_inner | epoch 001:    187 / 6299 loss=14.747, nll_loss=12.992, ppl=8145.14, wps=13400.6, ups=2.11, wpb=6362.5, bsz=216, num_updates=176, lr=3.52e-06, gnorm=28.075, loss_scale=0.125, train_wall=1, gb_free=5, wall=127\n",
            "2023-08-16 09:01:17 | INFO | train_inner | epoch 001:    189 / 6299 loss=14.577, nll_loss=12.845, ppl=7356.23, wps=13653.5, ups=2.16, wpb=6320, bsz=176, num_updates=178, lr=3.56e-06, gnorm=21.966, loss_scale=0.125, train_wall=1, gb_free=5.1, wall=128\n",
            "2023-08-16 09:01:18 | INFO | train_inner | epoch 001:    191 / 6299 loss=14.447, nll_loss=12.676, ppl=6544.62, wps=13714.9, ups=2.19, wpb=6267, bsz=192, num_updates=180, lr=3.6e-06, gnorm=34.901, loss_scale=0.125, train_wall=1, gb_free=4.9, wall=129\n",
            "2023-08-16 09:01:19 | INFO | train_inner | epoch 001:    193 / 6299 loss=14.987, nll_loss=13.314, ppl=10181, wps=11944.5, ups=2.21, wpb=5417, bsz=160, num_updates=182, lr=3.64e-06, gnorm=27.93, loss_scale=0.125, train_wall=1, gb_free=5.5, wall=130\n",
            "2023-08-16 09:01:20 | INFO | train_inner | epoch 001:    195 / 6299 loss=14.26, nll_loss=12.481, ppl=5717.19, wps=14612.2, ups=2.27, wpb=6430.5, bsz=184, num_updates=184, lr=3.68e-06, gnorm=32.861, loss_scale=0.125, train_wall=1, gb_free=4.3, wall=131\n",
            "2023-08-16 09:01:21 | INFO | train_inner | epoch 001:    197 / 6299 loss=14.798, nll_loss=13.021, ppl=8309.64, wps=13849.4, ups=1.99, wpb=6956, bsz=368, num_updates=186, lr=3.72e-06, gnorm=49.729, loss_scale=0.125, train_wall=1, gb_free=4.8, wall=132\n",
            "2023-08-16 09:01:22 | INFO | train_inner | epoch 001:    199 / 6299 loss=15.433, nll_loss=13.66, ppl=12943.2, wps=13472.1, ups=2.12, wpb=6364, bsz=460, num_updates=188, lr=3.76e-06, gnorm=49.908, loss_scale=0.125, train_wall=1, gb_free=5.2, wall=133\n",
            "2023-08-16 09:01:22 | INFO | train_inner | epoch 001:    201 / 6299 loss=14.313, nll_loss=12.573, ppl=6091.57, wps=13126.7, ups=2.1, wpb=6256.5, bsz=244, num_updates=190, lr=3.8e-06, gnorm=24.558, loss_scale=0.125, train_wall=1, gb_free=5.5, wall=133\n",
            "2023-08-16 09:01:23 | INFO | train_inner | epoch 001:    203 / 6299 loss=13.843, nll_loss=12.13, ppl=4482.83, wps=13086.1, ups=2.04, wpb=6407, bsz=124, num_updates=192, lr=3.84e-06, gnorm=17.419, loss_scale=0.125, train_wall=1, gb_free=4.6, wall=134\n",
            "2023-08-16 09:01:24 | INFO | train_inner | epoch 001:    205 / 6299 loss=14.022, nll_loss=12.261, ppl=4909.96, wps=14537.4, ups=2.09, wpb=6953.5, bsz=236, num_updates=194, lr=3.88e-06, gnorm=34.348, loss_scale=0.125, train_wall=1, gb_free=4.8, wall=135\n",
            "2023-08-16 09:01:25 | INFO | train_inner | epoch 001:    207 / 6299 loss=14.09, nll_loss=12.4, ppl=5404.86, wps=13080.2, ups=2.24, wpb=5828, bsz=156, num_updates=196, lr=3.92e-06, gnorm=18.264, loss_scale=0.125, train_wall=1, gb_free=5.3, wall=136\n",
            "2023-08-16 09:01:26 | INFO | train_inner | epoch 001:    209 / 6299 loss=14.967, nll_loss=13.184, ppl=9307.72, wps=14109.5, ups=1.99, wpb=7100, bsz=556, num_updates=198, lr=3.96e-06, gnorm=36.142, loss_scale=0.125, train_wall=1, gb_free=4.9, wall=137\n",
            "2023-08-16 09:01:27 | INFO | train_inner | epoch 001:    211 / 6299 loss=14.128, nll_loss=12.411, ppl=5447.31, wps=13901.5, ups=2.06, wpb=6745.5, bsz=256, num_updates=200, lr=4e-06, gnorm=31.884, loss_scale=0.125, train_wall=1, gb_free=4.6, wall=138\n",
            "2023-08-16 09:01:28 | INFO | train_inner | epoch 001:    213 / 6299 loss=13.823, nll_loss=12.105, ppl=4404.5, wps=13762.1, ups=2.15, wpb=6401, bsz=212, num_updates=202, lr=4.04e-06, gnorm=23.562, loss_scale=0.125, train_wall=1, gb_free=4.7, wall=139\n",
            "2023-08-16 09:01:29 | INFO | train_inner | epoch 001:    215 / 6299 loss=14.14, nll_loss=12.425, ppl=5498.13, wps=13745, ups=2.07, wpb=6632, bsz=296, num_updates=204, lr=4.08e-06, gnorm=24.929, loss_scale=0.125, train_wall=1, gb_free=5.4, wall=140\n",
            "2023-08-16 09:01:30 | INFO | train_inner | epoch 001:    217 / 6299 loss=13.7, nll_loss=12.008, ppl=4118.57, wps=14608.6, ups=2.14, wpb=6840, bsz=192, num_updates=206, lr=4.12e-06, gnorm=16.47, loss_scale=0.125, train_wall=1, gb_free=4.7, wall=141\n",
            "2023-08-16 09:01:31 | INFO | train_inner | epoch 001:    219 / 6299 loss=13.825, nll_loss=12.163, ppl=4585.14, wps=12371, ups=2.15, wpb=5762, bsz=180, num_updates=208, lr=4.16e-06, gnorm=20.348, loss_scale=0.125, train_wall=1, gb_free=5.9, wall=142\n",
            "2023-08-16 09:01:32 | INFO | train_inner | epoch 001:    221 / 6299 loss=14.708, nll_loss=13.024, ppl=8329.76, wps=13002.1, ups=2.21, wpb=5881.5, bsz=464, num_updates=210, lr=4.2e-06, gnorm=72.983, loss_scale=0.125, train_wall=1, gb_free=4.8, wall=143\n",
            "2023-08-16 09:01:33 | INFO | train_inner | epoch 001:    223 / 6299 loss=14.311, nll_loss=12.632, ppl=6345.95, wps=12086.4, ups=2.2, wpb=5499, bsz=384, num_updates=212, lr=4.24e-06, gnorm=58.89, loss_scale=0.125, train_wall=1, gb_free=6, wall=144\n",
            "2023-08-16 09:01:34 | INFO | train_inner | epoch 001:    225 / 6299 loss=13.75, nll_loss=12.095, ppl=4373.85, wps=13429, ups=2.11, wpb=6368, bsz=244, num_updates=214, lr=4.28e-06, gnorm=27.292, loss_scale=0.125, train_wall=1, gb_free=5.6, wall=145\n",
            "2023-08-16 09:01:35 | INFO | train_inner | epoch 001:    227 / 6299 loss=13.433, nll_loss=11.777, ppl=3509.27, wps=14358, ups=2.06, wpb=6979.5, bsz=184, num_updates=216, lr=4.32e-06, gnorm=17.458, loss_scale=0.125, train_wall=1, gb_free=4.7, wall=146\n",
            "2023-08-16 09:01:36 | INFO | train_inner | epoch 001:    229 / 6299 loss=13.879, nll_loss=12.229, ppl=4800.63, wps=12670.7, ups=2.04, wpb=6212, bsz=312, num_updates=218, lr=4.36e-06, gnorm=26.427, loss_scale=0.125, train_wall=1, gb_free=5.4, wall=147\n",
            "2023-08-16 09:01:37 | INFO | train_inner | epoch 001:    231 / 6299 loss=13.552, nll_loss=11.899, ppl=3817.97, wps=13534.1, ups=2, wpb=6779, bsz=232, num_updates=220, lr=4.4e-06, gnorm=37.522, loss_scale=0.125, train_wall=1, gb_free=5.1, wall=148\n",
            "2023-08-16 09:01:38 | INFO | train_inner | epoch 001:    233 / 6299 loss=13.191, nll_loss=11.553, ppl=3004.99, wps=14950.6, ups=2.09, wpb=7147.5, bsz=180, num_updates=222, lr=4.44e-06, gnorm=66.316, loss_scale=0.125, train_wall=1, gb_free=5.2, wall=149\n",
            "2023-08-16 09:01:39 | INFO | train_inner | epoch 001:    235 / 6299 loss=13.466, nll_loss=11.852, ppl=3697.16, wps=12554.9, ups=2.13, wpb=5897, bsz=220, num_updates=224, lr=4.48e-06, gnorm=18.661, loss_scale=0.125, train_wall=1, gb_free=4.6, wall=150\n",
            "2023-08-16 09:01:40 | INFO | train_inner | epoch 001:    237 / 6299 loss=13.817, nll_loss=12.201, ppl=4709.19, wps=12528.3, ups=2.12, wpb=5912, bsz=340, num_updates=226, lr=4.52e-06, gnorm=23.453, loss_scale=0.125, train_wall=1, gb_free=5.6, wall=151\n",
            "2023-08-16 09:01:41 | INFO | train_inner | epoch 001:    239 / 6299 loss=13.894, nll_loss=12.249, ppl=4866.26, wps=14950.5, ups=2.12, wpb=7037, bsz=436, num_updates=228, lr=4.56e-06, gnorm=24.859, loss_scale=0.125, train_wall=1, gb_free=5.3, wall=152\n",
            "2023-08-16 09:01:41 | INFO | train_inner | epoch 001:    241 / 6299 loss=13.028, nll_loss=11.435, ppl=2769.54, wps=13991.4, ups=2.15, wpb=6521.5, bsz=136, num_updates=230, lr=4.6e-06, gnorm=13.736, loss_scale=0.125, train_wall=1, gb_free=4.4, wall=152\n",
            "2023-08-16 09:01:42 | INFO | train_inner | epoch 001:    243 / 6299 loss=13.461, nll_loss=11.851, ppl=3695.25, wps=13588, ups=2.05, wpb=6628, bsz=328, num_updates=232, lr=4.64e-06, gnorm=14.787, loss_scale=0.125, train_wall=1, gb_free=5.2, wall=153\n",
            "2023-08-16 09:01:43 | INFO | train_inner | epoch 001:    245 / 6299 loss=13.232, nll_loss=11.66, ppl=3235.4, wps=13868.7, ups=2.12, wpb=6552, bsz=228, num_updates=234, lr=4.68e-06, gnorm=13.113, loss_scale=0.125, train_wall=1, gb_free=4.8, wall=154\n",
            "2023-08-16 09:01:44 | INFO | train_inner | epoch 001:    247 / 6299 loss=13.44, nll_loss=11.862, ppl=3722.29, wps=13676.5, ups=2.16, wpb=6344, bsz=312, num_updates=236, lr=4.72e-06, gnorm=19.972, loss_scale=0.125, train_wall=1, gb_free=5.7, wall=155\n",
            "2023-08-16 09:01:45 | INFO | train_inner | epoch 001:    249 / 6299 loss=13.06, nll_loss=11.518, ppl=2932.15, wps=14168.8, ups=2.1, wpb=6744, bsz=200, num_updates=238, lr=4.76e-06, gnorm=16.386, loss_scale=0.125, train_wall=1, gb_free=4.9, wall=156\n",
            "2023-08-16 09:01:46 | INFO | train_inner | epoch 001:    251 / 6299 loss=13.413, nll_loss=11.897, ppl=3813.28, wps=13605.6, ups=2.22, wpb=6115.5, bsz=196, num_updates=240, lr=4.8e-06, gnorm=38.313, loss_scale=0.125, train_wall=1, gb_free=5.3, wall=157\n",
            "2023-08-16 09:01:47 | INFO | train_inner | epoch 001:    253 / 6299 loss=12.963, nll_loss=11.446, ppl=2789.21, wps=13574.6, ups=2.16, wpb=6288, bsz=164, num_updates=242, lr=4.84e-06, gnorm=9.699, loss_scale=0.125, train_wall=1, gb_free=5, wall=158\n",
            "2023-08-16 09:01:48 | INFO | train_inner | epoch 001:    255 / 6299 loss=13.153, nll_loss=11.604, ppl=3112.32, wps=13280.7, ups=2.06, wpb=6434.5, bsz=260, num_updates=244, lr=4.88e-06, gnorm=23.174, loss_scale=0.125, train_wall=1, gb_free=5, wall=159\n",
            "2023-08-16 09:01:49 | INFO | train_inner | epoch 001:    257 / 6299 loss=12.974, nll_loss=11.451, ppl=2800.04, wps=14089.9, ups=1.97, wpb=7148, bsz=240, num_updates=246, lr=4.92e-06, gnorm=12.7, loss_scale=0.125, train_wall=1, gb_free=4.4, wall=160\n",
            "2023-08-16 09:01:50 | INFO | train_inner | epoch 001:    259 / 6299 loss=12.855, nll_loss=11.378, ppl=2660.72, wps=13179.5, ups=2.09, wpb=6295, bsz=148, num_updates=248, lr=4.96e-06, gnorm=12.668, loss_scale=0.125, train_wall=1, gb_free=4.7, wall=161\n",
            "2023-08-16 09:01:51 | INFO | train_inner | epoch 001:    261 / 6299 loss=12.829, nll_loss=11.359, ppl=2627.1, wps=13064.4, ups=2.15, wpb=6090, bsz=132, num_updates=250, lr=5e-06, gnorm=10.123, loss_scale=0.125, train_wall=1, gb_free=4.8, wall=162\n",
            "2023-08-16 09:01:52 | INFO | train_inner | epoch 001:    263 / 6299 loss=12.757, nll_loss=11.289, ppl=2502.29, wps=13459.9, ups=2.26, wpb=5965.5, bsz=144, num_updates=252, lr=5.04e-06, gnorm=9.214, loss_scale=0.125, train_wall=1, gb_free=4.6, wall=163\n",
            "2023-08-16 09:01:53 | INFO | train_inner | epoch 001:    265 / 6299 loss=12.723, nll_loss=11.253, ppl=2440.06, wps=13565.5, ups=2.06, wpb=6596, bsz=168, num_updates=254, lr=5.08e-06, gnorm=19.796, loss_scale=0.125, train_wall=1, gb_free=5, wall=164\n",
            "2023-08-16 09:01:54 | INFO | train_inner | epoch 001:    267 / 6299 loss=12.911, nll_loss=11.446, ppl=2790.51, wps=13320.5, ups=2.18, wpb=6120, bsz=224, num_updates=256, lr=5.12e-06, gnorm=12.068, loss_scale=0.125, train_wall=1, gb_free=5.4, wall=165\n",
            "2023-08-16 09:01:55 | INFO | train_inner | epoch 001:    269 / 6299 loss=12.924, nll_loss=11.438, ppl=2774.31, wps=13439.1, ups=2.13, wpb=6307, bsz=324, num_updates=258, lr=5.16e-06, gnorm=17.576, loss_scale=0.125, train_wall=1, gb_free=5, wall=166\n",
            "2023-08-16 09:01:56 | INFO | train_inner | epoch 001:    271 / 6299 loss=12.714, nll_loss=11.283, ppl=2492, wps=13519.7, ups=2.07, wpb=6533, bsz=208, num_updates=260, lr=5.2e-06, gnorm=10.186, loss_scale=0.125, train_wall=1, gb_free=5.6, wall=167\n",
            "2023-08-16 09:01:57 | INFO | train_inner | epoch 001:    273 / 6299 loss=12.672, nll_loss=11.233, ppl=2407.39, wps=14233.4, ups=1.97, wpb=7209.5, bsz=224, num_updates=262, lr=5.24e-06, gnorm=10.53, loss_scale=0.125, train_wall=1, gb_free=4.6, wall=168\n",
            "2023-08-16 09:01:58 | INFO | train_inner | epoch 001:    275 / 6299 loss=13.078, nll_loss=11.585, ppl=3071.8, wps=15198, ups=2.01, wpb=7549.5, bsz=476, num_updates=264, lr=5.28e-06, gnorm=20.967, loss_scale=0.125, train_wall=1, gb_free=4.4, wall=169\n",
            "2023-08-16 09:01:59 | INFO | train_inner | epoch 001:    277 / 6299 loss=12.665, nll_loss=11.256, ppl=2445.33, wps=10115.2, ups=1.61, wpb=6276, bsz=220, num_updates=266, lr=5.32e-06, gnorm=10.835, loss_scale=0.125, train_wall=1, gb_free=5.4, wall=170\n",
            "2023-08-16 09:02:00 | INFO | train_inner | epoch 001:    279 / 6299 loss=12.4, nll_loss=11, ppl=2047.66, wps=13878, ups=2.1, wpb=6606, bsz=172, num_updates=268, lr=5.36e-06, gnorm=16.401, loss_scale=0.125, train_wall=1, gb_free=4.7, wall=171\n",
            "2023-08-16 09:02:01 | INFO | train_inner | epoch 001:    281 / 6299 loss=12.673, nll_loss=11.284, ppl=2492.86, wps=12937.1, ups=2.09, wpb=6204, bsz=236, num_updates=270, lr=5.4e-06, gnorm=9.498, loss_scale=0.125, train_wall=1, gb_free=5.3, wall=172\n",
            "2023-08-16 09:02:02 | INFO | train_inner | epoch 001:    283 / 6299 loss=12.344, nll_loss=10.976, ppl=2014.36, wps=13540.3, ups=2.18, wpb=6197, bsz=120, num_updates=272, lr=5.44e-06, gnorm=7.778, loss_scale=0.125, train_wall=1, gb_free=5.6, wall=173\n",
            "2023-08-16 09:02:03 | INFO | train_inner | epoch 001:    285 / 6299 loss=12.598, nll_loss=11.18, ppl=2320.24, wps=13830.9, ups=2, wpb=6920, bsz=308, num_updates=274, lr=5.48e-06, gnorm=12.351, loss_scale=0.125, train_wall=1, gb_free=4.9, wall=174\n",
            "2023-08-16 09:02:04 | INFO | train_inner | epoch 001:    287 / 6299 loss=12.421, nll_loss=11.048, ppl=2117.65, wps=14142.7, ups=2.02, wpb=6988, bsz=212, num_updates=276, lr=5.52e-06, gnorm=8.056, loss_scale=0.125, train_wall=1, gb_free=4.9, wall=175\n",
            "2023-08-16 09:02:05 | INFO | train_inner | epoch 001:    289 / 6299 loss=12.364, nll_loss=11.032, ppl=2093.88, wps=12880.4, ups=2.18, wpb=5916.5, bsz=112, num_updates=278, lr=5.56e-06, gnorm=6.337, loss_scale=0.125, train_wall=1, gb_free=5.8, wall=176\n",
            "2023-08-16 09:02:06 | INFO | train_inner | epoch 001:    291 / 6299 loss=12.374, nll_loss=11.005, ppl=2054.72, wps=14751.8, ups=1.97, wpb=7488, bsz=252, num_updates=280, lr=5.6e-06, gnorm=19.675, loss_scale=0.125, train_wall=1, gb_free=4.4, wall=177\n",
            "2023-08-16 09:02:07 | INFO | train_inner | epoch 001:    293 / 6299 loss=12.356, nll_loss=10.987, ppl=2029.26, wps=14668.1, ups=2.09, wpb=7020, bsz=268, num_updates=282, lr=5.64e-06, gnorm=9.919, loss_scale=0.125, train_wall=1, gb_free=4.6, wall=178\n",
            "2023-08-16 09:02:08 | INFO | train_inner | epoch 001:    295 / 6299 loss=12.206, nll_loss=10.876, ppl=1879.35, wps=13741.2, ups=2.06, wpb=6683, bsz=148, num_updates=284, lr=5.68e-06, gnorm=15.765, loss_scale=0.125, train_wall=1, gb_free=4.4, wall=179\n",
            "2023-08-16 09:02:09 | INFO | train_inner | epoch 001:    297 / 6299 loss=12.445, nll_loss=11.077, ppl=2159.59, wps=14072.5, ups=2.15, wpb=6546, bsz=284, num_updates=286, lr=5.72e-06, gnorm=11.407, loss_scale=0.125, train_wall=1, gb_free=5.2, wall=180\n",
            "2023-08-16 09:02:10 | INFO | train_inner | epoch 001:    299 / 6299 loss=12.318, nll_loss=10.957, ppl=1987.32, wps=15021.9, ups=2.04, wpb=7357.5, bsz=320, num_updates=288, lr=5.76e-06, gnorm=10.951, loss_scale=0.125, train_wall=1, gb_free=5.1, wall=181\n",
            "2023-08-16 09:02:10 | INFO | train_inner | epoch 001:    301 / 6299 loss=12.581, nll_loss=11.192, ppl=2339.47, wps=15112, ups=2.14, wpb=7051.5, bsz=408, num_updates=290, lr=5.8e-06, gnorm=14.21, loss_scale=0.125, train_wall=1, gb_free=5.4, wall=181\n",
            "2023-08-16 09:02:11 | INFO | train_inner | epoch 001:    303 / 6299 loss=12.539, nll_loss=11.138, ppl=2253.24, wps=14715.5, ups=2.07, wpb=7122, bsz=484, num_updates=292, lr=5.84e-06, gnorm=34.531, loss_scale=0.125, train_wall=1, gb_free=5.3, wall=182\n",
            "2023-08-16 09:02:12 | INFO | train_inner | epoch 001:    305 / 6299 loss=12.263, nll_loss=10.971, ppl=2007.64, wps=12849.5, ups=2.11, wpb=6096, bsz=208, num_updates=294, lr=5.88e-06, gnorm=7.701, loss_scale=0.125, train_wall=1, gb_free=5.4, wall=183\n",
            "2023-08-16 09:02:13 | INFO | train_inner | epoch 001:    307 / 6299 loss=11.983, nll_loss=10.725, ppl=1692.39, wps=13519.3, ups=2.18, wpb=6190, bsz=112, num_updates=296, lr=5.92e-06, gnorm=5.898, loss_scale=0.125, train_wall=1, gb_free=5.2, wall=184\n",
            "2023-08-16 09:02:14 | INFO | train_inner | epoch 001:    309 / 6299 loss=12.247, nll_loss=10.907, ppl=1920.33, wps=14080.1, ups=1.98, wpb=7123.5, bsz=352, num_updates=298, lr=5.96e-06, gnorm=16.385, loss_scale=0.125, train_wall=1, gb_free=4.9, wall=185\n",
            "2023-08-16 09:02:15 | INFO | train_inner | epoch 001:    311 / 6299 loss=12.171, nll_loss=10.928, ppl=1948.78, wps=12860.4, ups=2.27, wpb=5668, bsz=140, num_updates=300, lr=6e-06, gnorm=9.87, loss_scale=0.125, train_wall=1, gb_free=5.3, wall=186\n",
            "2023-08-16 09:02:16 | INFO | train_inner | epoch 001:    313 / 6299 loss=12.103, nll_loss=10.797, ppl=1778.76, wps=14225.7, ups=2.08, wpb=6835.5, bsz=276, num_updates=302, lr=6.04e-06, gnorm=21.459, loss_scale=0.125, train_wall=1, gb_free=5.6, wall=187\n",
            "2023-08-16 09:02:17 | INFO | train_inner | epoch 001:    315 / 6299 loss=12.031, nll_loss=10.74, ppl=1709.7, wps=15392, ups=2.1, wpb=7346.5, bsz=296, num_updates=304, lr=6.08e-06, gnorm=6.93, loss_scale=0.125, train_wall=1, gb_free=4.8, wall=188\n",
            "2023-08-16 09:02:18 | INFO | train_inner | epoch 001:    317 / 6299 loss=12.176, nll_loss=10.951, ppl=1980.27, wps=12376.4, ups=2.16, wpb=5728.5, bsz=180, num_updates=306, lr=6.12e-06, gnorm=6.411, loss_scale=0.125, train_wall=1, gb_free=6.1, wall=189\n",
            "2023-08-16 09:02:19 | INFO | train_inner | epoch 001:    319 / 6299 loss=12.011, nll_loss=10.792, ppl=1773.09, wps=13472.7, ups=2.08, wpb=6484, bsz=156, num_updates=308, lr=6.16e-06, gnorm=5.149, loss_scale=0.125, train_wall=1, gb_free=5.2, wall=190\n",
            "2023-08-16 09:02:20 | INFO | train_inner | epoch 001:    321 / 6299 loss=12.117, nll_loss=10.909, ppl=1922.4, wps=13238.5, ups=2.15, wpb=6159.5, bsz=152, num_updates=310, lr=6.2e-06, gnorm=26.031, loss_scale=0.125, train_wall=1, gb_free=5.2, wall=191\n",
            "2023-08-16 09:02:21 | INFO | train_inner | epoch 001:    323 / 6299 loss=11.866, nll_loss=10.62, ppl=1573.87, wps=13705.4, ups=2.17, wpb=6327.5, bsz=200, num_updates=312, lr=6.24e-06, gnorm=7.787, loss_scale=0.125, train_wall=1, gb_free=5.3, wall=192\n",
            "2023-08-16 09:02:22 | INFO | train_inner | epoch 001:    325 / 6299 loss=12.132, nll_loss=10.823, ppl=1811.98, wps=14245.1, ups=2.08, wpb=6844, bsz=444, num_updates=314, lr=6.28e-06, gnorm=15.611, loss_scale=0.125, train_wall=1, gb_free=4.6, wall=193\n",
            "2023-08-16 09:02:23 | INFO | train_inner | epoch 001:    327 / 6299 loss=12.05, nll_loss=10.808, ppl=1793.05, wps=13338.2, ups=2.09, wpb=6372, bsz=260, num_updates=316, lr=6.32e-06, gnorm=7.975, loss_scale=0.125, train_wall=1, gb_free=5.5, wall=194\n",
            "2023-08-16 09:02:24 | INFO | train_inner | epoch 001:    329 / 6299 loss=12.197, nll_loss=10.912, ppl=1926.97, wps=13251.3, ups=2.19, wpb=6058.5, bsz=428, num_updates=318, lr=6.36e-06, gnorm=12.983, loss_scale=0.125, train_wall=1, gb_free=5.3, wall=195\n",
            "2023-08-16 09:02:25 | INFO | train_inner | epoch 001:    331 / 6299 loss=11.811, nll_loss=10.609, ppl=1562.23, wps=13698.9, ups=2.07, wpb=6606, bsz=136, num_updates=320, lr=6.4e-06, gnorm=4.038, loss_scale=0.125, train_wall=1, gb_free=4.9, wall=196\n",
            "2023-08-16 09:02:26 | INFO | train_inner | epoch 001:    333 / 6299 loss=11.969, nll_loss=10.761, ppl=1735.3, wps=13875.4, ups=2.01, wpb=6892, bsz=228, num_updates=322, lr=6.44e-06, gnorm=11.315, loss_scale=0.125, train_wall=1, gb_free=5.1, wall=197\n",
            "2023-08-16 09:02:27 | INFO | train_inner | epoch 001:    335 / 6299 loss=11.904, nll_loss=10.67, ppl=1629.07, wps=13755.3, ups=2.09, wpb=6590, bsz=268, num_updates=324, lr=6.48e-06, gnorm=7.557, loss_scale=0.125, train_wall=1, gb_free=4.5, wall=198\n",
            "2023-08-16 09:02:28 | INFO | train_inner | epoch 001:    337 / 6299 loss=11.867, nll_loss=10.694, ppl=1656.66, wps=12974.1, ups=2.08, wpb=6248, bsz=136, num_updates=326, lr=6.52e-06, gnorm=4.106, loss_scale=0.125, train_wall=1, gb_free=5, wall=199\n",
            "2023-08-16 09:02:29 | INFO | train_inner | epoch 001:    339 / 6299 loss=11.887, nll_loss=10.673, ppl=1632.43, wps=13546.7, ups=2.02, wpb=6721, bsz=244, num_updates=328, lr=6.56e-06, gnorm=6.809, loss_scale=0.125, train_wall=1, gb_free=5.2, wall=200\n",
            "2023-08-16 09:02:30 | INFO | train_inner | epoch 001:    341 / 6299 loss=11.905, nll_loss=10.679, ppl=1639.42, wps=14319.2, ups=2.01, wpb=7132, bsz=328, num_updates=330, lr=6.6e-06, gnorm=40.842, loss_scale=0.125, train_wall=1, gb_free=4.8, wall=201\n",
            "2023-08-16 09:02:30 | INFO | train_inner | epoch 001:    343 / 6299 loss=11.969, nll_loss=10.808, ppl=1792.92, wps=12233.7, ups=2.19, wpb=5574.5, bsz=188, num_updates=332, lr=6.64e-06, gnorm=5.49, loss_scale=0.125, train_wall=1, gb_free=5.3, wall=201\n",
            "2023-08-16 09:02:31 | INFO | train_inner | epoch 001:    345 / 6299 loss=11.823, nll_loss=10.631, ppl=1586.15, wps=14913.4, ups=2.13, wpb=7013.5, bsz=212, num_updates=334, lr=6.68e-06, gnorm=5.459, loss_scale=0.125, train_wall=1, gb_free=4.9, wall=202\n",
            "2023-08-16 09:02:32 | INFO | train_inner | epoch 001:    347 / 6299 loss=11.808, nll_loss=10.643, ppl=1599.36, wps=13316.9, ups=2.3, wpb=5784.5, bsz=148, num_updates=336, lr=6.72e-06, gnorm=6.277, loss_scale=0.125, train_wall=1, gb_free=5.9, wall=203\n",
            "2023-08-16 09:02:33 | INFO | train_inner | epoch 001:    349 / 6299 loss=11.77, nll_loss=10.622, ppl=1576.06, wps=12997.3, ups=2.07, wpb=6281.5, bsz=136, num_updates=338, lr=6.76e-06, gnorm=4.468, loss_scale=0.125, train_wall=1, gb_free=3.5, wall=204\n",
            "2023-08-16 09:02:34 | INFO | train_inner | epoch 001:    351 / 6299 loss=11.862, nll_loss=10.737, ppl=1706.59, wps=12636.1, ups=2.21, wpb=5729.5, bsz=120, num_updates=340, lr=6.8e-06, gnorm=3.964, loss_scale=0.125, train_wall=1, gb_free=5, wall=205\n",
            "2023-08-16 09:02:35 | INFO | train_inner | epoch 001:    353 / 6299 loss=11.762, nll_loss=10.621, ppl=1575.04, wps=13065.8, ups=2.16, wpb=6061, bsz=132, num_updates=342, lr=6.84e-06, gnorm=4.435, loss_scale=0.125, train_wall=1, gb_free=4.9, wall=206\n",
            "2023-08-16 09:02:36 | INFO | train_inner | epoch 001:    355 / 6299 loss=11.584, nll_loss=10.436, ppl=1384.87, wps=12173.8, ups=2.18, wpb=5579, bsz=120, num_updates=344, lr=6.88e-06, gnorm=4.393, loss_scale=0.125, train_wall=1, gb_free=4.4, wall=207\n",
            "2023-08-16 09:02:37 | INFO | train_inner | epoch 001:    357 / 6299 loss=11.701, nll_loss=10.53, ppl=1478.93, wps=14108.1, ups=2.09, wpb=6764, bsz=236, num_updates=346, lr=6.92e-06, gnorm=4.757, loss_scale=0.125, train_wall=1, gb_free=5.5, wall=208\n",
            "2023-08-16 09:02:38 | INFO | train_inner | epoch 001:    359 / 6299 loss=11.679, nll_loss=10.525, ppl=1473.38, wps=14904.4, ups=2.17, wpb=6864, bsz=200, num_updates=348, lr=6.96e-06, gnorm=4.485, loss_scale=0.125, train_wall=1, gb_free=4.9, wall=209\n",
            "2023-08-16 09:02:39 | INFO | train_inner | epoch 001:    361 / 6299 loss=11.647, nll_loss=10.494, ppl=1441.78, wps=14369.1, ups=2.23, wpb=6437.5, bsz=176, num_updates=350, lr=7e-06, gnorm=4.286, loss_scale=0.125, train_wall=1, gb_free=5.5, wall=210\n",
            "2023-08-16 09:02:40 | INFO | train_inner | epoch 001:    363 / 6299 loss=11.815, nll_loss=10.666, ppl=1625.24, wps=12642.1, ups=2.08, wpb=6073, bsz=220, num_updates=352, lr=7.04e-06, gnorm=6.225, loss_scale=0.125, train_wall=1, gb_free=5.4, wall=211\n",
            "2023-08-16 09:02:41 | INFO | train_inner | epoch 001:    365 / 6299 loss=11.73, nll_loss=10.589, ppl=1539.96, wps=13604.7, ups=2.04, wpb=6680, bsz=212, num_updates=354, lr=7.08e-06, gnorm=5.702, loss_scale=0.125, train_wall=1, gb_free=5.1, wall=212\n",
            "2023-08-16 09:02:42 | INFO | train_inner | epoch 001:    367 / 6299 loss=11.818, nll_loss=10.653, ppl=1610.65, wps=13284.6, ups=2.14, wpb=6208, bsz=324, num_updates=356, lr=7.12e-06, gnorm=11.009, loss_scale=0.125, train_wall=1, gb_free=5.1, wall=213\n",
            "2023-08-16 09:02:43 | INFO | train_inner | epoch 001:    369 / 6299 loss=11.507, nll_loss=10.375, ppl=1328.12, wps=14080.4, ups=2.11, wpb=6662.5, bsz=144, num_updates=358, lr=7.16e-06, gnorm=3.851, loss_scale=0.125, train_wall=1, gb_free=5, wall=214\n",
            "2023-08-16 09:02:44 | INFO | train_inner | epoch 001:    371 / 6299 loss=11.705, nll_loss=10.54, ppl=1489.06, wps=13998.6, ups=2.13, wpb=6573, bsz=308, num_updates=360, lr=7.2e-06, gnorm=6.023, loss_scale=0.125, train_wall=1, gb_free=4.7, wall=215\n",
            "2023-08-16 09:02:44 | INFO | train_inner | epoch 001:    373 / 6299 loss=11.704, nll_loss=10.599, ppl=1551.08, wps=14079.4, ups=2.06, wpb=6844, bsz=200, num_updates=362, lr=7.24e-06, gnorm=8.49, loss_scale=0.125, train_wall=1, gb_free=4.7, wall=215\n",
            "2023-08-16 09:02:45 | INFO | train_inner | epoch 001:    375 / 6299 loss=11.786, nll_loss=10.605, ppl=1556.95, wps=14851.8, ups=2.18, wpb=6811.5, bsz=376, num_updates=364, lr=7.28e-06, gnorm=11.661, loss_scale=0.125, train_wall=1, gb_free=5.7, wall=216\n",
            "2023-08-16 09:02:46 | INFO | train_inner | epoch 001:    377 / 6299 loss=11.725, nll_loss=10.609, ppl=1561.86, wps=13480.9, ups=2.16, wpb=6247.5, bsz=228, num_updates=366, lr=7.32e-06, gnorm=4.215, loss_scale=0.125, train_wall=1, gb_free=5.1, wall=217\n",
            "2023-08-16 09:02:47 | INFO | train_inner | epoch 001:    379 / 6299 loss=11.689, nll_loss=10.604, ppl=1556.53, wps=12850.7, ups=2.15, wpb=5983.5, bsz=156, num_updates=368, lr=7.36e-06, gnorm=3.619, loss_scale=0.125, train_wall=1, gb_free=5.5, wall=218\n",
            "2023-08-16 09:02:48 | INFO | train_inner | epoch 001:    381 / 6299 loss=11.535, nll_loss=10.408, ppl=1358.88, wps=15056.3, ups=2.04, wpb=7385, bsz=268, num_updates=370, lr=7.4e-06, gnorm=5.324, loss_scale=0.125, train_wall=1, gb_free=4.5, wall=219\n",
            "2023-08-16 09:02:49 | INFO | train_inner | epoch 001:    383 / 6299 loss=11.561, nll_loss=10.472, ppl=1420.75, wps=13018.7, ups=2.14, wpb=6089.5, bsz=140, num_updates=372, lr=7.44e-06, gnorm=3.446, loss_scale=0.125, train_wall=1, gb_free=5.1, wall=220\n",
            "2023-08-16 09:02:50 | INFO | train_inner | epoch 001:    385 / 6299 loss=11.685, nll_loss=10.589, ppl=1540.28, wps=12790.3, ups=2.14, wpb=5980, bsz=220, num_updates=374, lr=7.48e-06, gnorm=4.132, loss_scale=0.125, train_wall=1, gb_free=5.8, wall=221\n",
            "2023-08-16 09:02:51 | INFO | train_inner | epoch 001:    387 / 6299 loss=11.643, nll_loss=10.566, ppl=1516.08, wps=13657.8, ups=2.07, wpb=6589, bsz=168, num_updates=376, lr=7.52e-06, gnorm=3.283, loss_scale=0.125, train_wall=1, gb_free=4.8, wall=222\n",
            "2023-08-16 09:02:52 | INFO | train_inner | epoch 001:    389 / 6299 loss=11.568, nll_loss=10.441, ppl=1390.16, wps=14178.6, ups=2.1, wpb=6748, bsz=272, num_updates=378, lr=7.56e-06, gnorm=7.134, loss_scale=0.125, train_wall=1, gb_free=5.4, wall=223\n",
            "2023-08-16 09:02:53 | INFO | train_inner | epoch 001:    391 / 6299 loss=11.591, nll_loss=10.47, ppl=1417.96, wps=14296.6, ups=2, wpb=7148, bsz=284, num_updates=380, lr=7.6e-06, gnorm=5.858, loss_scale=0.125, train_wall=1, gb_free=4.6, wall=224\n",
            "2023-08-16 09:02:54 | INFO | train_inner | epoch 001:    393 / 6299 loss=11.554, nll_loss=10.407, ppl=1357.55, wps=12952.6, ups=2.08, wpb=6214, bsz=308, num_updates=382, lr=7.64e-06, gnorm=5.755, loss_scale=0.125, train_wall=1, gb_free=5.3, wall=225\n",
            "2023-08-16 09:02:55 | INFO | train_inner | epoch 001:    395 / 6299 loss=11.468, nll_loss=10.36, ppl=1313.94, wps=13775.5, ups=2, wpb=6884, bsz=224, num_updates=384, lr=7.68e-06, gnorm=4.037, loss_scale=0.125, train_wall=1, gb_free=4.4, wall=226\n",
            "2023-08-16 09:02:56 | INFO | train_inner | epoch 001:    397 / 6299 loss=11.442, nll_loss=10.317, ppl=1275.65, wps=13544.6, ups=2.13, wpb=6359, bsz=212, num_updates=386, lr=7.72e-06, gnorm=4.943, loss_scale=0.125, train_wall=1, gb_free=4.4, wall=227\n",
            "2023-08-16 09:02:57 | INFO | train_inner | epoch 001:    399 / 6299 loss=11.496, nll_loss=10.356, ppl=1310.9, wps=13551.3, ups=1.98, wpb=6860, bsz=360, num_updates=388, lr=7.76e-06, gnorm=5.735, loss_scale=0.125, train_wall=1, gb_free=4.9, wall=228\n",
            "2023-08-16 09:02:58 | INFO | train_inner | epoch 001:    401 / 6299 loss=11.566, nll_loss=10.494, ppl=1442.54, wps=13434.6, ups=2.21, wpb=6092, bsz=176, num_updates=390, lr=7.8e-06, gnorm=3.558, loss_scale=0.125, train_wall=1, gb_free=5, wall=229\n",
            "2023-08-16 09:02:59 | INFO | train_inner | epoch 001:    403 / 6299 loss=11.607, nll_loss=10.476, ppl=1424.6, wps=13498.6, ups=1.99, wpb=6768.5, bsz=412, num_updates=392, lr=7.84e-06, gnorm=8.241, loss_scale=0.125, train_wall=1, gb_free=4.2, wall=230\n",
            "2023-08-16 09:03:00 | INFO | train_inner | epoch 001:    405 / 6299 loss=11.492, nll_loss=10.404, ppl=1355.01, wps=13488.4, ups=2.04, wpb=6596, bsz=240, num_updates=394, lr=7.88e-06, gnorm=3.792, loss_scale=0.125, train_wall=1, gb_free=4.8, wall=231\n",
            "2023-08-16 09:03:01 | INFO | train_inner | epoch 001:    407 / 6299 loss=11.41, nll_loss=10.227, ppl=1198.84, wps=14271.6, ups=2.13, wpb=6692, bsz=488, num_updates=396, lr=7.92e-06, gnorm=9.478, loss_scale=0.125, train_wall=1, gb_free=5.4, wall=232\n",
            "2023-08-16 09:03:02 | INFO | train_inner | epoch 001:    409 / 6299 loss=11.577, nll_loss=10.521, ppl=1469.19, wps=11900.7, ups=2.25, wpb=5298.5, bsz=160, num_updates=398, lr=7.96e-06, gnorm=8.257, loss_scale=0.125, train_wall=1, gb_free=5.9, wall=233\n",
            "2023-08-16 09:03:03 | INFO | train_inner | epoch 001:    411 / 6299 loss=11.419, nll_loss=10.353, ppl=1307.43, wps=13855.7, ups=2.14, wpb=6474.5, bsz=168, num_updates=400, lr=8e-06, gnorm=3.295, loss_scale=0.125, train_wall=1, gb_free=4.6, wall=234\n",
            "2023-08-16 09:03:04 | INFO | train_inner | epoch 001:    413 / 6299 loss=11.366, nll_loss=10.28, ppl=1243.46, wps=14419.4, ups=2.1, wpb=6880.5, bsz=228, num_updates=402, lr=8.04e-06, gnorm=5.605, loss_scale=0.125, train_wall=1, gb_free=4.4, wall=235\n",
            "2023-08-16 09:03:04 | INFO | train_inner | epoch 001:    415 / 6299 loss=11.462, nll_loss=10.4, ppl=1351.6, wps=13627, ups=2.14, wpb=6372.5, bsz=176, num_updates=404, lr=8.08e-06, gnorm=3.229, loss_scale=0.125, train_wall=1, gb_free=4.7, wall=235\n",
            "2023-08-16 09:03:05 | INFO | train_inner | epoch 001:    417 / 6299 loss=11.334, nll_loss=10.247, ppl=1215.1, wps=13968.1, ups=1.98, wpb=7046, bsz=248, num_updates=406, lr=8.12e-06, gnorm=4.078, loss_scale=0.125, train_wall=1, gb_free=4.4, wall=236\n",
            "2023-08-16 09:03:06 | INFO | train_inner | epoch 001:    419 / 6299 loss=13.035, nll_loss=12.124, ppl=4463.59, wps=12553.8, ups=2.13, wpb=5902, bsz=224, num_updates=408, lr=8.16e-06, gnorm=19.413, loss_scale=0.125, train_wall=1, gb_free=4.8, wall=237\n",
            "2023-08-16 09:03:07 | INFO | train_inner | epoch 001:    421 / 6299 loss=11.381, nll_loss=10.322, ppl=1280.48, wps=12887, ups=2.05, wpb=6293.5, bsz=252, num_updates=410, lr=8.2e-06, gnorm=12.796, loss_scale=0.125, train_wall=1, gb_free=5, wall=238\n",
            "2023-08-16 09:03:08 | INFO | train_inner | epoch 001:    423 / 6299 loss=11.339, nll_loss=10.332, ppl=1288.83, wps=13120.8, ups=1.9, wpb=6921, bsz=156, num_updates=412, lr=8.24e-06, gnorm=2.955, loss_scale=0.125, train_wall=1, gb_free=4.7, wall=239\n",
            "2023-08-16 09:03:09 | INFO | train_inner | epoch 001:    425 / 6299 loss=11.337, nll_loss=10.271, ppl=1235.87, wps=13298.5, ups=2.13, wpb=6229.5, bsz=336, num_updates=414, lr=8.28e-06, gnorm=5.226, loss_scale=0.125, train_wall=1, gb_free=5.8, wall=240\n",
            "2023-08-16 09:03:10 | INFO | train_inner | epoch 001:    427 / 6299 loss=11.327, nll_loss=10.295, ppl=1256.07, wps=12814.3, ups=2.14, wpb=5997.5, bsz=236, num_updates=416, lr=8.32e-06, gnorm=4.218, loss_scale=0.125, train_wall=1, gb_free=5.1, wall=241\n",
            "2023-08-16 09:03:11 | INFO | train_inner | epoch 001:    429 / 6299 loss=11.359, nll_loss=10.332, ppl=1288.6, wps=13226, ups=2.14, wpb=6180, bsz=244, num_updates=418, lr=8.36e-06, gnorm=4.557, loss_scale=0.125, train_wall=1, gb_free=5.5, wall=242\n",
            "2023-08-16 09:03:12 | INFO | train_inner | epoch 001:    431 / 6299 loss=11.206, nll_loss=10.175, ppl=1155.88, wps=13708.4, ups=2.14, wpb=6396, bsz=156, num_updates=420, lr=8.4e-06, gnorm=3.045, loss_scale=0.125, train_wall=1, gb_free=4.6, wall=243\n",
            "2023-08-16 09:03:13 | INFO | train_inner | epoch 001:    433 / 6299 loss=11.214, nll_loss=10.175, ppl=1156.15, wps=14389.9, ups=2.29, wpb=6288.5, bsz=164, num_updates=422, lr=8.44e-06, gnorm=3.295, loss_scale=0.125, train_wall=1, gb_free=5.1, wall=244\n",
            "2023-08-16 09:03:14 | INFO | train_inner | epoch 001:    435 / 6299 loss=11.285, nll_loss=10.24, ppl=1209.37, wps=14021.3, ups=2.12, wpb=6612, bsz=240, num_updates=424, lr=8.48e-06, gnorm=10.016, loss_scale=0.125, train_wall=1, gb_free=4.5, wall=245\n",
            "2023-08-16 09:03:15 | INFO | train_inner | epoch 001:    437 / 6299 loss=11.356, nll_loss=10.32, ppl=1278.23, wps=12654.6, ups=2.13, wpb=5944, bsz=192, num_updates=426, lr=8.52e-06, gnorm=4.724, loss_scale=0.125, train_wall=1, gb_free=5.1, wall=246\n",
            "2023-08-16 09:03:16 | INFO | train_inner | epoch 001:    439 / 6299 loss=11.275, nll_loss=10.161, ppl=1144.98, wps=15003.6, ups=1.99, wpb=7531.5, bsz=476, num_updates=428, lr=8.56e-06, gnorm=6.821, loss_scale=0.125, train_wall=1, gb_free=4.4, wall=247\n",
            "2023-08-16 09:03:17 | INFO | train_inner | epoch 001:    441 / 6299 loss=11.163, nll_loss=10.133, ppl=1122.79, wps=14040.2, ups=2.1, wpb=6693, bsz=152, num_updates=430, lr=8.6e-06, gnorm=2.87, loss_scale=0.125, train_wall=1, gb_free=5.4, wall=248\n",
            "2023-08-16 09:03:18 | INFO | train_inner | epoch 001:    443 / 6299 loss=11.256, nll_loss=10.236, ppl=1205.59, wps=14207.2, ups=2.15, wpb=6612, bsz=172, num_updates=432, lr=8.64e-06, gnorm=2.572, loss_scale=0.125, train_wall=1, gb_free=5.3, wall=249\n",
            "2023-08-16 09:03:19 | INFO | train_inner | epoch 001:    445 / 6299 loss=11.293, nll_loss=10.279, ppl=1242.06, wps=14117.2, ups=2.08, wpb=6773, bsz=196, num_updates=434, lr=8.68e-06, gnorm=2.774, loss_scale=0.125, train_wall=1, gb_free=5.1, wall=250\n",
            "2023-08-16 09:03:20 | INFO | train_inner | epoch 001:    447 / 6299 loss=11.171, nll_loss=10.119, ppl=1112.12, wps=13925.8, ups=2.08, wpb=6688, bsz=348, num_updates=436, lr=8.72e-06, gnorm=4.804, loss_scale=0.125, train_wall=1, gb_free=4.7, wall=251\n",
            "2023-08-16 09:03:21 | INFO | train_inner | epoch 001:    449 / 6299 loss=11.251, nll_loss=10.241, ppl=1209.8, wps=13416.2, ups=2.11, wpb=6344, bsz=168, num_updates=438, lr=8.76e-06, gnorm=2.774, loss_scale=0.125, train_wall=1, gb_free=5.3, wall=252\n",
            "2023-08-16 09:03:22 | INFO | train_inner | epoch 001:    451 / 6299 loss=11.167, nll_loss=10.144, ppl=1131.3, wps=14076.2, ups=2.04, wpb=6886.5, bsz=192, num_updates=440, lr=8.8e-06, gnorm=6.789, loss_scale=0.125, train_wall=1, gb_free=5.2, wall=253\n",
            "2023-08-16 09:03:23 | INFO | train_inner | epoch 001:    453 / 6299 loss=11.108, nll_loss=10.065, ppl=1071.47, wps=13380.9, ups=2.01, wpb=6643, bsz=248, num_updates=442, lr=8.84e-06, gnorm=3.603, loss_scale=0.125, train_wall=1, gb_free=5, wall=254\n",
            "2023-08-16 09:03:24 | INFO | train_inner | epoch 001:    455 / 6299 loss=11.252, nll_loss=10.23, ppl=1201.12, wps=13065.2, ups=2.17, wpb=6024.5, bsz=216, num_updates=444, lr=8.88e-06, gnorm=3.297, loss_scale=0.125, train_wall=1, gb_free=5.9, wall=255\n",
            "2023-08-16 09:03:25 | INFO | train_inner | epoch 001:    457 / 6299 loss=11.078, nll_loss=10.045, ppl=1056.19, wps=14708.3, ups=2.09, wpb=7046, bsz=188, num_updates=446, lr=8.92e-06, gnorm=2.981, loss_scale=0.125, train_wall=1, gb_free=4.1, wall=256\n",
            "2023-08-16 09:03:26 | INFO | train_inner | epoch 001:    459 / 6299 loss=11.149, nll_loss=10.088, ppl=1088.28, wps=14045.8, ups=2.07, wpb=6784, bsz=312, num_updates=448, lr=8.96e-06, gnorm=5.799, loss_scale=0.125, train_wall=1, gb_free=5.1, wall=257\n",
            "2023-08-16 09:03:27 | INFO | train_inner | epoch 001:    461 / 6299 loss=11.138, nll_loss=10.085, ppl=1086.11, wps=13394.4, ups=2.07, wpb=6464, bsz=264, num_updates=450, lr=9e-06, gnorm=3.169, loss_scale=0.125, train_wall=1, gb_free=4.3, wall=258\n",
            "2023-08-16 09:03:27 | INFO | train_inner | epoch 001:    463 / 6299 loss=11.135, nll_loss=10.093, ppl=1092.46, wps=14007.7, ups=2.09, wpb=6702, bsz=220, num_updates=452, lr=9.04e-06, gnorm=2.718, loss_scale=0.125, train_wall=1, gb_free=4.7, wall=258\n",
            "2023-08-16 09:03:28 | INFO | train_inner | epoch 001:    465 / 6299 loss=11.174, nll_loss=10.158, ppl=1142.25, wps=12751.2, ups=2.18, wpb=5855.5, bsz=100, num_updates=454, lr=9.08e-06, gnorm=3.644, loss_scale=0.125, train_wall=1, gb_free=5.2, wall=259\n",
            "2023-08-16 09:03:29 | INFO | train_inner | epoch 001:    467 / 6299 loss=10.988, nll_loss=9.934, ppl=978.26, wps=12692.7, ups=2.17, wpb=5858, bsz=136, num_updates=456, lr=9.12e-06, gnorm=3.2, loss_scale=0.125, train_wall=1, gb_free=4.6, wall=260\n",
            "2023-08-16 09:03:30 | INFO | train_inner | epoch 001:    469 / 6299 loss=11, nll_loss=9.968, ppl=1001.49, wps=14199.6, ups=2.11, wpb=6726.5, bsz=108, num_updates=458, lr=9.16e-06, gnorm=2.932, loss_scale=0.125, train_wall=1, gb_free=4.7, wall=261\n",
            "2023-08-16 09:03:31 | INFO | train_inner | epoch 001:    471 / 6299 loss=10.959, nll_loss=9.883, ppl=944.46, wps=14802.7, ups=2.02, wpb=7317, bsz=268, num_updates=460, lr=9.2e-06, gnorm=3.753, loss_scale=0.125, train_wall=1, gb_free=4.7, wall=262\n",
            "2023-08-16 09:03:32 | INFO | train_inner | epoch 001:    473 / 6299 loss=11.023, nll_loss=9.976, ppl=1006.96, wps=13894.1, ups=2.16, wpb=6426.5, bsz=204, num_updates=462, lr=9.24e-06, gnorm=2.798, loss_scale=0.125, train_wall=1, gb_free=5.4, wall=263\n",
            "2023-08-16 09:03:33 | INFO | train_inner | epoch 001:    475 / 6299 loss=11.053, nll_loss=10.027, ppl=1043.53, wps=14591.8, ups=2.04, wpb=7140, bsz=172, num_updates=464, lr=9.28e-06, gnorm=2.47, loss_scale=0.125, train_wall=1, gb_free=5.2, wall=264\n",
            "2023-08-16 09:03:34 | INFO | train_inner | epoch 001:    477 / 6299 loss=11.113, nll_loss=10.097, ppl=1095.14, wps=12479.6, ups=2.11, wpb=5922.5, bsz=108, num_updates=466, lr=9.32e-06, gnorm=2.97, loss_scale=0.125, train_wall=1, gb_free=4.2, wall=265\n",
            "2023-08-16 09:03:35 | INFO | train_inner | epoch 001:    479 / 6299 loss=10.956, nll_loss=9.901, ppl=956.09, wps=13792.4, ups=2, wpb=6904.5, bsz=248, num_updates=468, lr=9.36e-06, gnorm=5.841, loss_scale=0.125, train_wall=1, gb_free=4.3, wall=266\n",
            "2023-08-16 09:03:36 | INFO | train_inner | epoch 001:    481 / 6299 loss=11.005, nll_loss=9.96, ppl=996, wps=12114.7, ups=2.05, wpb=5898.5, bsz=132, num_updates=470, lr=9.4e-06, gnorm=2.813, loss_scale=0.125, train_wall=1, gb_free=5.3, wall=267\n",
            "2023-08-16 09:03:37 | INFO | train_inner | epoch 001:    483 / 6299 loss=11.047, nll_loss=9.99, ppl=1016.66, wps=13054.2, ups=2.1, wpb=6220, bsz=296, num_updates=472, lr=9.44e-06, gnorm=4.408, loss_scale=0.125, train_wall=1, gb_free=5.2, wall=268\n",
            "2023-08-16 09:03:38 | INFO | train_inner | epoch 001:    485 / 6299 loss=10.988, nll_loss=9.922, ppl=969.91, wps=13240.3, ups=2.11, wpb=6288, bsz=228, num_updates=474, lr=9.48e-06, gnorm=3.249, loss_scale=0.125, train_wall=1, gb_free=5.1, wall=269\n",
            "2023-08-16 09:03:39 | INFO | train_inner | epoch 001:    487 / 6299 loss=10.887, nll_loss=9.799, ppl=890.65, wps=13180.9, ups=2.04, wpb=6468, bsz=276, num_updates=476, lr=9.52e-06, gnorm=3.595, loss_scale=0.125, train_wall=1, gb_free=5.3, wall=270\n",
            "2023-08-16 09:03:40 | INFO | train_inner | epoch 001:    489 / 6299 loss=10.96, nll_loss=9.896, ppl=952.94, wps=12704, ups=2.06, wpb=6172.5, bsz=196, num_updates=478, lr=9.56e-06, gnorm=2.826, loss_scale=0.125, train_wall=1, gb_free=5.2, wall=271\n",
            "2023-08-16 09:03:41 | INFO | train_inner | epoch 001:    491 / 6299 loss=10.811, nll_loss=9.706, ppl=835.23, wps=13767.9, ups=2.14, wpb=6442.5, bsz=256, num_updates=480, lr=9.6e-06, gnorm=4.552, loss_scale=0.125, train_wall=1, gb_free=4.8, wall=272\n",
            "2023-08-16 09:03:42 | INFO | train_inner | epoch 001:    493 / 6299 loss=10.821, nll_loss=9.744, ppl=857.44, wps=13829.7, ups=2.13, wpb=6480, bsz=156, num_updates=482, lr=9.64e-06, gnorm=2.922, loss_scale=0.125, train_wall=1, gb_free=4.9, wall=273\n",
            "2023-08-16 09:03:43 | INFO | train_inner | epoch 001:    495 / 6299 loss=10.771, nll_loss=9.669, ppl=814.3, wps=13356.7, ups=2.21, wpb=6040, bsz=172, num_updates=484, lr=9.68e-06, gnorm=2.813, loss_scale=0.125, train_wall=1, gb_free=4.7, wall=274\n",
            "2023-08-16 09:03:44 | INFO | train_inner | epoch 001:    497 / 6299 loss=10.668, nll_loss=9.52, ppl=734.29, wps=14052.7, ups=2.06, wpb=6808, bsz=356, num_updates=486, lr=9.72e-06, gnorm=4.085, loss_scale=0.125, train_wall=1, gb_free=4.6, wall=275\n",
            "2023-08-16 09:03:45 | INFO | train_inner | epoch 001:    499 / 6299 loss=10.706, nll_loss=9.557, ppl=753.06, wps=13193.4, ups=2.11, wpb=6252, bsz=268, num_updates=488, lr=9.76e-06, gnorm=4.277, loss_scale=0.125, train_wall=1, gb_free=4.9, wall=276\n",
            "2023-08-16 09:03:46 | INFO | train_inner | epoch 001:    501 / 6299 loss=10.574, nll_loss=9.412, ppl=681.08, wps=13860.4, ups=2.12, wpb=6530.5, bsz=248, num_updates=490, lr=9.8e-06, gnorm=3.914, loss_scale=0.125, train_wall=1, gb_free=4.6, wall=277\n",
            "2023-08-16 09:03:47 | INFO | train_inner | epoch 001:    503 / 6299 loss=10.621, nll_loss=9.517, ppl=732.89, wps=14309.7, ups=2.06, wpb=6931.5, bsz=164, num_updates=492, lr=9.84e-06, gnorm=2.86, loss_scale=0.125, train_wall=1, gb_free=5.4, wall=278\n",
            "2023-08-16 09:03:48 | INFO | train_inner | epoch 001:    505 / 6299 loss=10.596, nll_loss=9.455, ppl=701.65, wps=13768.8, ups=2.08, wpb=6604, bsz=352, num_updates=494, lr=9.88e-06, gnorm=4.004, loss_scale=0.125, train_wall=1, gb_free=5.2, wall=278\n",
            "2023-08-16 09:03:48 | INFO | train_inner | epoch 001:    507 / 6299 loss=10.586, nll_loss=9.445, ppl=697.06, wps=14433.4, ups=2.1, wpb=6873, bsz=272, num_updates=496, lr=9.92e-06, gnorm=10.6, loss_scale=0.125, train_wall=1, gb_free=5.3, wall=279\n",
            "2023-08-16 09:03:49 | INFO | train_inner | epoch 001:    509 / 6299 loss=10.549, nll_loss=9.419, ppl=684.57, wps=12292.3, ups=2.14, wpb=5734.5, bsz=92, num_updates=498, lr=9.96e-06, gnorm=3.274, loss_scale=0.125, train_wall=1, gb_free=5, wall=280\n",
            "2023-08-16 09:03:50 | INFO | train_inner | epoch 001:    511 / 6299 loss=10.504, nll_loss=9.353, ppl=653.79, wps=12988.6, ups=2.02, wpb=6418, bsz=248, num_updates=500, lr=1e-05, gnorm=3.353, loss_scale=0.125, train_wall=1, gb_free=5.3, wall=281\n",
            "2023-08-16 09:03:51 | INFO | train_inner | epoch 001:    513 / 6299 loss=10.396, nll_loss=9.22, ppl=596.46, wps=14326.1, ups=2.12, wpb=6761, bsz=264, num_updates=502, lr=1.004e-05, gnorm=3.521, loss_scale=0.125, train_wall=1, gb_free=4.6, wall=282\n",
            "2023-08-16 09:03:52 | INFO | train_inner | epoch 001:    515 / 6299 loss=10.397, nll_loss=9.21, ppl=592.1, wps=13336.1, ups=2.05, wpb=6508, bsz=272, num_updates=504, lr=1.008e-05, gnorm=4.267, loss_scale=0.125, train_wall=1, gb_free=5.2, wall=283\n",
            "2023-08-16 09:03:53 | INFO | train_inner | epoch 001:    517 / 6299 loss=10.347, nll_loss=9.145, ppl=566.17, wps=13551.5, ups=2.17, wpb=6248, bsz=228, num_updates=506, lr=1.012e-05, gnorm=3.22, loss_scale=0.125, train_wall=1, gb_free=5.4, wall=284\n",
            "2023-08-16 09:03:54 | INFO | train_inner | epoch 001:    519 / 6299 loss=10.216, nll_loss=9.014, ppl=517.03, wps=14595.2, ups=2.08, wpb=7026.5, bsz=252, num_updates=508, lr=1.016e-05, gnorm=4.267, loss_scale=0.125, train_wall=1, gb_free=4.4, wall=285\n",
            "2023-08-16 09:03:55 | INFO | train_inner | epoch 001:    521 / 6299 loss=10.304, nll_loss=9.095, ppl=547.02, wps=13670.1, ups=2.09, wpb=6551.5, bsz=172, num_updates=510, lr=1.02e-05, gnorm=4.157, loss_scale=0.125, train_wall=1, gb_free=5.2, wall=286\n",
            "2023-08-16 09:03:56 | INFO | train_inner | epoch 001:    523 / 6299 loss=10.212, nll_loss=8.967, ppl=500.45, wps=13503.8, ups=2.14, wpb=6316, bsz=152, num_updates=512, lr=1.024e-05, gnorm=96.138, loss_scale=0.125, train_wall=1, gb_free=5.2, wall=287\n",
            "2023-08-16 09:03:57 | INFO | train_inner | epoch 001:    525 / 6299 loss=10.099, nll_loss=8.846, ppl=460.07, wps=14258, ups=2.07, wpb=6898.5, bsz=336, num_updates=514, lr=1.028e-05, gnorm=4.605, loss_scale=0.125, train_wall=1, gb_free=5.4, wall=288\n",
            "2023-08-16 09:03:58 | INFO | train_inner | epoch 001:    527 / 6299 loss=10.174, nll_loss=8.954, ppl=495.88, wps=13725.8, ups=2.11, wpb=6509, bsz=148, num_updates=516, lr=1.032e-05, gnorm=4.439, loss_scale=0.125, train_wall=1, gb_free=5.5, wall=289\n",
            "2023-08-16 09:03:59 | INFO | train_inner | epoch 001:    529 / 6299 loss=10.088, nll_loss=8.858, ppl=463.9, wps=13653.4, ups=2.13, wpb=6406, bsz=204, num_updates=518, lr=1.036e-05, gnorm=3.182, loss_scale=0.125, train_wall=1, gb_free=4.9, wall=290\n",
            "2023-08-16 09:04:00 | INFO | train_inner | epoch 001:    531 / 6299 loss=10.092, nll_loss=8.869, ppl=467.61, wps=13581, ups=2.1, wpb=6480, bsz=292, num_updates=520, lr=1.04e-05, gnorm=3.845, loss_scale=0.125, train_wall=1, gb_free=5.5, wall=291\n",
            "2023-08-16 09:04:01 | INFO | train_inner | epoch 001:    533 / 6299 loss=10.005, nll_loss=8.753, ppl=431.32, wps=13829.2, ups=2.08, wpb=6656, bsz=264, num_updates=522, lr=1.044e-05, gnorm=4.159, loss_scale=0.125, train_wall=1, gb_free=4.8, wall=292\n",
            "2023-08-16 09:04:02 | INFO | train_inner | epoch 001:    535 / 6299 loss=9.902, nll_loss=8.615, ppl=392.15, wps=13459.4, ups=2.07, wpb=6498.5, bsz=320, num_updates=524, lr=1.048e-05, gnorm=3.801, loss_scale=0.125, train_wall=1, gb_free=4.5, wall=293\n",
            "2023-08-16 09:04:03 | INFO | train_inner | epoch 001:    537 / 6299 loss=9.997, nll_loss=8.728, ppl=424.07, wps=14484.1, ups=2.06, wpb=7043.5, bsz=228, num_updates=526, lr=1.052e-05, gnorm=4.341, loss_scale=0.125, train_wall=1, gb_free=5.1, wall=294\n",
            "2023-08-16 09:04:04 | INFO | train_inner | epoch 001:    539 / 6299 loss=9.948, nll_loss=8.652, ppl=402.15, wps=13423.3, ups=2.17, wpb=6178, bsz=172, num_updates=528, lr=1.056e-05, gnorm=3.41, loss_scale=0.125, train_wall=1, gb_free=5, wall=295\n",
            "2023-08-16 09:04:05 | INFO | train_inner | epoch 001:    541 / 6299 loss=9.865, nll_loss=8.563, ppl=378.1, wps=14228.5, ups=2.17, wpb=6547.5, bsz=180, num_updates=530, lr=1.06e-05, gnorm=3.372, loss_scale=0.125, train_wall=1, gb_free=4.4, wall=296\n",
            "2023-08-16 09:04:06 | INFO | train_inner | epoch 001:    543 / 6299 loss=9.728, nll_loss=8.413, ppl=340.78, wps=14323.8, ups=2.1, wpb=6814.5, bsz=268, num_updates=532, lr=1.064e-05, gnorm=3.477, loss_scale=0.125, train_wall=1, gb_free=4.7, wall=297\n",
            "2023-08-16 09:04:07 | INFO | train_inner | epoch 001:    545 / 6299 loss=9.786, nll_loss=8.474, ppl=355.62, wps=13861.4, ups=2.08, wpb=6668, bsz=264, num_updates=534, lr=1.068e-05, gnorm=3.647, loss_scale=0.125, train_wall=1, gb_free=5.5, wall=298\n",
            "2023-08-16 09:04:07 | INFO | train_inner | epoch 001:    547 / 6299 loss=9.745, nll_loss=8.428, ppl=344.31, wps=13612.8, ups=2.11, wpb=6460, bsz=224, num_updates=536, lr=1.072e-05, gnorm=3.373, loss_scale=0.125, train_wall=1, gb_free=5.1, wall=298\n",
            "2023-08-16 09:04:08 | INFO | train_inner | epoch 001:    549 / 6299 loss=9.601, nll_loss=8.26, ppl=306.49, wps=14520.6, ups=2.05, wpb=7097, bsz=244, num_updates=538, lr=1.076e-05, gnorm=3.631, loss_scale=0.125, train_wall=1, gb_free=4.8, wall=299\n",
            "2023-08-16 09:04:09 | INFO | train_inner | epoch 001:    551 / 6299 loss=9.652, nll_loss=8.289, ppl=312.85, wps=12826.8, ups=2.24, wpb=5729.5, bsz=160, num_updates=540, lr=1.08e-05, gnorm=3.368, loss_scale=0.125, train_wall=1, gb_free=5.7, wall=300\n",
            "2023-08-16 09:04:10 | INFO | train_inner | epoch 001:    553 / 6299 loss=9.534, nll_loss=8.156, ppl=285.29, wps=14647.3, ups=2.07, wpb=7076, bsz=204, num_updates=542, lr=1.084e-05, gnorm=3.212, loss_scale=0.125, train_wall=1, gb_free=4.9, wall=301\n",
            "2023-08-16 09:04:11 | INFO | train_inner | epoch 001:    555 / 6299 loss=9.549, nll_loss=8.152, ppl=284.52, wps=14455.7, ups=2.07, wpb=6995.5, bsz=164, num_updates=544, lr=1.088e-05, gnorm=3.468, loss_scale=0.125, train_wall=1, gb_free=4.9, wall=302\n",
            "2023-08-16 09:04:12 | INFO | train_inner | epoch 001:    557 / 6299 loss=9.709, nll_loss=8.331, ppl=322.06, wps=14658.8, ups=2.23, wpb=6577, bsz=324, num_updates=546, lr=1.092e-05, gnorm=6.205, loss_scale=0.125, train_wall=1, gb_free=5.2, wall=303\n",
            "2023-08-16 09:04:13 | INFO | train_inner | epoch 001:    559 / 6299 loss=9.481, nll_loss=8.118, ppl=277.77, wps=13442.3, ups=2.01, wpb=6700.5, bsz=260, num_updates=548, lr=1.096e-05, gnorm=4.207, loss_scale=0.125, train_wall=1, gb_free=4.4, wall=304\n",
            "2023-08-16 09:04:14 | INFO | train_inner | epoch 001:    561 / 6299 loss=9.399, nll_loss=8.018, ppl=259.29, wps=13406.2, ups=2.03, wpb=6610.5, bsz=152, num_updates=550, lr=1.1e-05, gnorm=2.969, loss_scale=0.125, train_wall=1, gb_free=4.3, wall=305\n",
            "2023-08-16 09:04:15 | INFO | train_inner | epoch 001:    563 / 6299 loss=9.403, nll_loss=8.014, ppl=258.53, wps=11761.7, ups=2.18, wpb=5406, bsz=124, num_updates=552, lr=1.104e-05, gnorm=3.103, loss_scale=0.125, train_wall=1, gb_free=5.6, wall=306\n",
            "2023-08-16 09:04:16 | INFO | train_inner | epoch 001:    565 / 6299 loss=9.317, nll_loss=7.926, ppl=243.24, wps=14452.7, ups=2.01, wpb=7189.5, bsz=248, num_updates=554, lr=1.108e-05, gnorm=3.375, loss_scale=0.125, train_wall=1, gb_free=5, wall=307\n",
            "2023-08-16 09:04:17 | INFO | train_inner | epoch 001:    567 / 6299 loss=9.243, nll_loss=7.835, ppl=228.34, wps=13692.1, ups=2.11, wpb=6496, bsz=296, num_updates=556, lr=1.112e-05, gnorm=4.671, loss_scale=0.125, train_wall=1, gb_free=5.7, wall=308\n",
            "2023-08-16 09:04:18 | INFO | train_inner | epoch 001:    569 / 6299 loss=9.36, nll_loss=7.967, ppl=250.26, wps=13488.8, ups=2.28, wpb=5927.5, bsz=236, num_updates=558, lr=1.116e-05, gnorm=4.342, loss_scale=0.125, train_wall=1, gb_free=6.5, wall=309\n",
            "2023-08-16 09:04:19 | INFO | train_inner | epoch 001:    571 / 6299 loss=9.239, nll_loss=7.788, ppl=220.95, wps=14055, ups=2.05, wpb=6851.5, bsz=260, num_updates=560, lr=1.12e-05, gnorm=4.311, loss_scale=0.125, train_wall=1, gb_free=5, wall=310\n",
            "2023-08-16 09:04:20 | INFO | train_inner | epoch 001:    573 / 6299 loss=9.262, nll_loss=7.782, ppl=220.17, wps=13053.5, ups=2.22, wpb=5886, bsz=132, num_updates=562, lr=1.124e-05, gnorm=3.599, loss_scale=0.125, train_wall=1, gb_free=5.1, wall=311\n",
            "2023-08-16 09:04:21 | INFO | train_inner | epoch 001:    575 / 6299 loss=9.145, nll_loss=7.689, ppl=206.29, wps=14375.2, ups=2.14, wpb=6724, bsz=172, num_updates=564, lr=1.128e-05, gnorm=3.34, loss_scale=0.125, train_wall=1, gb_free=5.1, wall=312\n",
            "2023-08-16 09:04:22 | INFO | train_inner | epoch 001:    577 / 6299 loss=9.024, nll_loss=7.569, ppl=189.88, wps=14060, ups=2.07, wpb=6796, bsz=240, num_updates=566, lr=1.132e-05, gnorm=3.426, loss_scale=0.125, train_wall=1, gb_free=4.6, wall=313\n",
            "2023-08-16 09:04:23 | INFO | train_inner | epoch 001:    579 / 6299 loss=9.108, nll_loss=7.686, ppl=205.99, wps=14384.1, ups=2.01, wpb=7156.5, bsz=288, num_updates=568, lr=1.136e-05, gnorm=4.255, loss_scale=0.125, train_wall=1, gb_free=4.5, wall=314\n",
            "2023-08-16 09:04:24 | INFO | train_inner | epoch 001:    581 / 6299 loss=8.953, nll_loss=7.505, ppl=181.63, wps=14904.3, ups=2.06, wpb=7228, bsz=284, num_updates=570, lr=1.14e-05, gnorm=3.986, loss_scale=0.125, train_wall=1, gb_free=4.6, wall=315\n",
            "2023-08-16 09:04:25 | INFO | train_inner | epoch 001:    583 / 6299 loss=8.958, nll_loss=7.481, ppl=178.68, wps=13652.2, ups=2.12, wpb=6448, bsz=228, num_updates=572, lr=1.144e-05, gnorm=3.396, loss_scale=0.125, train_wall=1, gb_free=5.7, wall=316\n",
            "2023-08-16 09:04:26 | INFO | train_inner | epoch 001:    585 / 6299 loss=9.007, nll_loss=7.515, ppl=182.95, wps=13003.1, ups=2.08, wpb=6248, bsz=240, num_updates=574, lr=1.148e-05, gnorm=4.187, loss_scale=0.125, train_wall=1, gb_free=4.4, wall=317\n",
            "2023-08-16 09:04:27 | INFO | train_inner | epoch 001:    587 / 6299 loss=9.233, nll_loss=7.749, ppl=215.12, wps=14601.1, ups=1.96, wpb=7448, bsz=760, num_updates=576, lr=1.152e-05, gnorm=8.772, loss_scale=0.125, train_wall=1, gb_free=5, wall=318\n",
            "2023-08-16 09:04:28 | INFO | train_inner | epoch 001:    589 / 6299 loss=8.789, nll_loss=7.266, ppl=153.9, wps=13985.5, ups=2, wpb=6982.5, bsz=268, num_updates=578, lr=1.156e-05, gnorm=4.357, loss_scale=0.125, train_wall=1, gb_free=5, wall=319\n",
            "2023-08-16 09:04:29 | INFO | train_inner | epoch 001:    591 / 6299 loss=8.782, nll_loss=7.275, ppl=154.92, wps=14091.1, ups=1.97, wpb=7162, bsz=300, num_updates=580, lr=1.16e-05, gnorm=4.066, loss_scale=0.125, train_wall=1, gb_free=4.8, wall=320\n",
            "2023-08-16 09:04:30 | INFO | train_inner | epoch 001:    593 / 6299 loss=8.699, nll_loss=7.182, ppl=145.25, wps=13305.6, ups=2.05, wpb=6504.5, bsz=428, num_updates=582, lr=1.164e-05, gnorm=5.019, loss_scale=0.125, train_wall=1, gb_free=4.9, wall=321\n",
            "2023-08-16 09:04:31 | INFO | train_inner | epoch 001:    595 / 6299 loss=8.952, nll_loss=7.465, ppl=176.71, wps=13611.2, ups=2.1, wpb=6470, bsz=140, num_updates=584, lr=1.168e-05, gnorm=3.793, loss_scale=0.125, train_wall=1, gb_free=3.7, wall=322\n",
            "2023-08-16 09:04:31 | INFO | train_inner | epoch 001:    597 / 6299 loss=9.358, nll_loss=7.937, ppl=245.04, wps=12236.9, ups=2.21, wpb=5536, bsz=172, num_updates=586, lr=1.172e-05, gnorm=11.752, loss_scale=0.125, train_wall=1, gb_free=4.4, wall=322\n",
            "2023-08-16 09:04:32 | INFO | train_inner | epoch 001:    599 / 6299 loss=8.661, nll_loss=7.185, ppl=145.51, wps=14044.1, ups=2.08, wpb=6756.5, bsz=276, num_updates=588, lr=1.176e-05, gnorm=4.124, loss_scale=0.125, train_wall=1, gb_free=5.1, wall=323\n",
            "2023-08-16 09:04:33 | INFO | train_inner | epoch 001:    601 / 6299 loss=8.486, nll_loss=6.982, ppl=126.41, wps=14164.9, ups=2.02, wpb=6996, bsz=408, num_updates=590, lr=1.18e-05, gnorm=4.692, loss_scale=0.125, train_wall=1, gb_free=5.4, wall=324\n",
            "2023-08-16 09:04:34 | INFO | train_inner | epoch 001:    603 / 6299 loss=8.521, nll_loss=6.999, ppl=127.91, wps=15130.9, ups=2.03, wpb=7445.5, bsz=236, num_updates=592, lr=1.184e-05, gnorm=3.552, loss_scale=0.125, train_wall=1, gb_free=4.5, wall=325\n",
            "2023-08-16 09:04:35 | INFO | train_inner | epoch 001:    605 / 6299 loss=8.619, nll_loss=7.101, ppl=137.27, wps=15110.2, ups=2.13, wpb=7100.5, bsz=356, num_updates=594, lr=1.188e-05, gnorm=4.392, loss_scale=0.125, train_wall=1, gb_free=4.6, wall=326\n",
            "2023-08-16 09:04:36 | INFO | train_inner | epoch 001:    607 / 6299 loss=8.633, nll_loss=7.097, ppl=136.94, wps=14669.5, ups=2.09, wpb=7012, bsz=420, num_updates=596, lr=1.192e-05, gnorm=6.061, loss_scale=0.125, train_wall=1, gb_free=5.2, wall=327\n",
            "2023-08-16 09:04:37 | INFO | train_inner | epoch 001:    609 / 6299 loss=8.47, nll_loss=6.898, ppl=119.28, wps=15628.5, ups=2.07, wpb=7540, bsz=244, num_updates=598, lr=1.196e-05, gnorm=3.765, loss_scale=0.125, train_wall=1, gb_free=4.8, wall=328\n",
            "2023-08-16 09:04:38 | INFO | train_inner | epoch 001:    611 / 6299 loss=8.381, nll_loss=6.829, ppl=113.68, wps=13677.1, ups=1.92, wpb=7120, bsz=352, num_updates=600, lr=1.2e-05, gnorm=3.678, loss_scale=0.125, train_wall=1, gb_free=5.2, wall=329\n",
            "2023-08-16 09:04:39 | INFO | train_inner | epoch 001:    613 / 6299 loss=8.593, nll_loss=7.082, ppl=135.48, wps=13289, ups=2.11, wpb=6289.5, bsz=184, num_updates=602, lr=1.204e-05, gnorm=3.655, loss_scale=0.125, train_wall=1, gb_free=5.2, wall=330\n",
            "2023-08-16 09:04:40 | INFO | train_inner | epoch 001:    615 / 6299 loss=8.394, nll_loss=6.859, ppl=116.11, wps=14464.6, ups=2.02, wpb=7176, bsz=256, num_updates=604, lr=1.208e-05, gnorm=3.331, loss_scale=0.125, train_wall=1, gb_free=5.1, wall=331\n",
            "2023-08-16 09:04:41 | INFO | train_inner | epoch 001:    617 / 6299 loss=8.48, nll_loss=6.951, ppl=123.72, wps=12315.2, ups=2.03, wpb=6074, bsz=180, num_updates=606, lr=1.212e-05, gnorm=3.59, loss_scale=0.125, train_wall=1, gb_free=5.6, wall=332\n",
            "2023-08-16 09:04:42 | INFO | train_inner | epoch 001:    619 / 6299 loss=8.295, nll_loss=6.718, ppl=105.25, wps=13646.3, ups=1.98, wpb=6904, bsz=264, num_updates=608, lr=1.216e-05, gnorm=3.096, loss_scale=0.125, train_wall=1, gb_free=4.6, wall=333\n",
            "2023-08-16 09:04:43 | INFO | train_inner | epoch 001:    621 / 6299 loss=8.391, nll_loss=6.803, ppl=111.69, wps=13568.7, ups=2.15, wpb=6304, bsz=152, num_updates=610, lr=1.22e-05, gnorm=3.048, loss_scale=0.125, train_wall=1, gb_free=5.1, wall=334\n",
            "2023-08-16 09:04:44 | INFO | train_inner | epoch 001:    623 / 6299 loss=8.165, nll_loss=6.555, ppl=94, wps=13931.4, ups=2.08, wpb=6688.5, bsz=316, num_updates=612, lr=1.224e-05, gnorm=3.529, loss_scale=0.125, train_wall=1, gb_free=5.1, wall=335\n",
            "2023-08-16 09:04:45 | INFO | train_inner | epoch 001:    625 / 6299 loss=8.272, nll_loss=6.657, ppl=100.88, wps=14874.6, ups=2.06, wpb=7223.5, bsz=168, num_updates=614, lr=1.228e-05, gnorm=3.649, loss_scale=0.125, train_wall=1, gb_free=4.8, wall=336\n",
            "2023-08-16 09:04:46 | INFO | train_inner | epoch 001:    627 / 6299 loss=8.39, nll_loss=6.805, ppl=111.78, wps=12287.8, ups=2.2, wpb=5594.5, bsz=160, num_updates=616, lr=1.232e-05, gnorm=3.125, loss_scale=0.125, train_wall=1, gb_free=4.6, wall=337\n",
            "2023-08-16 09:04:47 | INFO | train_inner | epoch 001:    629 / 6299 loss=8.39, nll_loss=6.818, ppl=112.82, wps=12994.3, ups=1.99, wpb=6544.5, bsz=160, num_updates=618, lr=1.236e-05, gnorm=3.543, loss_scale=0.125, train_wall=1, gb_free=5.1, wall=338\n",
            "2023-08-16 09:04:48 | INFO | train_inner | epoch 001:    631 / 6299 loss=8.313, nll_loss=6.727, ppl=105.93, wps=12851.3, ups=2.1, wpb=6112, bsz=152, num_updates=620, lr=1.24e-05, gnorm=3.093, loss_scale=0.125, train_wall=1, gb_free=4.6, wall=339\n",
            "2023-08-16 09:04:49 | INFO | train_inner | epoch 001:    633 / 6299 loss=8.167, nll_loss=6.563, ppl=94.53, wps=14277.7, ups=2.11, wpb=6776, bsz=248, num_updates=622, lr=1.244e-05, gnorm=3.303, loss_scale=0.125, train_wall=1, gb_free=4.9, wall=340\n",
            "2023-08-16 09:04:50 | INFO | train_inner | epoch 001:    635 / 6299 loss=8.074, nll_loss=6.473, ppl=88.84, wps=15031.9, ups=2.1, wpb=7152.5, bsz=348, num_updates=624, lr=1.248e-05, gnorm=3.681, loss_scale=0.125, train_wall=1, gb_free=5, wall=341\n",
            "2023-08-16 09:04:51 | INFO | train_inner | epoch 001:    637 / 6299 loss=8.107, nll_loss=6.478, ppl=89.16, wps=13372.8, ups=2.3, wpb=5809, bsz=220, num_updates=626, lr=1.252e-05, gnorm=4.304, loss_scale=0.125, train_wall=1, gb_free=5.5, wall=342\n",
            "2023-08-16 09:04:52 | INFO | train_inner | epoch 001:    639 / 6299 loss=8.014, nll_loss=6.378, ppl=83.14, wps=14353.8, ups=2.11, wpb=6797, bsz=344, num_updates=628, lr=1.256e-05, gnorm=3.954, loss_scale=0.125, train_wall=1, gb_free=5.3, wall=343\n",
            "2023-08-16 09:04:53 | INFO | train_inner | epoch 001:    641 / 6299 loss=8.158, nll_loss=6.523, ppl=91.97, wps=13471.9, ups=2.19, wpb=6150.5, bsz=164, num_updates=630, lr=1.26e-05, gnorm=3.101, loss_scale=0.125, train_wall=1, gb_free=4.9, wall=344\n",
            "2023-08-16 09:04:54 | INFO | train_inner | epoch 001:    643 / 6299 loss=7.993, nll_loss=6.341, ppl=81.07, wps=14475.3, ups=1.98, wpb=7310.5, bsz=276, num_updates=632, lr=1.264e-05, gnorm=3.594, loss_scale=0.125, train_wall=1, gb_free=4.9, wall=345\n",
            "2023-08-16 09:04:55 | INFO | train_inner | epoch 001:    645 / 6299 loss=7.99, nll_loss=6.357, ppl=81.95, wps=14204, ups=2.02, wpb=7026, bsz=404, num_updates=634, lr=1.268e-05, gnorm=4.339, loss_scale=0.125, train_wall=1, gb_free=4.6, wall=346\n",
            "2023-08-16 09:04:56 | INFO | train_inner | epoch 001:    647 / 6299 loss=8.35, nll_loss=6.802, ppl=111.6, wps=13522.6, ups=2.02, wpb=6683.5, bsz=196, num_updates=636, lr=1.272e-05, gnorm=4.025, loss_scale=0.125, train_wall=1, gb_free=5.1, wall=347\n",
            "2023-08-16 09:04:57 | INFO | train_inner | epoch 001:    649 / 6299 loss=7.965, nll_loss=6.324, ppl=80.11, wps=13965.2, ups=2.08, wpb=6708, bsz=272, num_updates=638, lr=1.276e-05, gnorm=3.135, loss_scale=0.125, train_wall=1, gb_free=5.3, wall=348\n",
            "2023-08-16 09:04:58 | INFO | train_inner | epoch 001:    651 / 6299 loss=7.962, nll_loss=6.328, ppl=80.32, wps=13037.3, ups=2.06, wpb=6340, bsz=296, num_updates=640, lr=1.28e-05, gnorm=3.949, loss_scale=0.125, train_wall=1, gb_free=5.3, wall=349\n",
            "2023-08-16 09:04:58 | INFO | train_inner | epoch 001:    653 / 6299 loss=7.94, nll_loss=6.287, ppl=78.09, wps=13827.1, ups=2.15, wpb=6440, bsz=184, num_updates=642, lr=1.284e-05, gnorm=2.837, loss_scale=0.125, train_wall=1, gb_free=5.3, wall=349\n",
            "2023-08-16 09:04:59 | INFO | train_inner | epoch 001:    655 / 6299 loss=7.952, nll_loss=6.294, ppl=78.47, wps=14034, ups=2.19, wpb=6398, bsz=204, num_updates=644, lr=1.288e-05, gnorm=3.635, loss_scale=0.125, train_wall=1, gb_free=4.5, wall=350\n",
            "2023-08-16 09:05:00 | INFO | train_inner | epoch 001:    657 / 6299 loss=7.937, nll_loss=6.275, ppl=77.44, wps=13787.6, ups=2.2, wpb=6272, bsz=144, num_updates=646, lr=1.292e-05, gnorm=2.884, loss_scale=0.125, train_wall=1, gb_free=5.1, wall=351\n",
            "2023-08-16 09:05:01 | INFO | train_inner | epoch 001:    659 / 6299 loss=7.58, nll_loss=5.889, ppl=59.27, wps=13440.9, ups=2.06, wpb=6513.5, bsz=452, num_updates=648, lr=1.296e-05, gnorm=4.51, loss_scale=0.125, train_wall=1, gb_free=4.7, wall=352\n",
            "2023-08-16 09:05:02 | INFO | train_inner | epoch 001:    661 / 6299 loss=8.063, nll_loss=6.436, ppl=86.58, wps=12866.8, ups=2.4, wpb=5354, bsz=104, num_updates=650, lr=1.3e-05, gnorm=3.627, loss_scale=0.125, train_wall=1, gb_free=6.5, wall=353\n",
            "2023-08-16 09:05:03 | INFO | train_inner | epoch 001:    663 / 6299 loss=8.088, nll_loss=6.463, ppl=88.2, wps=14612.2, ups=2.06, wpb=7091, bsz=132, num_updates=652, lr=1.304e-05, gnorm=3.656, loss_scale=0.125, train_wall=1, gb_free=4.9, wall=354\n",
            "2023-08-16 09:05:04 | INFO | train_inner | epoch 001:    665 / 6299 loss=7.981, nll_loss=6.328, ppl=80.33, wps=12490.3, ups=2.15, wpb=5818, bsz=148, num_updates=654, lr=1.308e-05, gnorm=3.451, loss_scale=0.125, train_wall=1, gb_free=5.5, wall=355\n",
            "2023-08-16 09:05:05 | INFO | train_inner | epoch 001:    667 / 6299 loss=7.575, nll_loss=5.852, ppl=57.78, wps=13368.7, ups=2.06, wpb=6484, bsz=304, num_updates=656, lr=1.312e-05, gnorm=3.732, loss_scale=0.125, train_wall=1, gb_free=5.3, wall=356\n",
            "2023-08-16 09:05:06 | INFO | train_inner | epoch 001:    669 / 6299 loss=8.102, nll_loss=6.492, ppl=90.02, wps=10784.9, ups=2.22, wpb=4863, bsz=120, num_updates=658, lr=1.316e-05, gnorm=5.546, loss_scale=0.125, train_wall=1, gb_free=5.5, wall=357\n",
            "2023-08-16 09:05:07 | INFO | train_inner | epoch 001:    671 / 6299 loss=7.682, nll_loss=5.994, ppl=63.73, wps=13600.5, ups=2.06, wpb=6588, bsz=212, num_updates=660, lr=1.32e-05, gnorm=3.121, loss_scale=0.125, train_wall=1, gb_free=5, wall=358\n",
            "2023-08-16 09:05:08 | INFO | train_inner | epoch 001:    673 / 6299 loss=7.809, nll_loss=6.159, ppl=71.48, wps=13615.9, ups=2.19, wpb=6229, bsz=120, num_updates=662, lr=1.324e-05, gnorm=2.857, loss_scale=0.125, train_wall=1, gb_free=5.8, wall=359\n",
            "2023-08-16 09:05:09 | INFO | train_inner | epoch 001:    675 / 6299 loss=7.988, nll_loss=6.351, ppl=81.64, wps=11234.2, ups=2.23, wpb=5039.5, bsz=144, num_updates=664, lr=1.328e-05, gnorm=3.191, loss_scale=0.125, train_wall=1, gb_free=5.7, wall=360\n",
            "2023-08-16 09:05:10 | INFO | train_inner | epoch 001:    677 / 6299 loss=7.948, nll_loss=6.309, ppl=79.31, wps=10807, ups=2.24, wpb=4818, bsz=172, num_updates=666, lr=1.332e-05, gnorm=4.203, loss_scale=0.125, train_wall=1, gb_free=5.9, wall=361\n",
            "2023-08-16 09:05:10 | INFO | train_inner | epoch 001:    679 / 6299 loss=7.697, nll_loss=6.035, ppl=65.56, wps=14403.6, ups=2.08, wpb=6925, bsz=356, num_updates=668, lr=1.336e-05, gnorm=3.918, loss_scale=0.125, train_wall=1, gb_free=5.2, wall=361\n",
            "2023-08-16 09:05:11 | INFO | train_inner | epoch 001:    681 / 6299 loss=7.655, nll_loss=5.966, ppl=62.5, wps=14459.3, ups=2.03, wpb=7131, bsz=176, num_updates=670, lr=1.34e-05, gnorm=2.518, loss_scale=0.125, train_wall=1, gb_free=4.3, wall=362\n",
            "2023-08-16 09:05:12 | INFO | train_inner | epoch 001:    683 / 6299 loss=7.729, nll_loss=6.084, ppl=67.86, wps=13373.4, ups=2.08, wpb=6438, bsz=272, num_updates=672, lr=1.344e-05, gnorm=3.384, loss_scale=0.125, train_wall=1, gb_free=4.8, wall=363\n",
            "2023-08-16 09:05:13 | INFO | train_inner | epoch 001:    685 / 6299 loss=8.613, nll_loss=7.072, ppl=134.59, wps=11665.5, ups=2.16, wpb=5389, bsz=199.5, num_updates=674, lr=1.348e-05, gnorm=14.686, loss_scale=0.125, train_wall=1, gb_free=4.2, wall=364\n",
            "2023-08-16 09:05:14 | INFO | train_inner | epoch 001:    687 / 6299 loss=7.43, nll_loss=5.734, ppl=53.22, wps=14520, ups=1.98, wpb=7336, bsz=408, num_updates=676, lr=1.352e-05, gnorm=4.253, loss_scale=0.125, train_wall=1, gb_free=5.4, wall=365\n",
            "2023-08-16 09:05:15 | INFO | train_inner | epoch 001:    689 / 6299 loss=7.624, nll_loss=5.962, ppl=62.33, wps=14448.4, ups=2.04, wpb=7080, bsz=468, num_updates=678, lr=1.356e-05, gnorm=5.035, loss_scale=0.125, train_wall=1, gb_free=4.9, wall=366\n",
            "2023-08-16 09:05:16 | INFO | train_inner | epoch 001:    691 / 6299 loss=7.689, nll_loss=6.014, ppl=64.62, wps=13411.5, ups=2.16, wpb=6200, bsz=144, num_updates=680, lr=1.36e-05, gnorm=3.853, loss_scale=0.125, train_wall=1, gb_free=5.7, wall=367\n",
            "2023-08-16 09:05:17 | INFO | train_inner | epoch 001:    693 / 6299 loss=7.57, nll_loss=5.865, ppl=58.29, wps=14499.4, ups=2.09, wpb=6936, bsz=196, num_updates=682, lr=1.364e-05, gnorm=3.194, loss_scale=0.125, train_wall=1, gb_free=4.3, wall=368\n",
            "2023-08-16 09:05:18 | INFO | train_inner | epoch 001:    695 / 6299 loss=7.496, nll_loss=5.771, ppl=54.6, wps=14252, ups=2.18, wpb=6528.5, bsz=200, num_updates=684, lr=1.368e-05, gnorm=2.891, loss_scale=0.125, train_wall=1, gb_free=5.3, wall=369\n",
            "2023-08-16 09:05:19 | INFO | train_inner | epoch 001:    697 / 6299 loss=7.627, nll_loss=5.944, ppl=61.57, wps=14397.6, ups=2.12, wpb=6784, bsz=164, num_updates=686, lr=1.372e-05, gnorm=2.818, loss_scale=0.125, train_wall=1, gb_free=5.4, wall=370\n",
            "2023-08-16 09:05:20 | INFO | train_inner | epoch 001:    699 / 6299 loss=7.534, nll_loss=5.856, ppl=57.92, wps=12922.1, ups=2.2, wpb=5866.5, bsz=188, num_updates=688, lr=1.376e-05, gnorm=3.523, loss_scale=0.125, train_wall=1, gb_free=5.7, wall=371\n",
            "2023-08-16 09:05:21 | INFO | train_inner | epoch 001:    701 / 6299 loss=7.593, nll_loss=5.959, ppl=62.19, wps=13566.4, ups=2.04, wpb=6650.5, bsz=204, num_updates=690, lr=1.38e-05, gnorm=2.771, loss_scale=0.125, train_wall=1, gb_free=5.3, wall=372\n",
            "2023-08-16 09:05:22 | INFO | train_inner | epoch 001:    703 / 6299 loss=7.488, nll_loss=5.83, ppl=56.9, wps=13730.7, ups=2.1, wpb=6540, bsz=200, num_updates=692, lr=1.384e-05, gnorm=3.041, loss_scale=0.125, train_wall=1, gb_free=4.8, wall=373\n",
            "2023-08-16 09:05:23 | INFO | train_inner | epoch 001:    705 / 6299 loss=7.42, nll_loss=5.737, ppl=53.34, wps=13632.6, ups=2.15, wpb=6348, bsz=196, num_updates=694, lr=1.388e-05, gnorm=3.228, loss_scale=0.125, train_wall=1, gb_free=5.2, wall=374\n",
            "2023-08-16 09:05:24 | INFO | train_inner | epoch 001:    707 / 6299 loss=7.365, nll_loss=5.676, ppl=51.13, wps=13031.3, ups=2.15, wpb=6056.5, bsz=240, num_updates=696, lr=1.392e-05, gnorm=3.012, loss_scale=0.125, train_wall=1, gb_free=5.1, wall=375\n",
            "2023-08-16 09:05:25 | INFO | train_inner | epoch 001:    709 / 6299 loss=7.469, nll_loss=5.759, ppl=54.17, wps=14185, ups=2.07, wpb=6856, bsz=160, num_updates=698, lr=1.396e-05, gnorm=2.812, loss_scale=0.125, train_wall=1, gb_free=4.9, wall=376\n",
            "2023-08-16 09:05:26 | INFO | train_inner | epoch 001:    711 / 6299 loss=7.227, nll_loss=5.507, ppl=45.49, wps=13920.5, ups=2.02, wpb=6893.5, bsz=476, num_updates=700, lr=1.4e-05, gnorm=4.49, loss_scale=0.125, train_wall=1, gb_free=5.3, wall=377\n",
            "2023-08-16 09:05:27 | INFO | train_inner | epoch 001:    713 / 6299 loss=7.417, nll_loss=5.693, ppl=51.72, wps=14196.3, ups=2.18, wpb=6506.5, bsz=196, num_updates=702, lr=1.404e-05, gnorm=3.037, loss_scale=0.125, train_wall=1, gb_free=4.9, wall=378\n",
            "2023-08-16 09:05:28 | INFO | train_inner | epoch 001:    715 / 6299 loss=7.572, nll_loss=5.885, ppl=59.1, wps=13154.9, ups=2.17, wpb=6064, bsz=112, num_updates=704, lr=1.408e-05, gnorm=3.749, loss_scale=0.125, train_wall=1, gb_free=5.4, wall=379\n",
            "2023-08-16 09:05:29 | INFO | train_inner | epoch 001:    717 / 6299 loss=7.27, nll_loss=5.502, ppl=45.32, wps=13798.3, ups=2.11, wpb=6541, bsz=252, num_updates=706, lr=1.412e-05, gnorm=2.807, loss_scale=0.125, train_wall=1, gb_free=5.1, wall=380\n",
            "2023-08-16 09:05:30 | INFO | train_inner | epoch 001:    719 / 6299 loss=7.063, nll_loss=5.278, ppl=38.79, wps=13954.1, ups=1.99, wpb=7020, bsz=408, num_updates=708, lr=1.416e-05, gnorm=3.34, loss_scale=0.125, train_wall=1, gb_free=4.8, wall=381\n",
            "2023-08-16 09:05:31 | INFO | train_inner | epoch 001:    721 / 6299 loss=7.342, nll_loss=5.589, ppl=48.15, wps=15008.2, ups=1.98, wpb=7593.5, bsz=204, num_updates=710, lr=1.42e-05, gnorm=3.149, loss_scale=0.125, train_wall=1, gb_free=4.3, wall=382\n",
            "2023-08-16 09:05:31 | INFO | train_inner | epoch 001:    723 / 6299 loss=7.422, nll_loss=5.711, ppl=52.37, wps=13232.7, ups=2.33, wpb=5681.5, bsz=152, num_updates=712, lr=1.424e-05, gnorm=3.022, loss_scale=0.125, train_wall=1, gb_free=6.1, wall=382\n",
            "2023-08-16 09:05:32 | INFO | train_inner | epoch 001:    725 / 6299 loss=7.343, nll_loss=5.623, ppl=49.3, wps=14455.7, ups=2.12, wpb=6808.5, bsz=188, num_updates=714, lr=1.428e-05, gnorm=2.582, loss_scale=0.125, train_wall=1, gb_free=4.9, wall=383\n",
            "2023-08-16 09:05:33 | INFO | train_inner | epoch 001:    727 / 6299 loss=7.301, nll_loss=5.593, ppl=48.26, wps=14012.5, ups=2.08, wpb=6724.5, bsz=304, num_updates=716, lr=1.432e-05, gnorm=3.611, loss_scale=0.125, train_wall=1, gb_free=4.9, wall=384\n",
            "2023-08-16 09:05:34 | INFO | train_inner | epoch 001:    729 / 6299 loss=7.04, nll_loss=5.284, ppl=38.98, wps=13119, ups=2.12, wpb=6180, bsz=396, num_updates=718, lr=1.436e-05, gnorm=4.584, loss_scale=0.125, train_wall=1, gb_free=5.3, wall=385\n",
            "2023-08-16 09:05:35 | INFO | train_inner | epoch 001:    731 / 6299 loss=7.271, nll_loss=5.531, ppl=46.23, wps=15090.3, ups=2.13, wpb=7082, bsz=216, num_updates=720, lr=1.44e-05, gnorm=2.849, loss_scale=0.125, train_wall=1, gb_free=4.9, wall=386\n",
            "2023-08-16 09:05:36 | INFO | train_inner | epoch 001:    733 / 6299 loss=7.216, nll_loss=5.459, ppl=43.97, wps=13205.4, ups=2.21, wpb=5988, bsz=180, num_updates=722, lr=1.444e-05, gnorm=2.886, loss_scale=0.125, train_wall=1, gb_free=5.3, wall=387\n",
            "2023-08-16 09:05:37 | INFO | train_inner | epoch 001:    735 / 6299 loss=7.358, nll_loss=5.636, ppl=49.72, wps=13881, ups=2.06, wpb=6741, bsz=192, num_updates=724, lr=1.448e-05, gnorm=3.036, loss_scale=0.125, train_wall=1, gb_free=4.5, wall=388\n",
            "2023-08-16 09:05:38 | INFO | train_inner | epoch 001:    737 / 6299 loss=7.304, nll_loss=5.551, ppl=46.88, wps=13384.1, ups=2.12, wpb=6307.5, bsz=164, num_updates=726, lr=1.452e-05, gnorm=2.605, loss_scale=0.125, train_wall=1, gb_free=5.2, wall=389\n",
            "2023-08-16 09:05:39 | INFO | train_inner | epoch 001:    739 / 6299 loss=7.35, nll_loss=5.624, ppl=49.3, wps=14617.1, ups=2.12, wpb=6884, bsz=144, num_updates=728, lr=1.456e-05, gnorm=2.795, loss_scale=0.125, train_wall=1, gb_free=4.5, wall=390\n",
            "2023-08-16 09:05:40 | INFO | train_inner | epoch 001:    741 / 6299 loss=7.318, nll_loss=5.605, ppl=48.67, wps=12357.3, ups=2.15, wpb=5736, bsz=104, num_updates=730, lr=1.46e-05, gnorm=3.082, loss_scale=0.125, train_wall=1, gb_free=5.3, wall=391\n",
            "2023-08-16 09:05:41 | INFO | train_inner | epoch 001:    743 / 6299 loss=7.261, nll_loss=5.545, ppl=46.69, wps=14177.8, ups=2.13, wpb=6666.5, bsz=248, num_updates=732, lr=1.464e-05, gnorm=3.504, loss_scale=0.125, train_wall=1, gb_free=5.3, wall=392\n",
            "2023-08-16 09:05:42 | INFO | train_inner | epoch 001:    745 / 6299 loss=7.04, nll_loss=5.263, ppl=38.39, wps=13704.5, ups=2.07, wpb=6612, bsz=268, num_updates=734, lr=1.468e-05, gnorm=2.961, loss_scale=0.125, train_wall=1, gb_free=5.3, wall=393\n",
            "2023-08-16 09:05:43 | INFO | train_inner | epoch 001:    747 / 6299 loss=7.193, nll_loss=5.467, ppl=44.24, wps=13738.9, ups=2.01, wpb=6818.5, bsz=232, num_updates=736, lr=1.472e-05, gnorm=2.941, loss_scale=0.125, train_wall=1, gb_free=4.8, wall=394\n",
            "2023-08-16 09:05:44 | INFO | train_inner | epoch 001:    749 / 6299 loss=6.847, nll_loss=5.05, ppl=33.13, wps=12981.5, ups=2.12, wpb=6112, bsz=416, num_updates=738, lr=1.476e-05, gnorm=4.039, loss_scale=0.125, train_wall=1, gb_free=5.3, wall=395\n",
            "2023-08-16 09:05:45 | INFO | train_inner | epoch 001:    751 / 6299 loss=7.237, nll_loss=5.483, ppl=44.72, wps=13509.2, ups=2.05, wpb=6605.5, bsz=180, num_updates=740, lr=1.48e-05, gnorm=3.384, loss_scale=0.125, train_wall=1, gb_free=5.1, wall=396\n",
            "2023-08-16 09:05:46 | INFO | train_inner | epoch 001:    753 / 6299 loss=7.133, nll_loss=5.359, ppl=41.05, wps=12709.5, ups=2.08, wpb=6099, bsz=176, num_updates=742, lr=1.484e-05, gnorm=3.11, loss_scale=0.125, train_wall=1, gb_free=5.1, wall=397\n",
            "2023-08-16 09:05:47 | INFO | train_inner | epoch 001:    755 / 6299 loss=7.082, nll_loss=5.294, ppl=39.25, wps=13843.1, ups=2.02, wpb=6852, bsz=204, num_updates=744, lr=1.488e-05, gnorm=2.571, loss_scale=0.125, train_wall=1, gb_free=4.6, wall=398\n",
            "2023-08-16 09:05:48 | INFO | train_inner | epoch 001:    757 / 6299 loss=7.069, nll_loss=5.304, ppl=39.52, wps=12622, ups=1.96, wpb=6440, bsz=220, num_updates=746, lr=1.492e-05, gnorm=2.986, loss_scale=0.125, train_wall=1, gb_free=5, wall=399\n",
            "2023-08-16 09:05:49 | INFO | train_inner | epoch 001:    759 / 6299 loss=7.056, nll_loss=5.285, ppl=39, wps=14015.9, ups=2.09, wpb=6720, bsz=184, num_updates=748, lr=1.496e-05, gnorm=2.74, loss_scale=0.125, train_wall=1, gb_free=4.8, wall=400\n",
            "2023-08-16 09:05:50 | INFO | train_inner | epoch 001:    761 / 6299 loss=7.197, nll_loss=5.473, ppl=44.4, wps=13114.6, ups=2.1, wpb=6239.5, bsz=168, num_updates=750, lr=1.5e-05, gnorm=2.811, loss_scale=0.125, train_wall=1, gb_free=5.3, wall=401\n",
            "2023-08-16 09:05:51 | INFO | train_inner | epoch 001:    763 / 6299 loss=7.132, nll_loss=5.392, ppl=41.99, wps=13830.8, ups=2.11, wpb=6540.5, bsz=176, num_updates=752, lr=1.504e-05, gnorm=3.037, loss_scale=0.125, train_wall=1, gb_free=5.4, wall=402\n",
            "2023-08-16 09:05:51 | INFO | train_inner | epoch 001:    765 / 6299 loss=7.242, nll_loss=5.487, ppl=44.85, wps=12276, ups=2.23, wpb=5501.5, bsz=136, num_updates=754, lr=1.508e-05, gnorm=4.817, loss_scale=0.125, train_wall=1, gb_free=4.6, wall=402\n",
            "2023-08-16 09:05:52 | INFO | train_inner | epoch 001:    767 / 6299 loss=6.87, nll_loss=5.083, ppl=33.89, wps=14098.4, ups=2.02, wpb=6968, bsz=332, num_updates=756, lr=1.512e-05, gnorm=2.905, loss_scale=0.125, train_wall=1, gb_free=4.7, wall=403\n",
            "2023-08-16 09:05:54 | INFO | train_inner | epoch 001:    769 / 6299 loss=6.758, nll_loss=4.957, ppl=31.06, wps=10523.9, ups=1.72, wpb=6102, bsz=420, num_updates=758, lr=1.516e-05, gnorm=3.875, loss_scale=0.125, train_wall=1, gb_free=4.9, wall=405\n",
            "2023-08-16 09:05:55 | INFO | train_inner | epoch 001:    771 / 6299 loss=7.203, nll_loss=5.466, ppl=44.2, wps=13591.3, ups=2.12, wpb=6420, bsz=164, num_updates=760, lr=1.52e-05, gnorm=2.926, loss_scale=0.125, train_wall=1, gb_free=5.3, wall=406\n",
            "2023-08-16 09:05:55 | INFO | train_inner | epoch 001:    773 / 6299 loss=7.013, nll_loss=5.272, ppl=38.65, wps=13008.1, ups=2.13, wpb=6098.5, bsz=300, num_updates=762, lr=1.524e-05, gnorm=3.603, loss_scale=0.125, train_wall=1, gb_free=4.9, wall=406\n",
            "2023-08-16 09:05:56 | INFO | train_inner | epoch 001:    775 / 6299 loss=6.93, nll_loss=5.172, ppl=36.06, wps=13252.6, ups=2.03, wpb=6512.5, bsz=208, num_updates=764, lr=1.528e-05, gnorm=2.511, loss_scale=0.125, train_wall=1, gb_free=5, wall=407\n",
            "2023-08-16 09:05:57 | INFO | train_inner | epoch 001:    777 / 6299 loss=6.976, nll_loss=5.215, ppl=37.14, wps=15300, ups=2.21, wpb=6921, bsz=296, num_updates=766, lr=1.532e-05, gnorm=3.127, loss_scale=0.125, train_wall=1, gb_free=5.5, wall=408\n",
            "2023-08-16 09:05:58 | INFO | train_inner | epoch 001:    779 / 6299 loss=6.434, nll_loss=4.595, ppl=24.16, wps=12533.7, ups=2.06, wpb=6096, bsz=476, num_updates=768, lr=1.536e-05, gnorm=4.131, loss_scale=0.125, train_wall=1, gb_free=5, wall=409\n",
            "2023-08-16 09:05:59 | INFO | train_inner | epoch 001:    781 / 6299 loss=7.077, nll_loss=5.336, ppl=40.4, wps=13388.8, ups=2.05, wpb=6545.5, bsz=132, num_updates=770, lr=1.54e-05, gnorm=3.081, loss_scale=0.125, train_wall=1, gb_free=3.7, wall=410\n",
            "2023-08-16 09:06:00 | INFO | train_inner | epoch 001:    783 / 6299 loss=6.639, nll_loss=4.806, ppl=27.98, wps=14129.9, ups=2.01, wpb=7040, bsz=376, num_updates=772, lr=1.544e-05, gnorm=2.762, loss_scale=0.125, train_wall=1, gb_free=5.4, wall=411\n",
            "2023-08-16 09:06:01 | INFO | train_inner | epoch 001:    785 / 6299 loss=6.784, nll_loss=4.978, ppl=31.51, wps=13336.9, ups=1.99, wpb=6696, bsz=352, num_updates=774, lr=1.548e-05, gnorm=3.107, loss_scale=0.125, train_wall=1, gb_free=4.9, wall=412\n",
            "2023-08-16 09:06:02 | INFO | train_inner | epoch 001:    787 / 6299 loss=6.986, nll_loss=5.209, ppl=36.98, wps=13912.1, ups=2.12, wpb=6568, bsz=164, num_updates=776, lr=1.552e-05, gnorm=2.532, loss_scale=0.125, train_wall=1, gb_free=4.5, wall=413\n",
            "2023-08-16 09:06:03 | INFO | train_inner | epoch 001:    789 / 6299 loss=6.713, nll_loss=4.9, ppl=29.85, wps=14205.6, ups=2.07, wpb=6855.5, bsz=328, num_updates=778, lr=1.556e-05, gnorm=2.729, loss_scale=0.125, train_wall=1, gb_free=4.5, wall=414\n",
            "2023-08-16 09:06:04 | INFO | train_inner | epoch 001:    791 / 6299 loss=6.673, nll_loss=4.846, ppl=28.77, wps=13907.7, ups=2.18, wpb=6380, bsz=264, num_updates=780, lr=1.56e-05, gnorm=2.537, loss_scale=0.125, train_wall=1, gb_free=5.7, wall=415\n",
            "2023-08-16 09:06:05 | INFO | train_inner | epoch 001:    793 / 6299 loss=6.762, nll_loss=4.956, ppl=31.03, wps=14037.4, ups=2.13, wpb=6575.5, bsz=184, num_updates=782, lr=1.564e-05, gnorm=2.319, loss_scale=0.125, train_wall=1, gb_free=4.9, wall=416\n",
            "2023-08-16 09:06:06 | INFO | train_inner | epoch 001:    795 / 6299 loss=6.917, nll_loss=5.152, ppl=35.57, wps=12961.7, ups=2.1, wpb=6165.5, bsz=136, num_updates=784, lr=1.568e-05, gnorm=2.343, loss_scale=0.125, train_wall=1, gb_free=5.4, wall=417\n",
            "2023-08-16 09:06:07 | INFO | train_inner | epoch 001:    797 / 6299 loss=6.789, nll_loss=4.991, ppl=31.8, wps=14227.1, ups=2.1, wpb=6760, bsz=252, num_updates=786, lr=1.572e-05, gnorm=2.475, loss_scale=0.125, train_wall=1, gb_free=4.9, wall=418\n",
            "2023-08-16 09:06:08 | INFO | train_inner | epoch 001:    799 / 6299 loss=6.786, nll_loss=4.992, ppl=31.81, wps=14933.9, ups=2.07, wpb=7226.5, bsz=200, num_updates=788, lr=1.576e-05, gnorm=2.305, loss_scale=0.125, train_wall=1, gb_free=4.8, wall=419\n",
            "2023-08-16 09:06:09 | INFO | train_inner | epoch 001:    801 / 6299 loss=7.149, nll_loss=5.441, ppl=43.44, wps=12747.8, ups=2.14, wpb=5954, bsz=104, num_updates=790, lr=1.58e-05, gnorm=3.159, loss_scale=0.125, train_wall=1, gb_free=5.7, wall=420\n",
            "2023-08-16 09:06:10 | INFO | train_inner | epoch 001:    803 / 6299 loss=6.41, nll_loss=4.559, ppl=23.57, wps=13986.1, ups=1.96, wpb=7120, bsz=436, num_updates=792, lr=1.584e-05, gnorm=2.735, loss_scale=0.125, train_wall=1, gb_free=4.5, wall=421\n",
            "2023-08-16 09:06:11 | INFO | train_inner | epoch 001:    805 / 6299 loss=6.641, nll_loss=4.816, ppl=28.16, wps=14026.7, ups=2.14, wpb=6567.5, bsz=236, num_updates=794, lr=1.588e-05, gnorm=2.447, loss_scale=0.125, train_wall=1, gb_free=4.8, wall=422\n",
            "2023-08-16 09:06:12 | INFO | train_inner | epoch 001:    807 / 6299 loss=6.562, nll_loss=4.724, ppl=26.43, wps=14325.5, ups=1.99, wpb=7202.5, bsz=300, num_updates=796, lr=1.592e-05, gnorm=2.37, loss_scale=0.125, train_wall=1, gb_free=4.4, wall=423\n",
            "2023-08-16 09:06:13 | INFO | train_inner | epoch 001:    809 / 6299 loss=6.723, nll_loss=4.902, ppl=29.9, wps=14181.3, ups=1.97, wpb=7204, bsz=232, num_updates=798, lr=1.596e-05, gnorm=2.486, loss_scale=0.125, train_wall=1, gb_free=4.7, wall=424\n",
            "2023-08-16 09:06:14 | INFO | train_inner | epoch 001:    811 / 6299 loss=6.797, nll_loss=5.012, ppl=32.26, wps=14492.8, ups=1.97, wpb=7352, bsz=184, num_updates=800, lr=1.6e-05, gnorm=2.339, loss_scale=0.125, train_wall=1, gb_free=4.5, wall=425\n",
            "2023-08-16 09:06:15 | INFO | train_inner | epoch 001:    813 / 6299 loss=6.955, nll_loss=5.225, ppl=37.41, wps=14051.9, ups=2.11, wpb=6655, bsz=328, num_updates=802, lr=1.604e-05, gnorm=3.49, loss_scale=0.125, train_wall=1, gb_free=4.9, wall=426\n",
            "2023-08-16 09:06:16 | INFO | train_inner | epoch 001:    815 / 6299 loss=6.556, nll_loss=4.738, ppl=26.68, wps=14593.5, ups=2.05, wpb=7112, bsz=292, num_updates=804, lr=1.608e-05, gnorm=2.421, loss_scale=0.125, train_wall=1, gb_free=4.6, wall=427\n",
            "2023-08-16 09:06:17 | INFO | train_inner | epoch 001:    817 / 6299 loss=6.558, nll_loss=4.736, ppl=26.65, wps=13772.8, ups=2.1, wpb=6556, bsz=244, num_updates=806, lr=1.612e-05, gnorm=2.38, loss_scale=0.125, train_wall=1, gb_free=5, wall=428\n",
            "2023-08-16 09:06:18 | INFO | train_inner | epoch 001:    819 / 6299 loss=6.481, nll_loss=4.648, ppl=25.07, wps=12686.3, ups=2.15, wpb=5910, bsz=384, num_updates=808, lr=1.616e-05, gnorm=3.448, loss_scale=0.125, train_wall=1, gb_free=6.1, wall=429\n",
            "2023-08-16 09:06:19 | INFO | train_inner | epoch 001:    821 / 6299 loss=6.814, nll_loss=5.016, ppl=32.35, wps=14243.6, ups=2.12, wpb=6724, bsz=160, num_updates=810, lr=1.62e-05, gnorm=2.756, loss_scale=0.125, train_wall=1, gb_free=4.9, wall=430\n",
            "2023-08-16 09:06:20 | INFO | train_inner | epoch 001:    823 / 6299 loss=6.729, nll_loss=4.912, ppl=30.11, wps=12725.4, ups=2.04, wpb=6231, bsz=184, num_updates=812, lr=1.624e-05, gnorm=2.751, loss_scale=0.125, train_wall=1, gb_free=5.3, wall=431\n",
            "2023-08-16 09:06:21 | INFO | train_inner | epoch 001:    825 / 6299 loss=6.558, nll_loss=4.721, ppl=26.38, wps=14362.4, ups=2, wpb=7178, bsz=268, num_updates=814, lr=1.628e-05, gnorm=2.519, loss_scale=0.125, train_wall=1, gb_free=4.7, wall=432\n",
            "2023-08-16 09:06:21 | INFO | train_inner | epoch 001:    827 / 6299 loss=6.75, nll_loss=4.947, ppl=30.86, wps=13474.3, ups=2.22, wpb=6076, bsz=172, num_updates=816, lr=1.632e-05, gnorm=2.534, loss_scale=0.125, train_wall=1, gb_free=4.8, wall=432\n",
            "2023-08-16 09:06:22 | INFO | train_inner | epoch 001:    829 / 6299 loss=6.996, nll_loss=5.249, ppl=38.04, wps=12545.4, ups=2.32, wpb=5397, bsz=116, num_updates=818, lr=1.636e-05, gnorm=5.191, loss_scale=0.125, train_wall=1, gb_free=5.7, wall=433\n",
            "2023-08-16 09:06:23 | INFO | train_inner | epoch 001:    831 / 6299 loss=6.599, nll_loss=4.815, ppl=28.15, wps=14503.6, ups=2.13, wpb=6806, bsz=232, num_updates=820, lr=1.64e-05, gnorm=2.357, loss_scale=0.125, train_wall=1, gb_free=4.9, wall=434\n",
            "2023-08-16 09:06:24 | INFO | train_inner | epoch 001:    833 / 6299 loss=6.766, nll_loss=5.012, ppl=32.27, wps=13401.8, ups=2.28, wpb=5869.5, bsz=124, num_updates=822, lr=1.644e-05, gnorm=3.114, loss_scale=0.125, train_wall=1, gb_free=5.6, wall=435\n",
            "2023-08-16 09:06:25 | INFO | train_inner | epoch 001:    835 / 6299 loss=6.7, nll_loss=4.904, ppl=29.95, wps=12472.2, ups=2.15, wpb=5804, bsz=172, num_updates=824, lr=1.648e-05, gnorm=3.152, loss_scale=0.125, train_wall=1, gb_free=5.2, wall=436\n",
            "2023-08-16 09:06:26 | INFO | train_inner | epoch 001:    837 / 6299 loss=6.568, nll_loss=4.725, ppl=26.45, wps=12994.4, ups=2.05, wpb=6341.5, bsz=184, num_updates=826, lr=1.652e-05, gnorm=2.461, loss_scale=0.125, train_wall=1, gb_free=5, wall=437\n",
            "2023-08-16 09:06:27 | INFO | train_inner | epoch 001:    839 / 6299 loss=6.666, nll_loss=4.843, ppl=28.7, wps=12755.5, ups=2.07, wpb=6168, bsz=208, num_updates=828, lr=1.656e-05, gnorm=3.149, loss_scale=0.125, train_wall=1, gb_free=5.9, wall=438\n",
            "2023-08-16 09:06:28 | INFO | train_inner | epoch 001:    841 / 6299 loss=6.861, nll_loss=5.092, ppl=34.1, wps=13469, ups=2.15, wpb=6253, bsz=120, num_updates=830, lr=1.66e-05, gnorm=2.97, loss_scale=0.125, train_wall=1, gb_free=4.8, wall=439\n",
            "2023-08-16 09:06:29 | INFO | train_inner | epoch 001:    843 / 6299 loss=6.737, nll_loss=4.957, ppl=31.05, wps=12716.5, ups=2.32, wpb=5489.5, bsz=96, num_updates=832, lr=1.664e-05, gnorm=2.766, loss_scale=0.125, train_wall=1, gb_free=5.5, wall=440\n",
            "2023-08-16 09:06:30 | INFO | train_inner | epoch 001:    845 / 6299 loss=6.165, nll_loss=4.308, ppl=19.81, wps=14010.3, ups=1.98, wpb=7084, bsz=400, num_updates=834, lr=1.668e-05, gnorm=2.491, loss_scale=0.125, train_wall=1, gb_free=5.1, wall=441\n",
            "2023-08-16 09:06:31 | INFO | train_inner | epoch 001:    847 / 6299 loss=6.16, nll_loss=4.312, ppl=19.87, wps=14849.9, ups=2.08, wpb=7124, bsz=448, num_updates=836, lr=1.672e-05, gnorm=2.741, loss_scale=0.125, train_wall=1, gb_free=4.9, wall=442\n",
            "2023-08-16 09:06:32 | INFO | train_inner | epoch 001:    849 / 6299 loss=6.312, nll_loss=4.488, ppl=22.45, wps=14517.3, ups=2.1, wpb=6908, bsz=348, num_updates=838, lr=1.676e-05, gnorm=2.474, loss_scale=0.125, train_wall=1, gb_free=5.4, wall=443\n",
            "2023-08-16 09:06:33 | INFO | train_inner | epoch 001:    851 / 6299 loss=6.631, nll_loss=4.842, ppl=28.68, wps=14203.6, ups=2.12, wpb=6692, bsz=140, num_updates=840, lr=1.68e-05, gnorm=2.326, loss_scale=0.125, train_wall=1, gb_free=5.1, wall=444\n",
            "2023-08-16 09:06:34 | INFO | train_inner | epoch 001:    853 / 6299 loss=6.631, nll_loss=4.832, ppl=28.48, wps=13525.4, ups=2.08, wpb=6491, bsz=124, num_updates=842, lr=1.684e-05, gnorm=2.483, loss_scale=0.125, train_wall=1, gb_free=4.4, wall=445\n",
            "2023-08-16 09:06:35 | INFO | train_inner | epoch 001:    855 / 6299 loss=6.47, nll_loss=4.627, ppl=24.7, wps=13086.3, ups=2.26, wpb=5794, bsz=152, num_updates=844, lr=1.688e-05, gnorm=2.344, loss_scale=0.125, train_wall=1, gb_free=5.4, wall=446\n",
            "2023-08-16 09:06:36 | INFO | train_inner | epoch 001:    857 / 6299 loss=6.332, nll_loss=4.447, ppl=21.81, wps=13809.3, ups=2.08, wpb=6648, bsz=260, num_updates=846, lr=1.692e-05, gnorm=2.334, loss_scale=0.125, train_wall=1, gb_free=4.6, wall=447\n",
            "2023-08-16 09:06:37 | INFO | train_inner | epoch 001:    859 / 6299 loss=6.462, nll_loss=4.608, ppl=24.39, wps=15132.4, ups=1.99, wpb=7592, bsz=272, num_updates=848, lr=1.696e-05, gnorm=2.346, loss_scale=0.125, train_wall=1, gb_free=4, wall=448\n",
            "2023-08-16 09:06:38 | INFO | train_inner | epoch 001:    861 / 6299 loss=6.487, nll_loss=4.694, ppl=25.88, wps=14944.9, ups=1.95, wpb=7664, bsz=588, num_updates=850, lr=1.7e-05, gnorm=4.034, loss_scale=0.125, train_wall=1, gb_free=4.6, wall=449\n",
            "2023-08-16 09:06:39 | INFO | train_inner | epoch 001:    863 / 6299 loss=6.649, nll_loss=4.891, ppl=29.67, wps=14208.7, ups=2, wpb=7092, bsz=116, num_updates=852, lr=1.704e-05, gnorm=2.534, loss_scale=0.125, train_wall=1, gb_free=4.3, wall=450\n",
            "2023-08-16 09:06:40 | INFO | train_inner | epoch 001:    865 / 6299 loss=6.285, nll_loss=4.46, ppl=22.01, wps=13649, ups=2, wpb=6812, bsz=264, num_updates=854, lr=1.708e-05, gnorm=2.2, loss_scale=0.125, train_wall=1, gb_free=4.7, wall=451\n",
            "2023-08-16 09:06:41 | INFO | train_inner | epoch 001:    867 / 6299 loss=6.273, nll_loss=4.438, ppl=21.68, wps=14191.2, ups=2.07, wpb=6863.5, bsz=272, num_updates=856, lr=1.712e-05, gnorm=2.638, loss_scale=0.125, train_wall=1, gb_free=4.7, wall=452\n",
            "2023-08-16 09:06:41 | INFO | train_inner | epoch 001:    869 / 6299 loss=6.315, nll_loss=4.47, ppl=22.16, wps=13816.2, ups=2.06, wpb=6692, bsz=252, num_updates=858, lr=1.716e-05, gnorm=2.41, loss_scale=0.125, train_wall=1, gb_free=4.5, wall=452\n",
            "2023-08-16 09:06:42 | INFO | train_inner | epoch 001:    871 / 6299 loss=6.168, nll_loss=4.289, ppl=19.55, wps=14092.4, ups=2, wpb=7037.5, bsz=364, num_updates=860, lr=1.72e-05, gnorm=2.424, loss_scale=0.125, train_wall=1, gb_free=4.8, wall=453\n",
            "2023-08-16 09:06:43 | INFO | train_inner | epoch 001:    873 / 6299 loss=6.294, nll_loss=4.42, ppl=21.4, wps=14319.4, ups=2.04, wpb=7020.5, bsz=252, num_updates=862, lr=1.724e-05, gnorm=2.276, loss_scale=0.125, train_wall=1, gb_free=4.7, wall=454\n",
            "2023-08-16 09:06:44 | INFO | train_inner | epoch 001:    875 / 6299 loss=6.742, nll_loss=4.977, ppl=31.5, wps=13839, ups=2.13, wpb=6502.5, bsz=80, num_updates=864, lr=1.728e-05, gnorm=3.346, loss_scale=0.125, train_wall=1, gb_free=4.1, wall=455\n",
            "2023-08-16 09:06:45 | INFO | train_inner | epoch 001:    877 / 6299 loss=6.287, nll_loss=4.42, ppl=21.41, wps=12871.9, ups=2.08, wpb=6182, bsz=264, num_updates=866, lr=1.732e-05, gnorm=2.797, loss_scale=0.125, train_wall=1, gb_free=4.5, wall=456\n",
            "2023-08-16 09:06:46 | INFO | train_inner | epoch 001:    879 / 6299 loss=6.353, nll_loss=4.529, ppl=23.09, wps=12912.8, ups=2.1, wpb=6148, bsz=216, num_updates=868, lr=1.736e-05, gnorm=2.722, loss_scale=0.125, train_wall=1, gb_free=5.4, wall=457\n",
            "2023-08-16 09:06:47 | INFO | train_inner | epoch 001:    881 / 6299 loss=6.078, nll_loss=4.232, ppl=18.8, wps=14093.3, ups=2.11, wpb=6684, bsz=488, num_updates=870, lr=1.74e-05, gnorm=2.714, loss_scale=0.125, train_wall=1, gb_free=5.1, wall=458\n",
            "2023-08-16 09:06:48 | INFO | train_inner | epoch 001:    883 / 6299 loss=6.186, nll_loss=4.326, ppl=20.05, wps=14493.4, ups=2.06, wpb=7026.5, bsz=268, num_updates=872, lr=1.744e-05, gnorm=2.082, loss_scale=0.125, train_wall=1, gb_free=5.3, wall=459\n",
            "2023-08-16 09:06:49 | INFO | train_inner | epoch 001:    885 / 6299 loss=6.482, nll_loss=4.652, ppl=25.14, wps=13890.1, ups=2.13, wpb=6520, bsz=120, num_updates=874, lr=1.748e-05, gnorm=2.553, loss_scale=0.125, train_wall=1, gb_free=5.1, wall=460\n",
            "2023-08-16 09:06:50 | INFO | train_inner | epoch 001:    887 / 6299 loss=6.098, nll_loss=4.207, ppl=18.47, wps=15143.9, ups=1.98, wpb=7650.5, bsz=420, num_updates=876, lr=1.752e-05, gnorm=2.495, loss_scale=0.125, train_wall=1, gb_free=4.9, wall=461\n",
            "2023-08-16 09:06:51 | INFO | train_inner | epoch 001:    889 / 6299 loss=6.408, nll_loss=4.574, ppl=23.82, wps=14373.8, ups=2.02, wpb=7112, bsz=216, num_updates=878, lr=1.756e-05, gnorm=2.23, loss_scale=0.125, train_wall=1, gb_free=4.8, wall=462\n",
            "2023-08-16 09:06:52 | INFO | train_inner | epoch 001:    891 / 6299 loss=5.952, nll_loss=4.104, ppl=17.19, wps=13036.4, ups=1.95, wpb=6692, bsz=624, num_updates=880, lr=1.76e-05, gnorm=5.364, loss_scale=0.125, train_wall=1, gb_free=4.6, wall=463\n",
            "2023-08-16 09:06:53 | INFO | train_inner | epoch 001:    893 / 6299 loss=6.107, nll_loss=4.26, ppl=19.16, wps=14285.4, ups=1.9, wpb=7525, bsz=348, num_updates=882, lr=1.764e-05, gnorm=1.969, loss_scale=0.125, train_wall=1, gb_free=4.7, wall=464\n",
            "2023-08-16 09:06:54 | INFO | train_inner | epoch 001:    895 / 6299 loss=5.933, nll_loss=4.057, ppl=16.65, wps=12964.7, ups=2.04, wpb=6344, bsz=320, num_updates=884, lr=1.768e-05, gnorm=2.272, loss_scale=0.125, train_wall=1, gb_free=5, wall=465\n",
            "2023-08-16 09:06:55 | INFO | train_inner | epoch 001:    897 / 6299 loss=6.231, nll_loss=4.396, ppl=21.06, wps=13386.7, ups=2.13, wpb=6284, bsz=200, num_updates=886, lr=1.772e-05, gnorm=2.442, loss_scale=0.125, train_wall=1, gb_free=5.2, wall=466\n",
            "2023-08-16 09:06:56 | INFO | train_inner | epoch 001:    899 / 6299 loss=6.354, nll_loss=4.512, ppl=22.81, wps=13696.6, ups=2.07, wpb=6618, bsz=132, num_updates=888, lr=1.776e-05, gnorm=2.363, loss_scale=0.125, train_wall=1, gb_free=5.1, wall=467\n",
            "2023-08-16 09:06:57 | INFO | train_inner | epoch 001:    901 / 6299 loss=6.208, nll_loss=4.316, ppl=19.91, wps=12843.8, ups=2.16, wpb=5953.5, bsz=196, num_updates=890, lr=1.78e-05, gnorm=2.288, loss_scale=0.125, train_wall=1, gb_free=5.2, wall=468\n",
            "2023-08-16 09:06:58 | INFO | train_inner | epoch 001:    903 / 6299 loss=6.381, nll_loss=4.518, ppl=22.91, wps=13137.4, ups=2.24, wpb=5857, bsz=144, num_updates=892, lr=1.784e-05, gnorm=3.266, loss_scale=0.125, train_wall=1, gb_free=5.3, wall=469\n",
            "2023-08-16 09:06:59 | INFO | train_inner | epoch 001:    905 / 6299 loss=6.318, nll_loss=4.459, ppl=21.99, wps=14090.6, ups=2.11, wpb=6686.5, bsz=200, num_updates=894, lr=1.788e-05, gnorm=2.277, loss_scale=0.125, train_wall=1, gb_free=4.5, wall=470\n",
            "2023-08-16 09:07:00 | INFO | train_inner | epoch 001:    907 / 6299 loss=6.149, nll_loss=4.288, ppl=19.53, wps=12865.8, ups=2.05, wpb=6284, bsz=212, num_updates=896, lr=1.792e-05, gnorm=1.991, loss_scale=0.125, train_wall=1, gb_free=5.6, wall=471\n",
            "2023-08-16 09:07:01 | INFO | train_inner | epoch 001:    909 / 6299 loss=6.195, nll_loss=4.357, ppl=20.49, wps=12998.3, ups=2.07, wpb=6265.5, bsz=172, num_updates=898, lr=1.796e-05, gnorm=2.048, loss_scale=0.125, train_wall=1, gb_free=4.9, wall=472\n",
            "2023-08-16 09:07:02 | INFO | train_inner | epoch 001:    911 / 6299 loss=6.438, nll_loss=4.652, ppl=25.14, wps=14106.7, ups=2.19, wpb=6451.5, bsz=160, num_updates=900, lr=1.8e-05, gnorm=2.566, loss_scale=0.125, train_wall=1, gb_free=5, wall=473\n",
            "2023-08-16 09:07:03 | INFO | train_inner | epoch 001:    913 / 6299 loss=6.146, nll_loss=4.304, ppl=19.76, wps=14062.6, ups=2.02, wpb=6976, bsz=304, num_updates=902, lr=1.804e-05, gnorm=2.597, loss_scale=0.125, train_wall=1, gb_free=5.3, wall=474\n",
            "2023-08-16 09:07:04 | INFO | train_inner | epoch 001:    915 / 6299 loss=6.043, nll_loss=4.178, ppl=18.1, wps=12001.8, ups=2.27, wpb=5297, bsz=252, num_updates=904, lr=1.808e-05, gnorm=3.302, loss_scale=0.125, train_wall=1, gb_free=5.3, wall=475\n",
            "2023-08-16 09:07:05 | INFO | train_inner | epoch 001:    917 / 6299 loss=6.254, nll_loss=4.412, ppl=21.29, wps=14772.3, ups=1.96, wpb=7547, bsz=236, num_updates=906, lr=1.812e-05, gnorm=2.219, loss_scale=0.125, train_wall=1, gb_free=4.3, wall=476\n",
            "2023-08-16 09:07:06 | INFO | train_inner | epoch 001:    919 / 6299 loss=6.26, nll_loss=4.425, ppl=21.48, wps=12767.2, ups=2.26, wpb=5653, bsz=156, num_updates=908, lr=1.816e-05, gnorm=2.636, loss_scale=0.125, train_wall=1, gb_free=5, wall=477\n",
            "2023-08-16 09:07:07 | INFO | train_inner | epoch 001:    921 / 6299 loss=6.004, nll_loss=4.117, ppl=17.35, wps=13423.3, ups=2.02, wpb=6653, bsz=288, num_updates=910, lr=1.82e-05, gnorm=1.867, loss_scale=0.125, train_wall=1, gb_free=5.2, wall=478\n",
            "2023-08-16 09:07:07 | INFO | train_inner | epoch 001:    923 / 6299 loss=6.308, nll_loss=4.485, ppl=22.4, wps=10329.6, ups=2.29, wpb=4516, bsz=108, num_updates=912, lr=1.824e-05, gnorm=2.973, loss_scale=0.125, train_wall=1, gb_free=5.2, wall=478\n",
            "2023-08-16 09:07:08 | INFO | train_inner | epoch 001:    925 / 6299 loss=6.452, nll_loss=4.642, ppl=24.98, wps=14159.3, ups=2.13, wpb=6642, bsz=136, num_updates=914, lr=1.828e-05, gnorm=2.754, loss_scale=0.125, train_wall=1, gb_free=5.2, wall=479\n",
            "2023-08-16 09:07:09 | INFO | train_inner | epoch 001:    927 / 6299 loss=5.971, nll_loss=4.083, ppl=16.94, wps=13818.2, ups=1.94, wpb=7116, bsz=332, num_updates=916, lr=1.832e-05, gnorm=1.978, loss_scale=0.125, train_wall=1, gb_free=5.4, wall=480\n",
            "2023-08-16 09:07:10 | INFO | train_inner | epoch 001:    929 / 6299 loss=6.36, nll_loss=4.537, ppl=23.21, wps=14279.6, ups=2.08, wpb=6874, bsz=136, num_updates=918, lr=1.836e-05, gnorm=2.085, loss_scale=0.125, train_wall=1, gb_free=4.9, wall=481\n",
            "2023-08-16 09:07:11 | INFO | train_inner | epoch 001:    931 / 6299 loss=6.04, nll_loss=4.185, ppl=18.19, wps=13219.5, ups=2.07, wpb=6372, bsz=252, num_updates=920, lr=1.84e-05, gnorm=2.214, loss_scale=0.125, train_wall=1, gb_free=5, wall=482\n",
            "2023-08-16 09:07:12 | INFO | train_inner | epoch 001:    933 / 6299 loss=5.958, nll_loss=4.076, ppl=16.87, wps=14467.8, ups=2.08, wpb=6968, bsz=304, num_updates=922, lr=1.844e-05, gnorm=1.883, loss_scale=0.125, train_wall=1, gb_free=5.2, wall=483\n",
            "2023-08-16 09:07:13 | INFO | train_inner | epoch 001:    935 / 6299 loss=6.065, nll_loss=4.2, ppl=18.38, wps=14232.4, ups=2.07, wpb=6864, bsz=236, num_updates=924, lr=1.848e-05, gnorm=2.089, loss_scale=0.125, train_wall=1, gb_free=4.7, wall=484\n",
            "2023-08-16 09:07:14 | INFO | train_inner | epoch 001:    937 / 6299 loss=6.173, nll_loss=4.327, ppl=20.07, wps=14133.7, ups=2.15, wpb=6568, bsz=172, num_updates=926, lr=1.852e-05, gnorm=2.265, loss_scale=0.125, train_wall=1, gb_free=4.9, wall=485\n",
            "2023-08-16 09:07:15 | INFO | train_inner | epoch 001:    939 / 6299 loss=5.931, nll_loss=4.065, ppl=16.73, wps=13985.2, ups=2.06, wpb=6790, bsz=276, num_updates=928, lr=1.856e-05, gnorm=2.007, loss_scale=0.125, train_wall=1, gb_free=4.9, wall=486\n",
            "2023-08-16 09:07:16 | INFO | train_inner | epoch 001:    941 / 6299 loss=5.957, nll_loss=4.097, ppl=17.12, wps=14857, ups=2.07, wpb=7177, bsz=332, num_updates=930, lr=1.86e-05, gnorm=2.674, loss_scale=0.125, train_wall=1, gb_free=4.8, wall=487\n",
            "2023-08-16 09:07:17 | INFO | train_inner | epoch 001:    943 / 6299 loss=6.023, nll_loss=4.17, ppl=18, wps=14513.8, ups=2.11, wpb=6864.5, bsz=268, num_updates=932, lr=1.864e-05, gnorm=1.853, loss_scale=0.125, train_wall=1, gb_free=4.9, wall=488\n",
            "2023-08-16 09:07:18 | INFO | train_inner | epoch 001:    945 / 6299 loss=6.058, nll_loss=4.197, ppl=18.34, wps=13676.2, ups=2.01, wpb=6820, bsz=212, num_updates=934, lr=1.868e-05, gnorm=1.918, loss_scale=0.125, train_wall=1, gb_free=4.5, wall=489\n",
            "2023-08-16 09:07:19 | INFO | train_inner | epoch 001:    947 / 6299 loss=6.058, nll_loss=4.198, ppl=18.35, wps=12070.4, ups=2.04, wpb=5925, bsz=208, num_updates=936, lr=1.872e-05, gnorm=2.595, loss_scale=0.125, train_wall=1, gb_free=4.8, wall=490\n",
            "2023-08-16 09:07:20 | INFO | train_inner | epoch 001:    949 / 6299 loss=6.09, nll_loss=4.233, ppl=18.8, wps=14557.2, ups=1.97, wpb=7405, bsz=276, num_updates=938, lr=1.876e-05, gnorm=2.339, loss_scale=0.125, train_wall=1, gb_free=4.8, wall=491\n",
            "2023-08-16 09:07:21 | INFO | train_inner | epoch 001:    951 / 6299 loss=5.851, nll_loss=3.939, ppl=15.34, wps=14434.5, ups=2.06, wpb=7020, bsz=324, num_updates=940, lr=1.88e-05, gnorm=1.988, loss_scale=0.125, train_wall=1, gb_free=4.9, wall=492\n",
            "2023-08-16 09:07:22 | INFO | train_inner | epoch 001:    953 / 6299 loss=6.042, nll_loss=4.169, ppl=17.98, wps=14481.3, ups=2.05, wpb=7050, bsz=228, num_updates=942, lr=1.884e-05, gnorm=1.947, loss_scale=0.125, train_wall=1, gb_free=4.8, wall=493\n",
            "2023-08-16 09:07:23 | INFO | train_inner | epoch 001:    955 / 6299 loss=6.012, nll_loss=4.149, ppl=17.74, wps=13907.4, ups=2.1, wpb=6629.5, bsz=204, num_updates=944, lr=1.888e-05, gnorm=1.887, loss_scale=0.125, train_wall=1, gb_free=4.9, wall=494\n",
            "2023-08-16 09:07:24 | INFO | train_inner | epoch 001:    957 / 6299 loss=5.668, nll_loss=3.785, ppl=13.78, wps=13802.5, ups=2, wpb=6904, bsz=416, num_updates=946, lr=1.892e-05, gnorm=2.209, loss_scale=0.125, train_wall=1, gb_free=5.1, wall=495\n",
            "2023-08-16 09:07:25 | INFO | train_inner | epoch 001:    959 / 6299 loss=5.977, nll_loss=4.125, ppl=17.45, wps=14661.5, ups=2.08, wpb=7044, bsz=256, num_updates=948, lr=1.896e-05, gnorm=1.828, loss_scale=0.125, train_wall=1, gb_free=4.8, wall=496\n",
            "2023-08-16 09:07:26 | INFO | train_inner | epoch 001:    961 / 6299 loss=5.901, nll_loss=4.028, ppl=16.31, wps=13142.5, ups=2.09, wpb=6281.5, bsz=240, num_updates=950, lr=1.9e-05, gnorm=2.192, loss_scale=0.125, train_wall=1, gb_free=5.2, wall=497\n",
            "2023-08-16 09:07:27 | INFO | train_inner | epoch 001:    963 / 6299 loss=6.039, nll_loss=4.174, ppl=18.05, wps=13174.3, ups=2.16, wpb=6102, bsz=184, num_updates=952, lr=1.904e-05, gnorm=2.127, loss_scale=0.125, train_wall=1, gb_free=5.2, wall=498\n",
            "2023-08-16 09:07:28 | INFO | train_inner | epoch 001:    965 / 6299 loss=6.126, nll_loss=4.259, ppl=19.14, wps=13779.6, ups=2.11, wpb=6532, bsz=120, num_updates=954, lr=1.908e-05, gnorm=2.116, loss_scale=0.125, train_wall=1, gb_free=4.8, wall=499\n",
            "2023-08-16 09:07:29 | INFO | train_inner | epoch 001:    967 / 6299 loss=6.086, nll_loss=4.209, ppl=18.5, wps=13527.4, ups=2.19, wpb=6182, bsz=120, num_updates=956, lr=1.912e-05, gnorm=2.187, loss_scale=0.125, train_wall=1, gb_free=5.3, wall=500\n",
            "2023-08-16 09:07:30 | INFO | train_inner | epoch 001:    969 / 6299 loss=6.076, nll_loss=4.222, ppl=18.66, wps=13994.7, ups=2.15, wpb=6524, bsz=148, num_updates=958, lr=1.916e-05, gnorm=2.11, loss_scale=0.125, train_wall=1, gb_free=5, wall=501\n",
            "2023-08-16 09:07:31 | INFO | train_inner | epoch 001:    971 / 6299 loss=5.943, nll_loss=4.078, ppl=16.88, wps=13192.9, ups=2.15, wpb=6125.5, bsz=156, num_updates=960, lr=1.92e-05, gnorm=2.233, loss_scale=0.125, train_wall=1, gb_free=5.5, wall=502\n",
            "2023-08-16 09:07:31 | INFO | train_inner | epoch 001:    973 / 6299 loss=6.18, nll_loss=4.364, ppl=20.59, wps=13701.8, ups=2.11, wpb=6503, bsz=140, num_updates=962, lr=1.924e-05, gnorm=2.1, loss_scale=0.125, train_wall=1, gb_free=4.8, wall=502\n",
            "2023-08-16 09:07:32 | INFO | train_inner | epoch 001:    975 / 6299 loss=6.057, nll_loss=4.222, ppl=18.66, wps=12896.4, ups=2.08, wpb=6209, bsz=164, num_updates=964, lr=1.928e-05, gnorm=2.075, loss_scale=0.125, train_wall=1, gb_free=5, wall=503\n",
            "2023-08-16 09:07:33 | INFO | train_inner | epoch 001:    977 / 6299 loss=5.926, nll_loss=4.06, ppl=16.68, wps=12933.8, ups=2.04, wpb=6340, bsz=256, num_updates=966, lr=1.932e-05, gnorm=1.995, loss_scale=0.125, train_wall=1, gb_free=5.2, wall=504\n",
            "2023-08-16 09:07:34 | INFO | train_inner | epoch 001:    979 / 6299 loss=5.881, nll_loss=3.999, ppl=15.99, wps=13675.2, ups=2.15, wpb=6372, bsz=212, num_updates=968, lr=1.936e-05, gnorm=1.93, loss_scale=0.125, train_wall=1, gb_free=4.8, wall=505\n",
            "2023-08-16 09:07:35 | INFO | train_inner | epoch 001:    981 / 6299 loss=6.089, nll_loss=4.247, ppl=18.99, wps=13403.2, ups=2.08, wpb=6456, bsz=120, num_updates=970, lr=1.94e-05, gnorm=2.196, loss_scale=0.125, train_wall=1, gb_free=4.7, wall=506\n",
            "2023-08-16 09:07:36 | INFO | train_inner | epoch 001:    983 / 6299 loss=6.04, nll_loss=4.191, ppl=18.27, wps=13472, ups=2.21, wpb=6101.5, bsz=136, num_updates=972, lr=1.944e-05, gnorm=2.183, loss_scale=0.125, train_wall=1, gb_free=5.8, wall=507\n",
            "2023-08-16 09:07:37 | INFO | train_inner | epoch 001:    985 / 6299 loss=5.97, nll_loss=4.096, ppl=17.1, wps=14009.5, ups=2.12, wpb=6603.5, bsz=204, num_updates=974, lr=1.948e-05, gnorm=2.102, loss_scale=0.125, train_wall=1, gb_free=5.1, wall=508\n",
            "2023-08-16 09:07:38 | INFO | train_inner | epoch 001:    987 / 6299 loss=5.899, nll_loss=4.043, ppl=16.48, wps=13522.4, ups=2.1, wpb=6445, bsz=276, num_updates=976, lr=1.952e-05, gnorm=3.072, loss_scale=0.125, train_wall=1, gb_free=5.5, wall=509\n",
            "2023-08-16 09:07:39 | INFO | train_inner | epoch 001:    989 / 6299 loss=5.877, nll_loss=4.012, ppl=16.13, wps=14076.3, ups=2.08, wpb=6783, bsz=212, num_updates=978, lr=1.956e-05, gnorm=1.815, loss_scale=0.125, train_wall=1, gb_free=4.4, wall=510\n",
            "2023-08-16 09:07:40 | INFO | train_inner | epoch 001:    991 / 6299 loss=6.025, nll_loss=4.187, ppl=18.22, wps=14757.5, ups=2.13, wpb=6942.5, bsz=184, num_updates=980, lr=1.96e-05, gnorm=2.028, loss_scale=0.125, train_wall=1, gb_free=5.1, wall=511\n",
            "2023-08-16 09:07:41 | INFO | train_inner | epoch 001:    993 / 6299 loss=5.807, nll_loss=3.921, ppl=15.15, wps=12700.2, ups=2.22, wpb=5732, bsz=196, num_updates=982, lr=1.964e-05, gnorm=2.336, loss_scale=0.125, train_wall=1, gb_free=5.4, wall=512\n",
            "2023-08-16 09:07:42 | INFO | train_inner | epoch 001:    995 / 6299 loss=5.848, nll_loss=3.961, ppl=15.57, wps=13814.8, ups=2.11, wpb=6547, bsz=236, num_updates=984, lr=1.968e-05, gnorm=2.059, loss_scale=0.125, train_wall=1, gb_free=5.5, wall=513\n",
            "2023-08-16 09:07:43 | INFO | train_inner | epoch 001:    997 / 6299 loss=5.961, nll_loss=4.087, ppl=16.99, wps=14388.8, ups=2.1, wpb=6837.5, bsz=180, num_updates=986, lr=1.972e-05, gnorm=2.18, loss_scale=0.125, train_wall=1, gb_free=4.8, wall=514\n",
            "2023-08-16 09:07:44 | INFO | train_inner | epoch 001:    999 / 6299 loss=5.872, nll_loss=4, ppl=16, wps=13583.5, ups=2.03, wpb=6693, bsz=216, num_updates=988, lr=1.976e-05, gnorm=2.161, loss_scale=0.125, train_wall=1, gb_free=4.9, wall=515\n",
            "2023-08-16 09:07:45 | INFO | train_inner | epoch 001:   1001 / 6299 loss=5.727, nll_loss=3.849, ppl=14.41, wps=14314, ups=2.1, wpb=6830, bsz=264, num_updates=990, lr=1.98e-05, gnorm=1.925, loss_scale=0.125, train_wall=1, gb_free=5, wall=516\n",
            "2023-08-16 09:07:45 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 0; 15.77 GiB total capacity; 13.60 GiB already allocated; 20.12 MiB free; 14.50 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2023-08-16 09:07:45 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 2            |        cudaMalloc retries: 6         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  13833 MiB |  13925 MiB |  59716 GiB |  59703 GiB |\n",
            "|       from large pool |  13826 MiB |  13918 MiB |  59308 GiB |  59294 GiB |\n",
            "|       from small pool |      6 MiB |      7 MiB |    408 GiB |    408 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  13833 MiB |  13925 MiB |  59716 GiB |  59703 GiB |\n",
            "|       from large pool |  13826 MiB |  13918 MiB |  59308 GiB |  59294 GiB |\n",
            "|       from small pool |      6 MiB |      7 MiB |    408 GiB |    408 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  13793 MiB |  13884 MiB |  59223 GiB |  59209 GiB |\n",
            "|       from large pool |  13786 MiB |  13878 MiB |  58815 GiB |  58801 GiB |\n",
            "|       from small pool |      6 MiB |      7 MiB |    408 GiB |    408 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14846 MiB |  14846 MiB |  86196 MiB |  71350 MiB |\n",
            "|       from large pool |  14838 MiB |  14838 MiB |  85924 MiB |  71086 MiB |\n",
            "|       from small pool |      8 MiB |     88 MiB |    272 MiB |    264 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    934 MiB |   2105 MiB |  55859 GiB |  55858 GiB |\n",
            "|       from large pool |    933 MiB |   2102 MiB |  55446 GiB |  55445 GiB |\n",
            "|       from small pool |      1 MiB |      3 MiB |    412 GiB |    412 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    2135    |    2140    |    8909 K  |    8906 K  |\n",
            "|       from large pool |    1004    |    1007    |    5022 K  |    5021 K  |\n",
            "|       from small pool |    1131    |    1310    |    3886 K  |    3885 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    2135    |    2140    |    8909 K  |    8906 K  |\n",
            "|       from large pool |    1004    |    1007    |    5022 K  |    5021 K  |\n",
            "|       from small pool |    1131    |    1310    |    3886 K  |    3885 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     297    |     337    |    1719    |    1422    |\n",
            "|       from large pool |     293    |     293    |    1583    |    1290    |\n",
            "|       from small pool |       4    |      44    |     136    |     132    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |     130    |     194    |    4447 K  |    4447 K  |\n",
            "|       from large pool |     116    |     159    |    3059 K  |    3059 K  |\n",
            "|       from small pool |      14    |      38    |    1388 K  |    1388 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2023-08-16 09:07:45 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2023-08-16 09:07:46 | INFO | train_inner | epoch 001:   1004 / 6299 loss=5.572, nll_loss=3.674, ppl=12.76, wps=11425.5, ups=1.7, wpb=6740, bsz=360, num_updates=992, lr=1.984e-05, gnorm=2.16, loss_scale=0.125, train_wall=1, gb_free=5, wall=517\n",
            "2023-08-16 09:07:47 | INFO | train_inner | epoch 001:   1006 / 6299 loss=6.128, nll_loss=4.338, ppl=20.22, wps=15171.7, ups=2.11, wpb=7194.5, bsz=500, num_updates=994, lr=1.988e-05, gnorm=2.88, loss_scale=0.125, train_wall=1, gb_free=5.5, wall=518\n",
            "2023-08-16 09:07:48 | INFO | train_inner | epoch 001:   1008 / 6299 loss=5.96, nll_loss=4.105, ppl=17.21, wps=14560.9, ups=2.08, wpb=6992.5, bsz=252, num_updates=996, lr=1.992e-05, gnorm=2.308, loss_scale=0.125, train_wall=1, gb_free=5.2, wall=519\n",
            "2023-08-16 09:07:49 | INFO | train_inner | epoch 001:   1010 / 6299 loss=5.964, nll_loss=4.095, ppl=17.08, wps=12193.3, ups=2.15, wpb=5680.5, bsz=180, num_updates=998, lr=1.996e-05, gnorm=2.038, loss_scale=0.125, train_wall=1, gb_free=5, wall=520\n",
            "2023-08-16 09:07:50 | INFO | train_inner | epoch 001:   1012 / 6299 loss=5.614, nll_loss=3.702, ppl=13.01, wps=13766.5, ups=2.08, wpb=6625.5, bsz=332, num_updates=1000, lr=2e-05, gnorm=2.054, loss_scale=0.125, train_wall=1, gb_free=4.6, wall=521\n",
            "2023-08-16 09:07:50 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-08-16 09:08:59 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 4.959 | nll_loss 2.647 | ppl 6.26 | wps 45807.3 | wpb 3144.6 | bsz 114.6 | num_updates 1000\n",
            "2023-08-16 09:08:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1000 updates\n",
            "2023-08-16 09:08:59 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/checkpoint_1_1000.pt\n",
            "2023-08-16 09:09:27 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/checkpoint_1_1000.pt\n",
            "2023-08-16 09:10:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_1_1000.pt (epoch 1 @ 1000 updates, score 4.959) (writing took 103.46573903699982 seconds)\n",
            "2023-08-16 09:10:43 | INFO | train_inner | epoch 001:   1014 / 6299 loss=5.774, nll_loss=3.885, ppl=14.77, wps=75.2, ups=0.01, wpb=6520, bsz=224, num_updates=1002, lr=2.004e-05, gnorm=1.841, loss_scale=0.125, train_wall=1, gb_free=5.1, wall=694\n",
            "2023-08-16 09:10:44 | INFO | train_inner | epoch 001:   1016 / 6299 loss=5.787, nll_loss=3.908, ppl=15.02, wps=13824.3, ups=2.13, wpb=6493, bsz=224, num_updates=1004, lr=2.008e-05, gnorm=1.912, loss_scale=0.125, train_wall=1, gb_free=5.3, wall=695\n",
            "2023-08-16 09:10:45 | INFO | train_inner | epoch 001:   1018 / 6299 loss=5.825, nll_loss=3.945, ppl=15.4, wps=12961.8, ups=2.1, wpb=6186.5, bsz=156, num_updates=1006, lr=2.012e-05, gnorm=2.024, loss_scale=0.125, train_wall=1, gb_free=5.5, wall=696\n",
            "2023-08-16 09:10:46 | INFO | train_inner | epoch 001:   1020 / 6299 loss=5.84, nll_loss=3.969, ppl=15.66, wps=13950.5, ups=2.13, wpb=6536, bsz=196, num_updates=1008, lr=2.016e-05, gnorm=2.022, loss_scale=0.125, train_wall=1, gb_free=5.1, wall=697\n",
            "2023-08-16 09:10:47 | INFO | train_inner | epoch 001:   1022 / 6299 loss=5.669, nll_loss=3.771, ppl=13.65, wps=11951.1, ups=2.17, wpb=5508, bsz=244, num_updates=1010, lr=2.02e-05, gnorm=2.246, loss_scale=0.125, train_wall=1, gb_free=5.7, wall=698\n",
            "2023-08-16 09:10:48 | INFO | train_inner | epoch 001:   1024 / 6299 loss=5.883, nll_loss=4.005, ppl=16.05, wps=14487.9, ups=2.05, wpb=7054, bsz=172, num_updates=1012, lr=2.024e-05, gnorm=2.202, loss_scale=0.125, train_wall=1, gb_free=5.1, wall=699\n",
            "2023-08-16 09:10:49 | INFO | train_inner | epoch 001:   1026 / 6299 loss=5.595, nll_loss=3.693, ppl=12.93, wps=12558.4, ups=2.23, wpb=5636, bsz=244, num_updates=1014, lr=2.028e-05, gnorm=2.206, loss_scale=0.125, train_wall=1, gb_free=5.4, wall=700\n",
            "2023-08-16 09:10:50 | INFO | train_inner | epoch 001:   1028 / 6299 loss=5.684, nll_loss=3.798, ppl=13.91, wps=13976.5, ups=2.17, wpb=6432, bsz=244, num_updates=1016, lr=2.032e-05, gnorm=2.035, loss_scale=0.125, train_wall=1, gb_free=5, wall=701\n",
            "2023-08-16 09:10:51 | INFO | train_inner | epoch 001:   1030 / 6299 loss=5.591, nll_loss=3.696, ppl=12.96, wps=14030.8, ups=2.07, wpb=6776, bsz=284, num_updates=1018, lr=2.036e-05, gnorm=1.847, loss_scale=0.125, train_wall=1, gb_free=4.9, wall=702\n",
            "2023-08-16 09:10:52 | INFO | train_inner | epoch 001:   1032 / 6299 loss=5.844, nll_loss=3.992, ppl=15.91, wps=13516.3, ups=2.13, wpb=6351, bsz=160, num_updates=1020, lr=2.04e-05, gnorm=1.903, loss_scale=0.125, train_wall=1, gb_free=5.1, wall=703\n",
            "2023-08-16 09:10:53 | INFO | train_inner | epoch 001:   1034 / 6299 loss=5.906, nll_loss=4.054, ppl=16.61, wps=12048, ups=2.1, wpb=5738, bsz=112, num_updates=1022, lr=2.044e-05, gnorm=2.23, loss_scale=0.125, train_wall=1, gb_free=4.5, wall=704\n",
            "2023-08-16 09:10:53 | INFO | train_inner | epoch 001:   1036 / 6299 loss=5.766, nll_loss=3.885, ppl=14.78, wps=14329, ups=2.09, wpb=6872, bsz=184, num_updates=1024, lr=2.048e-05, gnorm=1.763, loss_scale=0.125, train_wall=1, gb_free=4.9, wall=704\n",
            "2023-08-16 09:10:54 | INFO | train_inner | epoch 001:   1038 / 6299 loss=5.682, nll_loss=3.787, ppl=13.8, wps=13741.2, ups=2.04, wpb=6720, bsz=204, num_updates=1026, lr=2.052e-05, gnorm=1.681, loss_scale=0.125, train_wall=1, gb_free=5.2, wall=705\n",
            "2023-08-16 09:10:56 | INFO | train_inner | epoch 001:   1040 / 6299 loss=5.859, nll_loss=3.997, ppl=15.97, wps=12655.1, ups=1.95, wpb=6479, bsz=124, num_updates=1028, lr=2.056e-05, gnorm=1.963, loss_scale=0.125, train_wall=1, gb_free=4.8, wall=706\n",
            "2023-08-16 09:10:56 | INFO | train_inner | epoch 001:   1042 / 6299 loss=5.62, nll_loss=3.731, ppl=13.27, wps=13699.3, ups=2.02, wpb=6792, bsz=268, num_updates=1030, lr=2.06e-05, gnorm=1.691, loss_scale=0.125, train_wall=1, gb_free=5.3, wall=707\n",
            "2023-08-16 09:10:57 | INFO | train_inner | epoch 001:   1044 / 6299 loss=5.745, nll_loss=3.87, ppl=14.62, wps=13259.6, ups=2.01, wpb=6602, bsz=192, num_updates=1032, lr=2.064e-05, gnorm=1.907, loss_scale=0.125, train_wall=1, gb_free=4.9, wall=708\n",
            "2023-08-16 09:10:58 | INFO | train_inner | epoch 001:   1046 / 6299 loss=5.549, nll_loss=3.634, ppl=12.42, wps=12846.6, ups=2.17, wpb=5932, bsz=248, num_updates=1034, lr=2.068e-05, gnorm=1.886, loss_scale=0.125, train_wall=1, gb_free=5.4, wall=709\n",
            "2023-08-16 09:10:59 | INFO | train_inner | epoch 001:   1048 / 6299 loss=5.58, nll_loss=3.67, ppl=12.73, wps=14758.8, ups=1.97, wpb=7476, bsz=372, num_updates=1036, lr=2.072e-05, gnorm=1.929, loss_scale=0.125, train_wall=1, gb_free=5, wall=710\n",
            "2023-08-16 09:11:00 | INFO | train_inner | epoch 001:   1050 / 6299 loss=5.575, nll_loss=3.666, ppl=12.7, wps=12911.9, ups=2.09, wpb=6168, bsz=220, num_updates=1038, lr=2.076e-05, gnorm=1.835, loss_scale=0.125, train_wall=1, gb_free=4.7, wall=711\n",
            "2023-08-16 09:11:01 | INFO | train_inner | epoch 001:   1052 / 6299 loss=5.642, nll_loss=3.755, ppl=13.5, wps=14041.3, ups=2.11, wpb=6656, bsz=196, num_updates=1040, lr=2.08e-05, gnorm=1.74, loss_scale=0.125, train_wall=1, gb_free=5.3, wall=712\n",
            "2023-08-16 09:11:02 | INFO | train_inner | epoch 001:   1054 / 6299 loss=5.505, nll_loss=3.608, ppl=12.19, wps=13710.7, ups=2.09, wpb=6557, bsz=304, num_updates=1042, lr=2.084e-05, gnorm=1.682, loss_scale=0.125, train_wall=1, gb_free=4.9, wall=713\n",
            "2023-08-16 09:11:03 | INFO | train_inner | epoch 001:   1056 / 6299 loss=5.122, nll_loss=3.181, ppl=9.07, wps=14178.7, ups=2.03, wpb=6968, bsz=524, num_updates=1044, lr=2.088e-05, gnorm=1.835, loss_scale=0.125, train_wall=1, gb_free=4.9, wall=714\n",
            "2023-08-16 09:11:04 | INFO | train_inner | epoch 001:   1058 / 6299 loss=5.72, nll_loss=3.842, ppl=14.34, wps=13858.1, ups=2.13, wpb=6516, bsz=212, num_updates=1046, lr=2.092e-05, gnorm=2.028, loss_scale=0.125, train_wall=1, gb_free=5, wall=715\n",
            "2023-08-16 09:11:05 | INFO | train_inner | epoch 001:   1060 / 6299 loss=5.538, nll_loss=3.629, ppl=12.37, wps=14286.6, ups=2.06, wpb=6952, bsz=284, num_updates=1048, lr=2.096e-05, gnorm=1.718, loss_scale=0.125, train_wall=1, gb_free=5, wall=716\n",
            "2023-08-16 09:11:06 | INFO | train_inner | epoch 001:   1062 / 6299 loss=5.664, nll_loss=3.767, ppl=13.61, wps=12986.5, ups=2.16, wpb=6012, bsz=164, num_updates=1050, lr=2.1e-05, gnorm=1.946, loss_scale=0.125, train_wall=1, gb_free=4.6, wall=717\n",
            "2023-08-16 09:11:07 | INFO | train_inner | epoch 001:   1064 / 6299 loss=5.549, nll_loss=3.647, ppl=12.53, wps=13365.4, ups=1.96, wpb=6812.5, bsz=272, num_updates=1052, lr=2.104e-05, gnorm=1.721, loss_scale=0.125, train_wall=1, gb_free=4.3, wall=718\n",
            "2023-08-16 09:11:08 | INFO | train_inner | epoch 001:   1066 / 6299 loss=5.768, nll_loss=3.909, ppl=15.02, wps=13155.6, ups=2.12, wpb=6204.5, bsz=156, num_updates=1054, lr=2.108e-05, gnorm=1.974, loss_scale=0.125, train_wall=1, gb_free=5.1, wall=719\n",
            "2023-08-16 09:11:09 | INFO | train_inner | epoch 001:   1068 / 6299 loss=5.653, nll_loss=3.785, ppl=13.78, wps=14033.2, ups=2.13, wpb=6576, bsz=200, num_updates=1056, lr=2.112e-05, gnorm=1.799, loss_scale=0.125, train_wall=1, gb_free=5.4, wall=720\n",
            "2023-08-16 09:11:10 | INFO | train_inner | epoch 001:   1070 / 6299 loss=5.508, nll_loss=3.624, ppl=12.33, wps=14163.6, ups=2.05, wpb=6917, bsz=300, num_updates=1058, lr=2.116e-05, gnorm=2.032, loss_scale=0.125, train_wall=1, gb_free=5.3, wall=721\n",
            "2023-08-16 09:11:11 | INFO | train_inner | epoch 001:   1072 / 6299 loss=5.341, nll_loss=3.42, ppl=10.7, wps=12618.3, ups=2.13, wpb=5918, bsz=284, num_updates=1060, lr=2.12e-05, gnorm=1.972, loss_scale=0.125, train_wall=1, gb_free=4.9, wall=722\n",
            "2023-08-16 09:11:12 | INFO | train_inner | epoch 001:   1074 / 6299 loss=5.608, nll_loss=3.734, ppl=13.31, wps=13739.7, ups=2.21, wpb=6223, bsz=312, num_updates=1062, lr=2.124e-05, gnorm=2.424, loss_scale=0.125, train_wall=1, gb_free=4.4, wall=723\n",
            "2023-08-16 09:11:13 | INFO | train_inner | epoch 001:   1076 / 6299 loss=5.719, nll_loss=3.821, ppl=14.13, wps=14563, ups=2, wpb=7276.5, bsz=188, num_updates=1064, lr=2.128e-05, gnorm=2.154, loss_scale=0.125, train_wall=1, gb_free=4.4, wall=724\n",
            "2023-08-16 09:11:14 | INFO | train_inner | epoch 001:   1078 / 6299 loss=5.329, nll_loss=3.392, ppl=10.5, wps=13329.5, ups=2.01, wpb=6631, bsz=356, num_updates=1066, lr=2.132e-05, gnorm=2.244, loss_scale=0.125, train_wall=1, gb_free=4.6, wall=725\n",
            "2023-08-16 09:11:15 | INFO | train_inner | epoch 001:   1080 / 6299 loss=5.626, nll_loss=3.727, ppl=13.24, wps=13060.2, ups=2.16, wpb=6037, bsz=124, num_updates=1068, lr=2.136e-05, gnorm=2.137, loss_scale=0.125, train_wall=1, gb_free=5, wall=726\n",
            "2023-08-16 09:11:16 | INFO | train_inner | epoch 001:   1082 / 6299 loss=5.595, nll_loss=3.726, ppl=13.23, wps=13787.9, ups=2.13, wpb=6461, bsz=184, num_updates=1070, lr=2.14e-05, gnorm=2.435, loss_scale=0.125, train_wall=1, gb_free=4.7, wall=727\n",
            "2023-08-16 09:11:17 | INFO | train_inner | epoch 001:   1084 / 6299 loss=5.396, nll_loss=3.491, ppl=11.24, wps=13560.6, ups=2.05, wpb=6604, bsz=244, num_updates=1072, lr=2.144e-05, gnorm=1.629, loss_scale=0.125, train_wall=1, gb_free=5.1, wall=728\n",
            "2023-08-16 09:11:18 | INFO | train_inner | epoch 001:   1086 / 6299 loss=5.635, nll_loss=3.78, ppl=13.74, wps=12797.1, ups=2.2, wpb=5811, bsz=172, num_updates=1074, lr=2.148e-05, gnorm=2.452, loss_scale=0.125, train_wall=1, gb_free=5.4, wall=729\n",
            "2023-08-16 09:11:19 | INFO | train_inner | epoch 001:   1088 / 6299 loss=5.53, nll_loss=3.629, ppl=12.38, wps=13996.4, ups=2.1, wpb=6656, bsz=188, num_updates=1076, lr=2.152e-05, gnorm=1.85, loss_scale=0.125, train_wall=1, gb_free=5, wall=730\n",
            "2023-08-16 09:11:20 | INFO | train_inner | epoch 001:   1090 / 6299 loss=5.279, nll_loss=3.342, ppl=10.14, wps=13161.1, ups=2, wpb=6569, bsz=384, num_updates=1078, lr=2.156e-05, gnorm=2.035, loss_scale=0.125, train_wall=1, gb_free=4.5, wall=731\n",
            "2023-08-16 09:11:21 | INFO | train_inner | epoch 001:   1092 / 6299 loss=5.524, nll_loss=3.626, ppl=12.34, wps=13716, ups=2.03, wpb=6755, bsz=196, num_updates=1080, lr=2.16e-05, gnorm=1.774, loss_scale=0.125, train_wall=1, gb_free=5.2, wall=732\n",
            "2023-08-16 09:11:21 | INFO | train_inner | epoch 001:   1094 / 6299 loss=5.269, nll_loss=3.348, ppl=10.18, wps=12284.3, ups=2.07, wpb=5937, bsz=280, num_updates=1082, lr=2.164e-05, gnorm=1.82, loss_scale=0.125, train_wall=1, gb_free=5.3, wall=732\n",
            "2023-08-16 09:11:22 | INFO | train_inner | epoch 001:   1096 / 6299 loss=5.087, nll_loss=3.144, ppl=8.84, wps=13009.6, ups=2.07, wpb=6272, bsz=380, num_updates=1084, lr=2.168e-05, gnorm=1.837, loss_scale=0.125, train_wall=1, gb_free=5.9, wall=733\n",
            "2023-08-16 09:11:23 | INFO | train_inner | epoch 001:   1098 / 6299 loss=5.361, nll_loss=3.436, ppl=10.82, wps=14369.2, ups=2.05, wpb=7004, bsz=284, num_updates=1086, lr=2.172e-05, gnorm=1.906, loss_scale=0.125, train_wall=1, gb_free=5.3, wall=734\n",
            "2023-08-16 09:11:24 | INFO | train_inner | epoch 001:   1100 / 6299 loss=5.632, nll_loss=3.739, ppl=13.35, wps=14383.9, ups=2.08, wpb=6916, bsz=168, num_updates=1088, lr=2.176e-05, gnorm=2.035, loss_scale=0.125, train_wall=1, gb_free=4.9, wall=735\n",
            "2023-08-16 09:11:25 | INFO | train_inner | epoch 001:   1102 / 6299 loss=5.572, nll_loss=3.684, ppl=12.85, wps=13913.1, ups=2.09, wpb=6670, bsz=180, num_updates=1090, lr=2.18e-05, gnorm=1.801, loss_scale=0.125, train_wall=1, gb_free=4.8, wall=736\n",
            "2023-08-16 09:11:26 | INFO | train_inner | epoch 001:   1104 / 6299 loss=5.621, nll_loss=3.74, ppl=13.36, wps=13562.8, ups=2.12, wpb=6394.5, bsz=164, num_updates=1092, lr=2.184e-05, gnorm=2.16, loss_scale=0.125, train_wall=1, gb_free=5.6, wall=737\n",
            "2023-08-16 09:11:27 | INFO | train_inner | epoch 001:   1106 / 6299 loss=5.252, nll_loss=3.328, ppl=10.04, wps=14400.8, ups=1.98, wpb=7260, bsz=388, num_updates=1094, lr=2.188e-05, gnorm=1.593, loss_scale=0.125, train_wall=1, gb_free=4.6, wall=738\n",
            "2023-08-16 09:11:28 | INFO | train_inner | epoch 001:   1108 / 6299 loss=5.084, nll_loss=3.196, ppl=9.17, wps=13813.5, ups=2.04, wpb=6776, bsz=836, num_updates=1096, lr=2.192e-05, gnorm=3.107, loss_scale=0.125, train_wall=1, gb_free=5.7, wall=739\n",
            "2023-08-16 09:11:29 | INFO | train_inner | epoch 001:   1110 / 6299 loss=5.378, nll_loss=3.471, ppl=11.09, wps=13872, ups=2.09, wpb=6631, bsz=276, num_updates=1098, lr=2.196e-05, gnorm=2.273, loss_scale=0.125, train_wall=1, gb_free=4.9, wall=740\n",
            "2023-08-16 09:11:30 | INFO | train_inner | epoch 001:   1112 / 6299 loss=5.581, nll_loss=3.69, ppl=12.91, wps=14455.3, ups=2.08, wpb=6954, bsz=192, num_updates=1100, lr=2.2e-05, gnorm=2.195, loss_scale=0.125, train_wall=1, gb_free=4.5, wall=741\n",
            "2023-08-16 09:11:31 | INFO | train_inner | epoch 001:   1114 / 6299 loss=5.386, nll_loss=3.468, ppl=11.07, wps=13530, ups=2.03, wpb=6670, bsz=232, num_updates=1102, lr=2.204e-05, gnorm=1.801, loss_scale=0.125, train_wall=1, gb_free=4.7, wall=742\n",
            "2023-08-16 09:11:32 | INFO | train_inner | epoch 001:   1116 / 6299 loss=5.318, nll_loss=3.395, ppl=10.52, wps=13818.1, ups=2.1, wpb=6568, bsz=280, num_updates=1104, lr=2.208e-05, gnorm=2.135, loss_scale=0.125, train_wall=1, gb_free=4.8, wall=743\n",
            "2023-08-16 09:11:33 | INFO | train_inner | epoch 001:   1118 / 6299 loss=5.233, nll_loss=3.306, ppl=9.89, wps=13607.7, ups=1.96, wpb=6931.5, bsz=352, num_updates=1106, lr=2.212e-05, gnorm=1.595, loss_scale=0.125, train_wall=1, gb_free=5.2, wall=744\n",
            "2023-08-16 09:11:34 | INFO | train_inner | epoch 001:   1120 / 6299 loss=5.437, nll_loss=3.557, ppl=11.77, wps=13521, ups=2.04, wpb=6631.5, bsz=204, num_updates=1108, lr=2.216e-05, gnorm=2.115, loss_scale=0.125, train_wall=1, gb_free=4.7, wall=745\n",
            "2023-08-16 09:11:35 | INFO | train_inner | epoch 001:   1122 / 6299 loss=5.511, nll_loss=3.633, ppl=12.41, wps=12743, ups=2.08, wpb=6132, bsz=144, num_updates=1110, lr=2.22e-05, gnorm=2.302, loss_scale=0.125, train_wall=1, gb_free=5.4, wall=746\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/tasks/fairseq_task.py\", line 515, in train_step\n",
            "    loss, sample_size, logging_output = criterion(model, sample)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/criterions/label_smoothed_cross_entropy.py\", line 79, in forward\n",
            "    net_output = model(**sample[\"net_input\"])\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/models/bart/model.py\", line 94, in forward\n",
            "    x, extra = self.decoder(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/models/transformer/transformer_decoder.py\", line 217, in forward\n",
            "    x, extra = self.extract_features(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/models/transformer/transformer_decoder.py\", line 239, in extract_features\n",
            "    return self.extract_features_scriptable(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/models/transformer/transformer_decoder.py\", line 340, in extract_features_scriptable\n",
            "    x, layer_attn, _ = layer(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/modules/transformer_layer.py\", line 660, in forward\n",
            "    x = self.activation_fn(self.fc1(x))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/modules/gelu.py\", line 25, in gelu\n",
            "    return torch.nn.functional.gelu(x.float()).type_as(x)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/fairseq-train\", line 8, in <module>\n",
            "    sys.exit(cli_main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq_cli/train.py\", line 557, in cli_main\n",
            "    distributed_utils.call_main(cfg, main)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/distributed/utils.py\", line 369, in call_main\n",
            "    main(cfg, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq_cli/train.py\", line 190, in main\n",
            "    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)\n",
            "  File \"/usr/lib/python3.10/contextlib.py\", line 79, in inner\n",
            "    return func(*args, **kwds)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq_cli/train.py\", line 316, in train\n",
            "    log_output = trainer.train_step(samples)\n",
            "  File \"/usr/lib/python3.10/contextlib.py\", line 79, in inner\n",
            "    return func(*args, **kwds)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/trainer.py\", line 824, in train_step\n",
            "    loss, sample_size_i, logging_output = self.task.train_step(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/tasks/fairseq_task.py\", line 513, in train_step\n",
            "    with torch.autograd.profiler.record_function(\"forward\"):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/profiler.py\", line 507, in __exit__\n",
            "    torch.ops.profiler._record_function_exit._RecordFunction(record)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_ops.py\", line 286, in __call__\n",
            "    def __call__(self, *args, **kwargs):\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate on Vi-En\n",
        "\n",
        "Get sacrebleu on finetuned vi-en model"
      ],
      "metadata": {
        "id": "g22afSZP4CNs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !fairseq-generate \"bin\" \\\n",
        "#   --path checkpoints/checkpoint_best.pt \\\n",
        "#   --task translation_from_pretrained_bart \\\n",
        "#   --gen-subset valid \\\n",
        "#   --source-lang vi --target-lang en \\\n",
        "#   --bpe 'sentencepiece' --sentencepiece-model drive/MyDrive/sentencepiece.bpe.model \\\n",
        "#   --sacrebleu --remove-bpe 'sentencepiece' \\\n",
        "#   --beam 5 --nbest 1 \\\n",
        "#   --batch-size 32 --langs ar_AR,cs_CZ,de_DE,en_XX,es_XX,et_EE,fi_FI,fr_XX,gu_IN,hi_IN,it_IT,ja_XX,kk_KZ,ko_KR,lt_LT,lv_LV,my_MM,ne_NP,nl_XX,ro_RO,ru_RU,si_LK,tr_TR,vi_VN,zh_CN > result.txt\n"
      ],
      "metadata": {
        "id": "6ViI6m3n4H_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !tail -n 20 result.txt"
      ],
      "metadata": {
        "id": "b95RHw0sbjGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Write test data to file\n",
        "# with open('vi_en.ref', 'w') as f:\n",
        "#     for line in en_test:\n",
        "#         f.write(f\"{line}\\n\")"
      ],
      "metadata": {
        "id": "GY5sFmDBL_MZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !cat result.txt | grep -P \"^H\" | sort -V | cut -f 3- | sed 's/\\[en\\]//g' > vi_en.hyp\n",
        "# # #!cat result.txt | grep -P \"^T\" | sort -V | cut -f 2- | sed 's/\\[en\\]//g' | $SPM/spm_decode --model drive/MyDrive/mbart.cc25.v2/mbart.cc25.v2/sentencepiece.bpe.model > vi_en.ref\n",
        "# !sacrebleu -tok 'none' -s 'none' drive/MyDrive/valid.en < vi_en.hyp"
      ],
      "metadata": {
        "id": "7wZcDPC46yVd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}