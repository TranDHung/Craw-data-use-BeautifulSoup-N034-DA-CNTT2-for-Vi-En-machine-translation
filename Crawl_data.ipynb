{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ESTGjsj4PpwK",
        "2tUD9nBLPxfu",
        "ZrAQS94APxs4",
        "EvnnoEkhPx35",
        "ruRHM2ZGXJ1W",
        "9PfdP8gSdVF-",
        "_cyv-LDsYe3u"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# install lib for translate\n",
        "!pip install googletrans==3.1.0a0\n",
        "\n",
        "# Import all lib need for crawl and translation\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import googletrans\n",
        "from googletrans import Translator"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKtey0ziSs0x",
        "outputId": "0e6dab87-e716-451b-915a-d2debe54087e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting googletrans==3.1.0a0\n",
            "  Downloading googletrans-3.1.0a0.tar.gz (19 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting httpx==0.13.3 (from googletrans==3.1.0a0)\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2023.7.22)\n",
            "Collecting hstspreload (from httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Downloading hstspreload-2023.1.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.3.0)\n",
            "Collecting chardet==3.* (from httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting idna==2.* (from httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-3.1.0a0-py3-none-any.whl size=16352 sha256=aae63e480aea57f045aad5beffcf330e4198203c880a9bd932564741e4217fe8\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/5d/3c/8477d0af4ca2b8b1308812c09f1930863caeebc762fe265a95\n",
            "Successfully built googletrans\n",
            "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.4\n",
            "    Uninstalling idna-3.4:\n",
            "      Successfully uninstalled idna-3.4\n",
            "Successfully installed chardet-3.0.4 googletrans-3.1.0a0 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2023.1.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Algorithm 1:\n",
        "Code crawl data from ieltstrainingonline.com with BeautifulSoup"
      ],
      "metadata": {
        "id": "ESTGjsj4PpwK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GB_rOZEqPnfM"
      },
      "outputs": [],
      "source": [
        "# Crawl data from ieltstrainingonline.com with BeautifulSoup\n",
        "def crawlDataIeltstrainingonline():\n",
        "  with open('data.txt', 'w') as f:\n",
        "    # crawl all data of cambridge book 10 - 18, reading 4 test\n",
        "    for i in range(10, 19):\n",
        "      for j in range(1,5):\n",
        "        url = \"https://ieltstrainingonline.com/practice-cambridge-ielts-\"+str(i)+\"-reading-test-0\"+str(j)+\"-with-answer/\"\n",
        "        html_content = requests.get(url).text\n",
        "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "        texts = soup.find_all('p')\n",
        "        for text in texts:\n",
        "          # get paragraph with len > 10\n",
        "          if len((text.get_text()).strip()) > 10:\n",
        "            # get paragraph not include special char\n",
        "            if '……………' not in text.get_text() and (text.get_text())[0].isdigit() != True and len((text.get_text()).strip()) > 20:\n",
        "              # write data to file\n",
        "              f.write((text.get_text()).strip())\n",
        "              f.write('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# crawl all data from ieltstrainingonline.com\n",
        "crawlDataIeltstrainingonline()"
      ],
      "metadata": {
        "id": "GQBL1ETLcmDJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Algorithm 2:\n",
        "Code get all URL data pages of web site, prepare for crawl data with URLs"
      ],
      "metadata": {
        "id": "2tUD9nBLPxfu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "UrlOukMnPxfu"
      },
      "outputs": [],
      "source": [
        "# crawl all link data pages mini ielts.com, hrw.org\n",
        "def crawlAllURL(page_start, page_end, url_web, url_write, name_file):\n",
        "  HEADERS = {'User-Agent': 'Mozilla/5.0 (iPad; CPU OS 12_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148'}\n",
        "  with open('url.txt', 'w') as f:\n",
        "    for i in range(page_start, page_end+1):\n",
        "      # get url and all link tag a in page of this url\n",
        "      url = url_web+str(i)\n",
        "      html_content = requests.get(url, headers=HEADERS).text\n",
        "\n",
        "      soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "      texts = soup.find_all('a')\n",
        "      for text in texts:\n",
        "        if text:\n",
        "          if text.has_attr('href'):\n",
        "            # write all url satisfies with conditions\n",
        "            if 'reading/' in text['href'] and 'view-solution' not in text['href'] and 'https://www.hrw.org/' not in text['href'] and 'https://www.hrw.org/' not in text['href'] and 'dispatches' not in text['href'] and 'video-photos' not in text['href'] and 'interview' not in text['href'] and 'news-release' not in text['href']:\n",
        "              f.write((url_write+text['href']).strip())\n",
        "              f.write('\\n')\n",
        "  # remove duplicate\n",
        "  data = open(\"/content/url.txt\")\n",
        "  all_url = data.readlines()\n",
        "  urls = [*set(all_url)]\n",
        "\n",
        "  # write all url to file\n",
        "  with open(\"/content/\"+name_file, 'w') as f:\n",
        "    for line in urls:\n",
        "      f.write(line.strip())\n",
        "      f.write('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# crawl reading test URL in  mini ielts.com\n",
        "crawlAllURL(1, 55, \"https://mini-ielts.com/reading?page=\", \"https://mini-ielts.com\", \"url_mini_test.txt\")\n",
        "\n",
        "# crawl news data URL in hrw.org\n",
        "crawlAllURL(1, 726, \"https://www.hrw.org/languages?language=vi&page=\", \"https://www.hrw.org\", \"url_hrw.txt\")"
      ],
      "metadata": {
        "id": "1IAwkCaoUKZx"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Algorithm 3:\n",
        "Code crawl all data mini-ielts.com with url"
      ],
      "metadata": {
        "id": "ZrAQS94APxs4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "upjOyRuQPxs4"
      },
      "outputs": [],
      "source": [
        "def crawlDataMiniIelts(urls):\n",
        "  with open('data.txt', 'w') as f:\n",
        "    for i in urls:\n",
        "      if i.strip():\n",
        "        url = i.strip()\n",
        "        html_content = requests.get(url).text\n",
        "        # get all data of page\n",
        "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "        # get main data in tag <div class=\"reading-text\">\n",
        "        div = soup.find(\"div\", class_=\"reading-text\")\n",
        "\n",
        "        # write data to file\n",
        "        f.write((div.get_text()).strip())\n",
        "        f.write('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read all URL\n",
        "data = open(\"/content/url_mini_test.txt\")\n",
        "urls = data.readlines()\n",
        "\n",
        "# crawl all data with URL\n",
        "crawlDataMiniIelts(urls)"
      ],
      "metadata": {
        "id": "o7auytAgVwgj"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Algorithm 4:\n",
        "Code crawl all data hrw.org with url"
      ],
      "metadata": {
        "id": "EvnnoEkhPx35"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "7He5buWHPx35"
      },
      "outputs": [],
      "source": [
        "# crawl all data hrw.org with url\n",
        "def crawlDataHrw(urls):\n",
        "  HEADERS = {'User-Agent': 'Mozilla/5.0 (iPad; CPU OS 12_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148'}\n",
        "  with open('data.txt', 'w') as f:\n",
        "    for i in urls:\n",
        "      url = i.strip()\n",
        "      html_content = requests.get(url, headers=HEADERS).text\n",
        "      # get all data of page\n",
        "      soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "\n",
        "      # get main data in tag <div class =\"WordSection2\">\n",
        "      divs = soup.find(\"div\", class_=\"WordSection2\")\n",
        "      if divs:\n",
        "        # get data in tag p, i\n",
        "        p = divs.find_all('p')\n",
        "        li = divs.find_all('li')\n",
        "        if p:\n",
        "          for pt in p:\n",
        "            if pt.get_text().strip() != '' and len(pt.get_text().strip()) > 10:\n",
        "              # write data to file\n",
        "              f.write(pt.get_text().strip())\n",
        "              f.write('\\n')\n",
        "        if li:\n",
        "          for lit in li:\n",
        "            if lit.get_text().strip() != '' and len(lit.get_text().strip()) > 10:\n",
        "              # write data to file\n",
        "              f.write(lit.get_text().strip())\n",
        "              f.write('\\n')\n",
        "      else:\n",
        "        # get main data in tag <div class =\"article-body\">\n",
        "        div1s = soup.find(\"div\", class_=\"article-body\")\n",
        "        if div1s:\n",
        "          # get data in tag p\n",
        "          p = div1s.find_all('p')\n",
        "          if p:\n",
        "            for pt in p:\n",
        "              if pt.get_text().strip() != '' and len(pt.get_text().strip()) > 10:\n",
        "                # write data to file\n",
        "                f.write(pt.get_text().strip())\n",
        "                f.write('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read all URL\n",
        "data = open(\"/content/url_hrw.txt\")\n",
        "urls = data.readlines()\n",
        "\n",
        "# crawl all data with URL\n",
        "crawlDataHrw(urls)"
      ],
      "metadata": {
        "id": "a3_DRjjWW7yk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Algorithm 5:\n",
        "Crawl all data CNN with urls (manual handling)"
      ],
      "metadata": {
        "id": "ruRHM2ZGXJ1W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crawl all data CNN with urls\n",
        "def crawlDataCNN(url):\n",
        "  with open('data_cnn.txt', 'w') as f:\n",
        "    for i in urls:\n",
        "      url = i.strip()\n",
        "      html_content = requests.get(url).text\n",
        "      # get all data of page\n",
        "      soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "      # get main data in tag <div class=\"article__content-container\"\n",
        "      div = soup.find(\"div\", class_=\"article__content-container\")\n",
        "      if div:\n",
        "        # get data in tag p\n",
        "        texts = div.find_all('p')\n",
        "        for text in texts:\n",
        "          if len((text.get_text()).strip()) > 100:\n",
        "            # write data to file\n",
        "            f.write((text.get_text()).strip())\n",
        "            f.write('\\n')"
      ],
      "metadata": {
        "id": "kaxEOroaXTuv"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read all URL\n",
        "data = open(\"/content/url_cnn.txt\")\n",
        "urls = data.readlines()\n",
        "\n",
        "# crawl all data with URL\n",
        "crawlDataCNN(urls)"
      ],
      "metadata": {
        "id": "X6bEeGjVYBF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Divide data\n",
        "Divide data with one sentence in one line"
      ],
      "metadata": {
        "id": "9PfdP8gSdVF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read data in file\n",
        "data = open(\"/content/data.txt\") #change name data file for divide\n",
        "data = data.readlines()\n",
        "\n",
        "lines = []\n",
        "lines_wrong = []\n",
        "for line in data:\n",
        "  if(line != '\\n' and line != ''):\n",
        "    # divide data with '. '\n",
        "    if '. ' in line:\n",
        "      # except special cases\n",
        "      if 'Mr. ' not in line and 'Mrs. ' not in line and 'Dr. ' not in line and 'p.m. ' not in line and 'a.m. ' not in line:\n",
        "        ln = line.split('. ')\n",
        "        for ln_c in ln:\n",
        "          # remove empty data and data too short\n",
        "          if ln_c != '\\n' and ln_c != '' and len(ln_c) > 10:\n",
        "            lines.append((ln_c.replace(\"\\n\", \"\")).strip())\n",
        "      else:\n",
        "        lines_wrong.append(line.replace(\"\\n\", \"\").strip())\n",
        "    else:\n",
        "      if line.strip():\n",
        "        ln_c1 = line.replace(\"\\n\", \"\")\n",
        "        lines.append(ln_c1.strip())\n",
        "\n",
        "# write to file with same name\n",
        "with open('data.txt', 'w') as f:\n",
        "  for line in lines:\n",
        "    f.write(line)\n",
        "    f.write('\\n')"
      ],
      "metadata": {
        "id": "d7pRLtJndkJR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Algorithm 6:\n",
        "Code translate all data English to Vietnamese use googletrans"
      ],
      "metadata": {
        "id": "_cyv-LDsYe3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Translate all data English to Vietnamese use googletrans\n",
        "def translateData(data):\n",
        "  ftranslate = Translator()\n",
        "\n",
        "  result = []\n",
        "  # remove ‘\\n’ in data\n",
        "  for en in contents:\n",
        "    result.append(en.replace('\\n',''))\n",
        "  # call Translator to translate data to Vietnamese\n",
        "  result_vie = ftranslate.translate(result, dest='vi')\n",
        "\n",
        "  vie = []\n",
        "  for x in result_vie:\n",
        "    # get result translate\n",
        "    vie.append(x.text)\n",
        "  # write all Vietnamese to vi.txt file\n",
        "  with open('vi.txt', 'w') as f:\n",
        "      for line in vie:\n",
        "          f.write(line)\n",
        "          f.write('\\n')"
      ],
      "metadata": {
        "id": "TcUoezS_YjPN"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read English data for translate, run with small data in a time (divide data file crawled)\n",
        "data_en = open(\"/content/data_en.txt\")\n",
        "contents = data_en.readlines()\n",
        "\n",
        "# translate all data\n",
        "translateData(contents)"
      ],
      "metadata": {
        "id": "VIA_YoJYY-3h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}